2025-08-27 14:49:20,909 p=19789 u=root n=ansible WARNING| [WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'

2025-08-27 14:49:21,071 p=19789 u=root n=ansible WARNING| [WARNING]: Found variable using reserved name: namespace

2025-08-27 14:49:21,106 p=19789 u=root n=ansible INFO| PLAY [all] **************************************************************************************************************************************************************************************************************************************************************
2025-08-27 14:49:21,107 p=19789 u=root n=ansible INFO| skipping: no hosts matched
2025-08-27 14:49:21,107 p=19789 u=root n=ansible WARNING| [WARNING]: Could not match supplied host pattern, ignoring: local

2025-08-27 14:49:21,107 p=19789 u=root n=ansible INFO| PLAY [all,local] ********************************************************************************************************************************************************************************************************************************************************
2025-08-27 14:49:21,107 p=19789 u=root n=ansible INFO| skipping: no hosts matched
2025-08-27 14:49:21,108 p=19789 u=root n=ansible WARNING| [WARNING]: Could not match supplied host pattern, ignoring: masters

2025-08-27 14:49:21,108 p=19789 u=root n=ansible INFO| PLAY [masters] **********************************************************************************************************************************************************************************************************************************************************
2025-08-27 14:49:21,108 p=19789 u=root n=ansible INFO| skipping: no hosts matched
2025-08-27 14:49:21,109 p=19789 u=root n=ansible INFO| PLAY [masters] **********************************************************************************************************************************************************************************************************************************************************
2025-08-27 14:49:21,109 p=19789 u=root n=ansible INFO| skipping: no hosts matched
2025-08-27 14:49:21,109 p=19789 u=root n=ansible INFO| PLAY [Pausa dopo creazione cluster] *************************************************************************************************************************************************************************************************************************************
2025-08-27 14:49:21,115 p=19789 u=root n=ansible INFO| TASK [Attendi 180 secondi prima setup kubectl] **************************************************************************************************************************************************************************************************************************
2025-08-27 14:49:21,123 p=19789 u=root n=ansible INFO| Pausing for 180 seconds
2025-08-27 14:49:21,123 p=19789 u=root n=ansible INFO| (ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
2025-08-27 14:49:21,123 p=19789 u=root n=ansible INFO| [Attendi 180 secondi prima setup kubectl]
Attendendo 60 secondi dopo creazione cluster...:
2025-08-27 14:49:24,646 p=19789 u=root n=ansible INFO| Press 'C' to continue the play or 'A' to abort 
2025-08-27 14:52:19,881 p=19789 u=root n=ansible INFO| fatal: [localhost]: FAILED! => {"msg": "user requested abort!"}
2025-08-27 14:52:19,881 p=19789 u=root n=ansible INFO| PLAY RECAP **************************************************************************************************************************************************************************************************************************************************************
2025-08-27 14:52:19,882 p=19789 u=root n=ansible INFO| localhost                  : ok=0    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-08-27 14:55:33,807 p=20016 u=root n=ansible WARNING| [WARNING]: Found variable using reserved name: namespace

2025-08-27 14:55:33,844 p=20016 u=root n=ansible INFO| PLAY [all] **************************************************************************************************************************************************************************************************************************************************************
2025-08-27 14:55:33,851 p=20016 u=root n=ansible INFO| TASK [Gathering Facts] **************************************************************************************************************************************************************************************************************************************************
2025-08-27 14:55:35,966 p=20016 u=root n=ansible INFO| ok: [rke2-worker2]
2025-08-27 14:55:35,984 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 14:55:36,005 p=20016 u=root n=ansible INFO| ok: [rke2-worker1]
2025-08-27 14:55:36,009 p=20016 u=root n=ansible INFO| ok: [rke2-worker3]
2025-08-27 14:55:36,030 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:55:36,799 p=20016 u=root n=ansible INFO| ok: [localhost]
2025-08-27 14:55:37,680 p=20016 u=root n=ansible INFO| ok: [nfs-server-01]
2025-08-27 14:55:37,736 p=20016 u=root n=ansible INFO| ok: [nginx]
2025-08-27 14:55:37,744 p=20016 u=root n=ansible INFO| TASK [create_disk : Gather facts to access ansible_devices] *************************************************************************************************************************************************************************************************************
2025-08-27 14:55:37,798 p=20016 u=root n=ansible INFO| skipping: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:55:37,813 p=20016 u=root n=ansible INFO| skipping: [rke2-manager2]
2025-08-27 14:55:37,825 p=20016 u=root n=ansible INFO| skipping: [rke2-worker1]
2025-08-27 14:55:37,840 p=20016 u=root n=ansible INFO| skipping: [rke2-worker2]
2025-08-27 14:55:37,856 p=20016 u=root n=ansible INFO| skipping: [rke2-worker3]
2025-08-27 14:55:37,858 p=20016 u=root n=ansible INFO| skipping: [nfs-server-01]
2025-08-27 14:55:37,858 p=20016 u=root n=ansible INFO| skipping: [localhost]
2025-08-27 14:55:37,867 p=20016 u=root n=ansible INFO| skipping: [nginx]
2025-08-27 14:55:37,871 p=20016 u=root n=ansible INFO| TASK [create_disk : Get root device] ************************************************************************************************************************************************************************************************************************************
2025-08-27 14:55:38,399 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:55:38,420 p=20016 u=root n=ansible INFO| changed: [rke2-worker3]
2025-08-27 14:55:38,434 p=20016 u=root n=ansible INFO| changed: [rke2-worker1]
2025-08-27 14:55:38,437 p=20016 u=root n=ansible INFO| changed: [rke2-worker2]
2025-08-27 14:55:38,437 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 14:55:38,626 p=20016 u=root n=ansible INFO| changed: [localhost]
2025-08-27 14:55:38,672 p=20016 u=root n=ansible INFO| changed: [nfs-server-01]
2025-08-27 14:55:38,694 p=20016 u=root n=ansible INFO| changed: [nginx]
2025-08-27 14:55:38,699 p=20016 u=root n=ansible INFO| TASK [create_disk : Extract root block device name] *********************************************************************************************************************************************************************************************************************
2025-08-27 14:55:38,769 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:55:38,796 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 14:55:38,821 p=20016 u=root n=ansible INFO| ok: [rke2-worker1]
2025-08-27 14:55:38,844 p=20016 u=root n=ansible INFO| ok: [rke2-worker2]
2025-08-27 14:55:38,868 p=20016 u=root n=ansible INFO| ok: [rke2-worker3]
2025-08-27 14:55:38,876 p=20016 u=root n=ansible INFO| ok: [nfs-server-01]
2025-08-27 14:55:38,883 p=20016 u=root n=ansible INFO| ok: [localhost]
2025-08-27 14:55:38,900 p=20016 u=root n=ansible INFO| ok: [nginx]
2025-08-27 14:55:38,905 p=20016 u=root n=ansible INFO| TASK [create_disk : Find usable disk with shell command] ****************************************************************************************************************************************************************************************************************
2025-08-27 14:55:39,214 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:55:39,261 p=20016 u=root n=ansible INFO| ok: [rke2-worker1]
2025-08-27 14:55:39,292 p=20016 u=root n=ansible INFO| ok: [rke2-worker2]
2025-08-27 14:55:39,294 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 14:55:39,296 p=20016 u=root n=ansible INFO| ok: [rke2-worker3]
2025-08-27 14:55:39,486 p=20016 u=root n=ansible INFO| ok: [localhost]
2025-08-27 14:55:39,497 p=20016 u=root n=ansible INFO| ok: [nfs-server-01]
2025-08-27 14:55:39,550 p=20016 u=root n=ansible INFO| ok: [nginx]
2025-08-27 14:55:39,555 p=20016 u=root n=ansible INFO| TASK [create_disk : Set usable disk] ************************************************************************************************************************************************************************************************************************************
2025-08-27 14:55:39,571 p=20016 u=root n=ansible INFO| skipping: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:55:39,582 p=20016 u=root n=ansible INFO| skipping: [rke2-manager2]
2025-08-27 14:55:39,594 p=20016 u=root n=ansible INFO| skipping: [rke2-worker1]
2025-08-27 14:55:39,606 p=20016 u=root n=ansible INFO| skipping: [rke2-worker2]
2025-08-27 14:55:39,618 p=20016 u=root n=ansible INFO| skipping: [rke2-worker3]
2025-08-27 14:55:39,629 p=20016 u=root n=ansible INFO| skipping: [nfs-server-01]
2025-08-27 14:55:39,641 p=20016 u=root n=ansible INFO| skipping: [localhost]
2025-08-27 14:55:39,647 p=20016 u=root n=ansible INFO| skipping: [nginx]
2025-08-27 14:55:39,652 p=20016 u=root n=ansible INFO| TASK [create_disk : Skip LVM setup if no usable disk found] *************************************************************************************************************************************************************************************************************
2025-08-27 14:55:39,682 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it] => {
    "msg": "No additional disk found, skipping LVM setup"
}
2025-08-27 14:55:39,697 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "No additional disk found, skipping LVM setup"
}
2025-08-27 14:55:39,733 p=20016 u=root n=ansible INFO| ok: [rke2-worker1] => {
    "msg": "No additional disk found, skipping LVM setup"
}
2025-08-27 14:55:39,746 p=20016 u=root n=ansible INFO| ok: [rke2-worker2] => {
    "msg": "No additional disk found, skipping LVM setup"
}
2025-08-27 14:55:39,761 p=20016 u=root n=ansible INFO| ok: [rke2-worker3] => {
    "msg": "No additional disk found, skipping LVM setup"
}
2025-08-27 14:55:39,774 p=20016 u=root n=ansible INFO| ok: [nfs-server-01] => {
    "msg": "No additional disk found, skipping LVM setup"
}
2025-08-27 14:55:39,777 p=20016 u=root n=ansible INFO| ok: [localhost] => {
    "msg": "No additional disk found, skipping LVM setup"
}
2025-08-27 14:55:39,785 p=20016 u=root n=ansible INFO| ok: [nginx] => {
    "msg": "No additional disk found, skipping LVM setup"
}
2025-08-27 14:55:39,790 p=20016 u=root n=ansible INFO| TASK [create_disk : Show selected disk] *********************************************************************************************************************************************************************************************************************************
2025-08-27 14:55:39,804 p=20016 u=root n=ansible INFO| skipping: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:55:39,816 p=20016 u=root n=ansible INFO| skipping: [rke2-manager2]
2025-08-27 14:55:39,827 p=20016 u=root n=ansible INFO| skipping: [rke2-worker1]
2025-08-27 14:55:39,839 p=20016 u=root n=ansible INFO| skipping: [rke2-worker2]
2025-08-27 14:55:39,850 p=20016 u=root n=ansible INFO| skipping: [rke2-worker3]
2025-08-27 14:55:39,861 p=20016 u=root n=ansible INFO| skipping: [nfs-server-01]
2025-08-27 14:55:39,872 p=20016 u=root n=ansible INFO| skipping: [localhost]
2025-08-27 14:55:39,879 p=20016 u=root n=ansible INFO| skipping: [nginx]
2025-08-27 14:55:39,886 p=20016 u=root n=ansible INFO| TASK [create_disk : Partition disk with parted] *************************************************************************************************************************************************************************************************************************
2025-08-27 14:55:39,911 p=20016 u=root n=ansible INFO| skipping: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:55:39,923 p=20016 u=root n=ansible INFO| skipping: [rke2-manager2]
2025-08-27 14:55:39,935 p=20016 u=root n=ansible INFO| skipping: [rke2-worker1]
2025-08-27 14:55:39,947 p=20016 u=root n=ansible INFO| skipping: [rke2-worker2]
2025-08-27 14:55:39,958 p=20016 u=root n=ansible INFO| skipping: [rke2-worker3]
2025-08-27 14:55:39,970 p=20016 u=root n=ansible INFO| skipping: [nfs-server-01]
2025-08-27 14:55:39,970 p=20016 u=root n=ansible INFO| skipping: [localhost]
2025-08-27 14:55:39,976 p=20016 u=root n=ansible INFO| skipping: [nginx]
2025-08-27 14:55:39,981 p=20016 u=root n=ansible INFO| TASK [create_disk : Set partition path] *********************************************************************************************************************************************************************************************************************************
2025-08-27 14:55:40,005 p=20016 u=root n=ansible INFO| skipping: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:55:40,016 p=20016 u=root n=ansible INFO| skipping: [rke2-manager2]
2025-08-27 14:55:40,031 p=20016 u=root n=ansible INFO| skipping: [rke2-worker1]
2025-08-27 14:55:40,043 p=20016 u=root n=ansible INFO| skipping: [rke2-worker2]
2025-08-27 14:55:40,055 p=20016 u=root n=ansible INFO| skipping: [rke2-worker3]
2025-08-27 14:55:40,067 p=20016 u=root n=ansible INFO| skipping: [nfs-server-01]
2025-08-27 14:55:40,068 p=20016 u=root n=ansible INFO| skipping: [localhost]
2025-08-27 14:55:40,073 p=20016 u=root n=ansible INFO| skipping: [nginx]
2025-08-27 14:55:40,078 p=20016 u=root n=ansible INFO| TASK [create_disk : Create physical volume] *****************************************************************************************************************************************************************************************************************************
2025-08-27 14:55:40,092 p=20016 u=root n=ansible INFO| skipping: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:55:40,103 p=20016 u=root n=ansible INFO| skipping: [rke2-manager2]
2025-08-27 14:55:40,115 p=20016 u=root n=ansible INFO| skipping: [rke2-worker1]
2025-08-27 14:55:40,126 p=20016 u=root n=ansible INFO| skipping: [rke2-worker2]
2025-08-27 14:55:40,138 p=20016 u=root n=ansible INFO| skipping: [rke2-worker3]
2025-08-27 14:55:40,149 p=20016 u=root n=ansible INFO| skipping: [nfs-server-01]
2025-08-27 14:55:40,160 p=20016 u=root n=ansible INFO| skipping: [localhost]
2025-08-27 14:55:40,167 p=20016 u=root n=ansible INFO| skipping: [nginx]
2025-08-27 14:55:40,174 p=20016 u=root n=ansible INFO| TASK [create_disk : Create volume group] ********************************************************************************************************************************************************************************************************************************
2025-08-27 14:55:40,188 p=20016 u=root n=ansible INFO| skipping: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:55:40,200 p=20016 u=root n=ansible INFO| skipping: [rke2-manager2]
2025-08-27 14:55:40,211 p=20016 u=root n=ansible INFO| skipping: [rke2-worker1]
2025-08-27 14:55:40,223 p=20016 u=root n=ansible INFO| skipping: [rke2-worker2]
2025-08-27 14:55:40,257 p=20016 u=root n=ansible INFO| skipping: [rke2-worker3]
2025-08-27 14:55:40,268 p=20016 u=root n=ansible INFO| skipping: [nfs-server-01]
2025-08-27 14:55:40,272 p=20016 u=root n=ansible INFO| skipping: [localhost]
2025-08-27 14:55:40,277 p=20016 u=root n=ansible INFO| skipping: [nginx]
2025-08-27 14:55:40,281 p=20016 u=root n=ansible INFO| TASK [create_disk : Create logical volume] ******************************************************************************************************************************************************************************************************************************
2025-08-27 14:55:40,295 p=20016 u=root n=ansible INFO| skipping: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:55:40,306 p=20016 u=root n=ansible INFO| skipping: [rke2-manager2]
2025-08-27 14:55:40,317 p=20016 u=root n=ansible INFO| skipping: [rke2-worker1]
2025-08-27 14:55:40,331 p=20016 u=root n=ansible INFO| skipping: [rke2-worker2]
2025-08-27 14:55:40,342 p=20016 u=root n=ansible INFO| skipping: [rke2-worker3]
2025-08-27 14:55:40,354 p=20016 u=root n=ansible INFO| skipping: [nfs-server-01]
2025-08-27 14:55:40,366 p=20016 u=root n=ansible INFO| skipping: [localhost]
2025-08-27 14:55:40,372 p=20016 u=root n=ansible INFO| skipping: [nginx]
2025-08-27 14:55:40,378 p=20016 u=root n=ansible INFO| TASK [create_disk : Format logical volume as xfs] ***********************************************************************************************************************************************************************************************************************
2025-08-27 14:55:40,391 p=20016 u=root n=ansible INFO| skipping: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:55:40,402 p=20016 u=root n=ansible INFO| skipping: [rke2-manager2]
2025-08-27 14:55:40,413 p=20016 u=root n=ansible INFO| skipping: [rke2-worker1]
2025-08-27 14:55:40,424 p=20016 u=root n=ansible INFO| skipping: [rke2-worker2]
2025-08-27 14:55:40,435 p=20016 u=root n=ansible INFO| skipping: [rke2-worker3]
2025-08-27 14:55:40,446 p=20016 u=root n=ansible INFO| skipping: [nfs-server-01]
2025-08-27 14:55:40,457 p=20016 u=root n=ansible INFO| skipping: [localhost]
2025-08-27 14:55:40,463 p=20016 u=root n=ansible INFO| skipping: [nginx]
2025-08-27 14:55:40,470 p=20016 u=root n=ansible INFO| TASK [create_disk : Ensure mount point /var/lib/rancher exists] *********************************************************************************************************************************************************************************************************
2025-08-27 14:55:40,921 p=20016 u=root n=ansible INFO| ok: [rke2-worker1]
2025-08-27 14:55:40,949 p=20016 u=root n=ansible INFO| ok: [rke2-worker2]
2025-08-27 14:55:40,963 p=20016 u=root n=ansible INFO| ok: [rke2-worker3]
2025-08-27 14:55:40,965 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:55:40,965 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 14:55:41,165 p=20016 u=root n=ansible INFO| ok: [localhost]
2025-08-27 14:55:41,203 p=20016 u=root n=ansible INFO| ok: [nfs-server-01]
2025-08-27 14:55:41,228 p=20016 u=root n=ansible INFO| ok: [nginx]
2025-08-27 14:55:41,234 p=20016 u=root n=ansible INFO| TASK [create_disk : Get UUID of logical volume] *************************************************************************************************************************************************************************************************************************
2025-08-27 14:55:41,248 p=20016 u=root n=ansible INFO| skipping: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:55:41,259 p=20016 u=root n=ansible INFO| skipping: [rke2-manager2]
2025-08-27 14:55:41,270 p=20016 u=root n=ansible INFO| skipping: [rke2-worker1]
2025-08-27 14:55:41,283 p=20016 u=root n=ansible INFO| skipping: [rke2-worker2]
2025-08-27 14:55:41,295 p=20016 u=root n=ansible INFO| skipping: [rke2-worker3]
2025-08-27 14:55:41,306 p=20016 u=root n=ansible INFO| skipping: [nfs-server-01]
2025-08-27 14:55:41,319 p=20016 u=root n=ansible INFO| skipping: [localhost]
2025-08-27 14:55:41,325 p=20016 u=root n=ansible INFO| skipping: [nginx]
2025-08-27 14:55:41,330 p=20016 u=root n=ansible INFO| TASK [create_disk : Add mount to /etc/fstab] ****************************************************************************************************************************************************************************************************************************
2025-08-27 14:55:41,343 p=20016 u=root n=ansible INFO| skipping: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:55:41,354 p=20016 u=root n=ansible INFO| skipping: [rke2-manager2]
2025-08-27 14:55:41,365 p=20016 u=root n=ansible INFO| skipping: [rke2-worker1]
2025-08-27 14:55:41,376 p=20016 u=root n=ansible INFO| skipping: [rke2-worker2]
2025-08-27 14:55:41,387 p=20016 u=root n=ansible INFO| skipping: [rke2-worker3]
2025-08-27 14:55:41,398 p=20016 u=root n=ansible INFO| skipping: [nfs-server-01]
2025-08-27 14:55:41,408 p=20016 u=root n=ansible INFO| skipping: [localhost]
2025-08-27 14:55:41,414 p=20016 u=root n=ansible INFO| skipping: [nginx]
2025-08-27 14:55:41,421 p=20016 u=root n=ansible INFO| TASK [create_disk : Mount volume] ***************************************************************************************************************************************************************************************************************************************
2025-08-27 14:55:41,434 p=20016 u=root n=ansible INFO| skipping: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:55:41,446 p=20016 u=root n=ansible INFO| skipping: [rke2-manager2]
2025-08-27 14:55:41,457 p=20016 u=root n=ansible INFO| skipping: [rke2-worker1]
2025-08-27 14:55:41,468 p=20016 u=root n=ansible INFO| skipping: [rke2-worker2]
2025-08-27 14:55:41,479 p=20016 u=root n=ansible INFO| skipping: [rke2-worker3]
2025-08-27 14:55:41,490 p=20016 u=root n=ansible INFO| skipping: [nfs-server-01]
2025-08-27 14:55:41,501 p=20016 u=root n=ansible INFO| skipping: [localhost]
2025-08-27 14:55:41,507 p=20016 u=root n=ansible INFO| skipping: [nginx]
2025-08-27 14:55:41,537 p=20016 u=root n=ansible INFO| PLAY [all,local] ********************************************************************************************************************************************************************************************************************************************************
2025-08-27 14:55:41,543 p=20016 u=root n=ansible INFO| TASK [update_hosts_file : Get master node IP from inventory] ************************************************************************************************************************************************************************************************************
2025-08-27 14:55:41,559 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:55:41,564 p=20016 u=root n=ansible INFO| TASK [update_hosts_file : Debug values] *********************************************************************************************************************************************************************************************************************************
2025-08-27 14:55:41,589 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it] => {
    "msg": "Rancher Domain: ansible.app.iac-svil.almaviva.it\nMaster IP: 10.205.166.216\nCurrent Host: cornelio.app.iac-svil.almaviva.it\n"
}
2025-08-27 14:55:41,600 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "Rancher Domain: ansible.app.iac-svil.almaviva.it\nMaster IP: 10.205.166.216\nCurrent Host: rke2-manager2\n"
}
2025-08-27 14:55:41,616 p=20016 u=root n=ansible INFO| ok: [rke2-worker1] => {
    "msg": "Rancher Domain: ansible.app.iac-svil.almaviva.it\nMaster IP: 10.205.166.216\nCurrent Host: rke2-worker1\n"
}
2025-08-27 14:55:41,630 p=20016 u=root n=ansible INFO| ok: [rke2-worker2] => {
    "msg": "Rancher Domain: ansible.app.iac-svil.almaviva.it\nMaster IP: 10.205.166.216\nCurrent Host: rke2-worker2\n"
}
2025-08-27 14:55:41,643 p=20016 u=root n=ansible INFO| ok: [rke2-worker3] => {
    "msg": "Rancher Domain: ansible.app.iac-svil.almaviva.it\nMaster IP: 10.205.166.216\nCurrent Host: rke2-worker3\n"
}
2025-08-27 14:55:41,659 p=20016 u=root n=ansible INFO| ok: [nfs-server-01] => {
    "msg": "Rancher Domain: ansible.app.iac-svil.almaviva.it\nMaster IP: 10.205.166.216\nCurrent Host: nfs-server-01\n"
}
2025-08-27 14:55:41,659 p=20016 u=root n=ansible INFO| ok: [localhost] => {
    "msg": "Rancher Domain: ansible.app.iac-svil.almaviva.it\nMaster IP: 10.205.166.216\nCurrent Host: localhost\n"
}
2025-08-27 14:55:41,666 p=20016 u=root n=ansible INFO| ok: [nginx] => {
    "msg": "Rancher Domain: ansible.app.iac-svil.almaviva.it\nMaster IP: 10.205.166.216\nCurrent Host: nginx\n"
}
2025-08-27 14:55:41,671 p=20016 u=root n=ansible INFO| TASK [update_hosts_file : Remove old entries for rancher domain from /etc/hosts] ****************************************************************************************************************************************************************************************
2025-08-27 14:55:42,179 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:55:42,209 p=20016 u=root n=ansible INFO| ok: [rke2-worker2]
2025-08-27 14:55:42,222 p=20016 u=root n=ansible INFO| ok: [rke2-worker3]
2025-08-27 14:55:42,222 p=20016 u=root n=ansible INFO| ok: [rke2-worker1]
2025-08-27 14:55:42,225 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 14:55:42,460 p=20016 u=root n=ansible INFO| [0;31m--- before: /etc/hosts (content)[0m
[0;31m[0m[0;32m+++ after: /etc/hosts (content)[0m
[0;32m[0m[0;36m@@ -31,4 +31,3 @@[0m
[0;36m[0m ff02::1 ip6-allnodes
 ff02::2 ip6-allrouters
 ff02::3 ip6-allhosts
[0;31m-10.205.166.216 ansible.app.iac-svil.almaviva.it[0m
[0;31m[0m

2025-08-27 14:55:42,460 p=20016 u=root n=ansible INFO| changed: [localhost]
2025-08-27 14:55:42,479 p=20016 u=root n=ansible INFO| ok: [nfs-server-01]
2025-08-27 14:55:42,514 p=20016 u=root n=ansible INFO| ok: [nginx]
2025-08-27 14:55:42,519 p=20016 u=root n=ansible INFO| TASK [update_hosts_file : Remove old entries by IP (cleanup any existing entries with the master IP)] *******************************************************************************************************************************************************************
2025-08-27 14:55:42,880 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:55:42,950 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 14:55:42,976 p=20016 u=root n=ansible INFO| ok: [rke2-worker1]
2025-08-27 14:55:42,980 p=20016 u=root n=ansible INFO| ok: [rke2-worker2]
2025-08-27 14:55:42,985 p=20016 u=root n=ansible INFO| ok: [rke2-worker3]
2025-08-27 14:55:43,167 p=20016 u=root n=ansible INFO| ok: [nfs-server-01]
2025-08-27 14:55:43,279 p=20016 u=root n=ansible INFO| ok: [localhost]
2025-08-27 14:55:43,302 p=20016 u=root n=ansible INFO| ok: [nginx]
2025-08-27 14:55:43,308 p=20016 u=root n=ansible INFO| TASK [update_hosts_file : Add new entry for rancher domain in /etc/hosts] ***********************************************************************************************************************************************************************************************
2025-08-27 14:55:43,656 p=20016 u=root n=ansible INFO| [0;31m--- before: /etc/hosts (content)[0m
[0;31m[0m[0;32m+++ after: /etc/hosts (content)[0m
[0;32m[0m[0;36m@@ -6,3 +6,4 @@[0m
[0;36m[0m ff02::1 ip6-allnodes
 ff02::2 ip6-allrouters
 ff02::3 ip6-allhosts
[0;32m+10.205.166.216 ansible.app.iac-svil.almaviva.it[0m
[0;32m[0m

2025-08-27 14:55:43,657 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:55:43,712 p=20016 u=root n=ansible INFO| [0;31m--- before: /etc/hosts (content)[0m
[0;31m[0m[0;32m+++ after: /etc/hosts (content)[0m
[0;32m[0m[0;36m@@ -6,3 +6,4 @@[0m
[0;36m[0m ff02::1 ip6-allnodes
 ff02::2 ip6-allrouters
 ff02::3 ip6-allhosts
[0;32m+10.205.166.216 ansible.app.iac-svil.almaviva.it[0m
[0;32m[0m

2025-08-27 14:55:43,719 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 14:55:43,754 p=20016 u=root n=ansible INFO| [0;31m--- before: /etc/hosts (content)[0m
[0;31m[0m[0;32m+++ after: /etc/hosts (content)[0m
[0;32m[0m[0;36m@@ -6,3 +6,4 @@[0m
[0;36m[0m ff02::1 ip6-allnodes
 ff02::2 ip6-allrouters
 ff02::3 ip6-allhosts
[0;32m+10.205.166.216 ansible.app.iac-svil.almaviva.it[0m
[0;32m[0m

2025-08-27 14:55:43,754 p=20016 u=root n=ansible INFO| changed: [rke2-worker1]
2025-08-27 14:55:43,758 p=20016 u=root n=ansible INFO| [0;31m--- before: /etc/hosts (content)[0m
[0;31m[0m[0;32m+++ after: /etc/hosts (content)[0m
[0;32m[0m[0;36m@@ -6,3 +6,4 @@[0m
[0;36m[0m ff02::1 ip6-allnodes
 ff02::2 ip6-allrouters
 ff02::3 ip6-allhosts
[0;32m+10.205.166.216 ansible.app.iac-svil.almaviva.it[0m
[0;32m[0m

2025-08-27 14:55:43,758 p=20016 u=root n=ansible INFO| changed: [rke2-worker2]
2025-08-27 14:55:43,787 p=20016 u=root n=ansible INFO| [0;31m--- before: /etc/hosts (content)[0m
[0;31m[0m[0;32m+++ after: /etc/hosts (content)[0m
[0;32m[0m[0;36m@@ -6,3 +6,4 @@[0m
[0;36m[0m ff02::1 ip6-allnodes
 ff02::2 ip6-allrouters
 ff02::3 ip6-allhosts
[0;32m+10.205.166.216 ansible.app.iac-svil.almaviva.it[0m
[0;32m[0m

2025-08-27 14:55:43,787 p=20016 u=root n=ansible INFO| changed: [rke2-worker3]
2025-08-27 14:55:44,027 p=20016 u=root n=ansible INFO| [0;31m--- before: /etc/hosts (content)[0m
[0;31m[0m[0;32m+++ after: /etc/hosts (content)[0m
[0;32m[0m[0;36m@@ -6,3 +6,4 @@[0m
[0;36m[0m ff02::1 ip6-allnodes
 ff02::2 ip6-allrouters
 ff02::3 ip6-allhosts
[0;32m+10.205.166.216 ansible.app.iac-svil.almaviva.it[0m
[0;32m[0m

2025-08-27 14:55:44,027 p=20016 u=root n=ansible INFO| changed: [nfs-server-01]
2025-08-27 14:55:44,038 p=20016 u=root n=ansible INFO| [0;31m--- before: /etc/hosts (content)[0m
[0;31m[0m[0;32m+++ after: /etc/hosts (content)[0m
[0;32m[0m[0;36m@@ -31,3 +31,4 @@[0m
[0;36m[0m ff02::1 ip6-allnodes
 ff02::2 ip6-allrouters
 ff02::3 ip6-allhosts
[0;32m+10.205.166.216 ansible.app.iac-svil.almaviva.it[0m
[0;32m[0m

2025-08-27 14:55:44,039 p=20016 u=root n=ansible INFO| changed: [localhost]
2025-08-27 14:55:44,107 p=20016 u=root n=ansible INFO| [0;31m--- before: /etc/hosts (content)[0m
[0;31m[0m[0;32m+++ after: /etc/hosts (content)[0m
[0;32m[0m[0;36m@@ -6,3 +6,4 @@[0m
[0;36m[0m ff02::1 ip6-allnodes
 ff02::2 ip6-allrouters
 ff02::3 ip6-allhosts
[0;32m+10.205.166.216 ansible.app.iac-svil.almaviva.it[0m
[0;32m[0m

2025-08-27 14:55:44,107 p=20016 u=root n=ansible INFO| changed: [nginx]
2025-08-27 14:55:44,138 p=20016 u=root n=ansible INFO| TASK [update_hosts_file : Verify /etc/hosts entry] **********************************************************************************************************************************************************************************************************************
2025-08-27 14:55:44,443 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:55:44,521 p=20016 u=root n=ansible INFO| ok: [rke2-worker1]
2025-08-27 14:55:44,556 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 14:55:44,557 p=20016 u=root n=ansible INFO| ok: [rke2-worker2]
2025-08-27 14:55:44,557 p=20016 u=root n=ansible INFO| ok: [rke2-worker3]
2025-08-27 14:55:44,747 p=20016 u=root n=ansible INFO| ok: [localhost]
2025-08-27 14:55:44,771 p=20016 u=root n=ansible INFO| ok: [nfs-server-01]
2025-08-27 14:55:44,818 p=20016 u=root n=ansible INFO| ok: [nginx]
2025-08-27 14:55:44,826 p=20016 u=root n=ansible INFO| TASK [update_hosts_file : Show hosts file verification] *****************************************************************************************************************************************************************************************************************
2025-08-27 14:55:44,855 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it] => {
    "msg": "Hosts file updated: True\nCurrent entry: 10.205.166.216 ansible.app.iac-svil.almaviva.it\n"
}
2025-08-27 14:55:44,869 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "Hosts file updated: True\nCurrent entry: 10.205.166.216 ansible.app.iac-svil.almaviva.it\n"
}
2025-08-27 14:55:44,884 p=20016 u=root n=ansible INFO| ok: [rke2-worker1] => {
    "msg": "Hosts file updated: True\nCurrent entry: 10.205.166.216 ansible.app.iac-svil.almaviva.it\n"
}
2025-08-27 14:55:44,898 p=20016 u=root n=ansible INFO| ok: [rke2-worker2] => {
    "msg": "Hosts file updated: True\nCurrent entry: 10.205.166.216 ansible.app.iac-svil.almaviva.it\n"
}
2025-08-27 14:55:44,913 p=20016 u=root n=ansible INFO| ok: [rke2-worker3] => {
    "msg": "Hosts file updated: True\nCurrent entry: 10.205.166.216 ansible.app.iac-svil.almaviva.it\n"
}
2025-08-27 14:55:44,926 p=20016 u=root n=ansible INFO| ok: [nfs-server-01] => {
    "msg": "Hosts file updated: True\nCurrent entry: 10.205.166.216 ansible.app.iac-svil.almaviva.it\n"
}
2025-08-27 14:55:44,926 p=20016 u=root n=ansible INFO| ok: [localhost] => {
    "msg": "Hosts file updated: True\nCurrent entry: 10.205.166.216 ansible.app.iac-svil.almaviva.it\n"
}
2025-08-27 14:55:44,933 p=20016 u=root n=ansible INFO| ok: [nginx] => {
    "msg": "Hosts file updated: True\nCurrent entry: 10.205.166.216 ansible.app.iac-svil.almaviva.it\n"
}
2025-08-27 14:55:44,938 p=20016 u=root n=ansible INFO| TASK [update_hosts_file : Test DNS resolution] **************************************************************************************************************************************************************************************************************************
2025-08-27 14:55:45,307 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:55:45,328 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 14:55:45,358 p=20016 u=root n=ansible INFO| ok: [rke2-worker1]
2025-08-27 14:55:45,361 p=20016 u=root n=ansible INFO| ok: [rke2-worker2]
2025-08-27 14:55:45,361 p=20016 u=root n=ansible INFO| fatal: [rke2-worker3]: FAILED! => {"changed": false, "cmd": "nslookup ansible.app.iac-svil.almaviva.it", "delta": "0:00:00.055229", "end": "2025-08-27 14:55:45.312061", "msg": "non-zero return code", "rc": 1, "start": "2025-08-27 14:55:45.256832", "stderr": "", "stderr_lines": [], "stdout": "Server:\t\t127.0.0.53\nAddress:\t127.0.0.53#53\n\n** server can't find ansible.app.iac-svil.almaviva.it: NXDOMAIN", "stdout_lines": ["Server:\t\t127.0.0.53", "Address:\t127.0.0.53#53", "", "** server can't find ansible.app.iac-svil.almaviva.it: NXDOMAIN"]}
2025-08-27 14:55:45,361 p=20016 u=root n=ansible INFO| ...ignoring
2025-08-27 14:55:45,557 p=20016 u=root n=ansible INFO| ok: [localhost]
2025-08-27 14:55:45,640 p=20016 u=root n=ansible INFO| ok: [nfs-server-01]
2025-08-27 14:55:45,661 p=20016 u=root n=ansible INFO| ok: [nginx]
2025-08-27 14:55:45,668 p=20016 u=root n=ansible INFO| TASK [update_hosts_file : Show DNS resolution test] *********************************************************************************************************************************************************************************************************************
2025-08-27 14:55:45,699 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it] => {
    "msg": "DNS Resolution Test:\nServer:\t\t127.0.0.53\nAddress:\t127.0.0.53#53\n\nName:\tansible.app.iac-svil.almaviva.it\nAddress: 10.205.166.216\n"
}
2025-08-27 14:55:45,712 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "DNS Resolution Test:\nServer:\t\t127.0.0.53\nAddress:\t127.0.0.53#53\n\nName:\tansible.app.iac-svil.almaviva.it\nAddress: 10.205.166.216\n"
}
2025-08-27 14:55:45,725 p=20016 u=root n=ansible INFO| ok: [rke2-worker1] => {
    "msg": "DNS Resolution Test:\nServer:\t\t127.0.0.53\nAddress:\t127.0.0.53#53\n\nName:\tansible.app.iac-svil.almaviva.it\nAddress: 10.205.166.216\n"
}
2025-08-27 14:55:45,739 p=20016 u=root n=ansible INFO| ok: [rke2-worker2] => {
    "msg": "DNS Resolution Test:\nServer:\t\t127.0.0.53\nAddress:\t127.0.0.53#53\n\nName:\tansible.app.iac-svil.almaviva.it\nAddress: 10.205.166.216\n"
}
2025-08-27 14:55:45,750 p=20016 u=root n=ansible INFO| ok: [rke2-worker3] => {
    "msg": "DNS Resolution Test:\nDNS resolution failed, using /etc/hosts entry\n"
}
2025-08-27 14:55:45,763 p=20016 u=root n=ansible INFO| ok: [nfs-server-01] => {
    "msg": "DNS Resolution Test:\nServer:\t\t127.0.0.53\nAddress:\t127.0.0.53#53\n\nName:\tansible.app.iac-svil.almaviva.it\nAddress: 10.205.166.216\n"
}
2025-08-27 14:55:45,766 p=20016 u=root n=ansible INFO| ok: [localhost] => {
    "msg": "DNS Resolution Test:\nServer:\t\t127.0.0.53\nAddress:\t127.0.0.53#53\n\nName:\tansible.app.iac-svil.almaviva.it\nAddress: 10.205.166.216\n"
}
2025-08-27 14:55:45,774 p=20016 u=root n=ansible INFO| ok: [nginx] => {
    "msg": "DNS Resolution Test:\nServer:\t\t127.0.0.53\nAddress:\t127.0.0.53#53\n\nName:\tansible.app.iac-svil.almaviva.it\nAddress: 10.205.166.216\n"
}
2025-08-27 14:55:45,779 p=20016 u=root n=ansible INFO| TASK [update_hosts_file : Test connectivity to rancher domain] **********************************************************************************************************************************************************************************************************
2025-08-27 14:55:47,136 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:55:47,155 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 14:55:47,180 p=20016 u=root n=ansible INFO| ok: [rke2-worker1]
2025-08-27 14:55:47,186 p=20016 u=root n=ansible INFO| ok: [rke2-worker2]
2025-08-27 14:55:47,199 p=20016 u=root n=ansible INFO| ok: [rke2-worker3]
2025-08-27 14:55:48,464 p=20016 u=root n=ansible INFO| ok: [nfs-server-01]
2025-08-27 14:55:48,505 p=20016 u=root n=ansible INFO| ok: [nginx]
2025-08-27 14:55:58,446 p=20016 u=root n=ansible INFO| fatal: [localhost]: FAILED! => {"changed": false, "cmd": "ping -c 2 ansible.app.iac-svil.almaviva.it", "delta": "0:00:11.067688", "end": "2025-08-27 14:55:58.426537", "msg": "non-zero return code", "rc": 1, "start": "2025-08-27 14:55:47.358849", "stderr": "", "stderr_lines": [], "stdout": "PING ansible.app.iac-svil.almaviva.it (10.205.166.216) 56(84) bytes of data.\n\n--- ansible.app.iac-svil.almaviva.it ping statistics ---\n2 packets transmitted, 0 received, 100% packet loss, time 1064ms", "stdout_lines": ["PING ansible.app.iac-svil.almaviva.it (10.205.166.216) 56(84) bytes of data.", "", "--- ansible.app.iac-svil.almaviva.it ping statistics ---", "2 packets transmitted, 0 received, 100% packet loss, time 1064ms"]}
2025-08-27 14:55:58,446 p=20016 u=root n=ansible INFO| ...ignoring
2025-08-27 14:55:58,452 p=20016 u=root n=ansible INFO| TASK [update_hosts_file : Show connectivity test] ***********************************************************************************************************************************************************************************************************************
2025-08-27 14:55:58,480 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it] => {
    "msg": "Connectivity Test:\nSUCCESS - Can reach ansible.app.iac-svil.almaviva.it\n"
}
2025-08-27 14:55:58,495 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "Connectivity Test:\nSUCCESS - Can reach ansible.app.iac-svil.almaviva.it\n"
}
2025-08-27 14:55:58,509 p=20016 u=root n=ansible INFO| ok: [rke2-worker1] => {
    "msg": "Connectivity Test:\nSUCCESS - Can reach ansible.app.iac-svil.almaviva.it\n"
}
2025-08-27 14:55:58,522 p=20016 u=root n=ansible INFO| ok: [rke2-worker2] => {
    "msg": "Connectivity Test:\nSUCCESS - Can reach ansible.app.iac-svil.almaviva.it\n"
}
2025-08-27 14:55:58,534 p=20016 u=root n=ansible INFO| ok: [rke2-worker3] => {
    "msg": "Connectivity Test:\nSUCCESS - Can reach ansible.app.iac-svil.almaviva.it\n"
}
2025-08-27 14:55:58,549 p=20016 u=root n=ansible INFO| ok: [nfs-server-01] => {
    "msg": "Connectivity Test:\nSUCCESS - Can reach ansible.app.iac-svil.almaviva.it\n"
}
2025-08-27 14:55:58,550 p=20016 u=root n=ansible INFO| ok: [localhost] => {
    "msg": "Connectivity Test:\nFAILED - Cannot reach ansible.app.iac-svil.almaviva.it\n"
}
2025-08-27 14:55:58,558 p=20016 u=root n=ansible INFO| ok: [nginx] => {
    "msg": "Connectivity Test:\nSUCCESS - Can reach ansible.app.iac-svil.almaviva.it\n"
}
2025-08-27 14:55:58,587 p=20016 u=root n=ansible INFO| PLAY [masters] **********************************************************************************************************************************************************************************************************************************************************
2025-08-27 14:55:58,594 p=20016 u=root n=ansible INFO| TASK [master1_install : Install RKE2 on master1] ************************************************************************************************************************************************************************************************************************
2025-08-27 14:56:01,954 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:56:01,959 p=20016 u=root n=ansible INFO| TASK [master1_install : Enable RKE2 server service] *********************************************************************************************************************************************************************************************************************
2025-08-27 14:56:02,818 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:56:02,825 p=20016 u=root n=ansible INFO| TASK [master1_install : Start RKE2 server service] **********************************************************************************************************************************************************************************************************************
2025-08-27 14:56:31,926 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:56:31,934 p=20016 u=root n=ansible INFO| TASK [master1_config : Create /etc/rancher/rke2/config.yaml on master1] *************************************************************************************************************************************************************************************************
2025-08-27 14:56:32,874 p=20016 u=root n=ansible INFO| [0;31m--- before[0m
[0;31m[0m[0;32m+++ after: /root/.ansible/tmp/ansible-local-20016_chgpspt/tmpqjv6g0c5/config_master1.yaml.j2[0m
[0;32m[0m[0;36m@@ -0,0 +1,2 @@[0m
[0;36m[0m[0;32m+tls-san:[0m
[0;32m[0m[0;32m+   - ansible.app.iac-svil.almaviva.it[0m
[0;32m[0m

2025-08-27 14:56:32,874 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:56:32,883 p=20016 u=root n=ansible INFO| TASK [master1_kubectl : Copy kubectl binary to /usr/local/bin/] *********************************************************************************************************************************************************************************************************
2025-08-27 14:56:33,533 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:56:33,538 p=20016 u=root n=ansible INFO| TASK [master1_kubectl : Create /root/.kube directory] *******************************************************************************************************************************************************************************************************************
2025-08-27 14:56:33,776 p=20016 u=root n=ansible INFO| [0;31m--- before[0m
[0;31m[0m[0;32m+++ after[0m
[0;32m[0m[0;36m@@ -1,4 +1,4 @@[0m
[0;36m[0m {
     "path": "/root/.kube",
[0;31m-    "state": "absent"[0m
[0;31m[0m[0;32m+    "state": "directory"[0m
[0;32m[0m }


2025-08-27 14:56:33,776 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:56:33,781 p=20016 u=root n=ansible INFO| TASK [master1_kubectl : Copy RKE2 kubeconfig to /root/.kube/config] *****************************************************************************************************************************************************************************************************
2025-08-27 14:56:34,014 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:56:34,023 p=20016 u=root n=ansible INFO| TASK [master1_helm_cert_manager_install : Download Helm] ****************************************************************************************************************************************************************************************************************
2025-08-27 14:56:34,766 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:56:34,772 p=20016 u=root n=ansible INFO| TASK [master1_helm_cert_manager_install : Extract Helm] *****************************************************************************************************************************************************************************************************************
2025-08-27 14:56:36,547 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:56:36,552 p=20016 u=root n=ansible INFO| TASK [master1_helm_cert_manager_install : Move Helm to /usr/local/bin] **************************************************************************************************************************************************************************************************
2025-08-27 14:56:36,782 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:56:36,787 p=20016 u=root n=ansible INFO| TASK [master1_helm_cert_manager_install : Copy kubectl binary to /usr/local/bin/] ***************************************************************************************************************************************************************************************
2025-08-27 14:56:37,258 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:56:37,267 p=20016 u=root n=ansible INFO| TASK [master1_helm_cert_manager_install : Create /root/.kube directory] *************************************************************************************************************************************************************************************************
2025-08-27 14:56:37,516 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:56:37,522 p=20016 u=root n=ansible INFO| TASK [master1_helm_cert_manager_install : Copy RKE2 kubeconfig to /root/.kube/config] ***********************************************************************************************************************************************************************************
2025-08-27 14:56:37,756 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:56:37,765 p=20016 u=root n=ansible INFO| TASK [master1_helm_cert_manager_install : include_tasks] ****************************************************************************************************************************************************************************************************************
2025-08-27 14:56:37,786 p=20016 u=root n=ansible INFO| included: /root/IAC/COMPLETO-8-server/roles/master1_helm_cert_manager_install/tasks/master1_helm_cert_manager_install.yml for cornelio.app.iac-svil.almaviva.it
2025-08-27 14:56:37,790 p=20016 u=root n=ansible INFO| TASK [master1_helm_cert_manager_install : Add Rancher Helm repository] **************************************************************************************************************************************************************************************************
2025-08-27 14:56:38,107 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:56:38,112 p=20016 u=root n=ansible INFO| TASK [master1_helm_cert_manager_install : Create cattle-system namespace] ***********************************************************************************************************************************************************************************************
2025-08-27 14:56:38,425 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:56:38,430 p=20016 u=root n=ansible INFO| TASK [master1_helm_cert_manager_install : Update Helm repositories] *****************************************************************************************************************************************************************************************************
2025-08-27 14:56:39,134 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:56:39,140 p=20016 u=root n=ansible INFO| TASK [master1_helm_cert_manager_install : Set kube permission] **********************************************************************************************************************************************************************************************************
2025-08-27 14:56:39,381 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:56:39,388 p=20016 u=root n=ansible INFO| TASK [master1_helm_cert_manager_install : Copy fullchain.pem to remote host] ********************************************************************************************************************************************************************************************
2025-08-27 14:56:39,858 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:56:39,863 p=20016 u=root n=ansible INFO| TASK [master1_helm_cert_manager_install : Copy privkey.pem to remote host] **********************************************************************************************************************************************************************************************
2025-08-27 14:56:40,398 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:56:40,404 p=20016 u=root n=ansible INFO| TASK [master1_helm_cert_manager_install : Copy cacerts.pem to remote host] **********************************************************************************************************************************************************************************************
2025-08-27 14:56:40,998 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:56:41,006 p=20016 u=root n=ansible INFO| TASK [master1_helm_cert_manager_install : Create TLS secret for Rancher ingress] ****************************************************************************************************************************************************************************************
2025-08-27 14:56:41,668 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:56:41,673 p=20016 u=root n=ansible INFO| TASK [master1_helm_cert_manager_install : Create CA secret for Rancher] *************************************************************************************************************************************************************************************************
2025-08-27 14:56:42,147 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:56:42,153 p=20016 u=root n=ansible INFO| TASK [master1_helm_cert_manager_install : Verify secrets creation] ******************************************************************************************************************************************************************************************************
2025-08-27 14:56:42,760 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:56:42,765 p=20016 u=root n=ansible INFO| TASK [master1_helm_cert_manager_install : Install Rancher with Helm] ****************************************************************************************************************************************************************************************************
2025-08-27 14:56:45,057 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:56:45,062 p=20016 u=root n=ansible INFO| TASK [master1_helm_cert_manager_install : Wait for Rancher to be ready] *************************************************************************************************************************************************************************************************
2025-08-27 14:58:55,553 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:58:55,558 p=20016 u=root n=ansible INFO| TASK [master1_helm_cert_manager_install : Check Rancher pod status] *****************************************************************************************************************************************************************************************************
2025-08-27 14:58:56,182 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:58:56,187 p=20016 u=root n=ansible INFO| TASK [master1_helm_cert_manager_install : Display Rancher pod status] ***************************************************************************************************************************************************************************************************
2025-08-27 14:58:56,199 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it] => {
    "rancher_pods.stdout_lines": [
        "NAME                       READY   STATUS    RESTARTS      AGE",
        "rancher-6bd7dbdb77-5vw2w   1/1     Running   0             2m12s",
        "rancher-6bd7dbdb77-hwwj5   1/1     Running   2 (36s ago)   2m12s",
        "rancher-6bd7dbdb77-stw4r   1/1     Running   2 (49s ago)   2m12s"
    ]
}
2025-08-27 14:58:56,204 p=20016 u=root n=ansible INFO| TASK [master1_helm_cert_manager_install : Check Rancher service status] *************************************************************************************************************************************************************************************************
2025-08-27 14:58:56,758 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 14:58:56,764 p=20016 u=root n=ansible INFO| TASK [master1_helm_cert_manager_install : Display Rancher service status] ***********************************************************************************************************************************************************************************************
2025-08-27 14:58:56,776 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it] => {
    "rancher_svc.stdout_lines": [
        "NAME      TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE",
        "rancher   ClusterIP   10.43.7.115   <none>        80/TCP,443/TCP   2m12s"
    ]
}
2025-08-27 14:58:56,781 p=20016 u=root n=ansible INFO| PLAY [masters] **********************************************************************************************************************************************************************************************************************************************************
2025-08-27 14:58:56,788 p=20016 u=root n=ansible INFO| TASK [master1_create_cluster : Wait for Rancher cluster to be fully ready] **********************************************************************************************************************************************************************************************
2025-08-27 14:58:56,804 p=20016 u=root n=ansible INFO| Pausing for 280 seconds
2025-08-27 14:58:56,805 p=20016 u=root n=ansible INFO| (ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
2025-08-27 15:03:36,807 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it -> localhost]
2025-08-27 15:03:36,815 p=20016 u=root n=ansible INFO| TASK [master1_create_cluster : Get Rancher API token] *******************************************************************************************************************************************************************************************************************
2025-08-27 15:03:37,272 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it -> localhost]
2025-08-27 15:03:37,283 p=20016 u=root n=ansible INFO| TASK [master1_create_cluster : Set Rancher server URL (completa setup iniziale)] ****************************************************************************************************************************************************************************************
2025-08-27 15:03:37,593 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it -> localhost]
2025-08-27 15:03:37,598 p=20016 u=root n=ansible INFO| TASK [master1_create_cluster : Accept Rancher EULA] *********************************************************************************************************************************************************************************************************************
2025-08-27 15:03:37,887 p=20016 u=root n=ansible INFO| fatal: [cornelio.app.iac-svil.almaviva.it -> localhost]: FAILED! => {"cache_control": "no-cache, no-store, must-revalidate", "changed": false, "connection": "close", "content_length": "111", "content_type": "application/json", "date": "Wed, 27 Aug 2025 13:03:37 GMT", "elapsed": 0, "expires": "Wed 24 Feb 1982 18:42:00 GMT", "json": {"baseType": "error", "code": "MethodNotAllow", "message": "Method POST not supported", "status": 405, "type": "error"}, "msg": "Status code was 405 and not [200, 204, 409]: HTTP Error 405: Method Not Allowed", "redirected": false, "status": 405, "strict_transport_security": "max-age=31536000; includeSubDomains", "url": "https://ansible.app.iac-svil.almaviva.it/v3/setting/first-login", "x_api_cattle_auth": "true", "x_api_schemas": "https://ansible.app.iac-svil.almaviva.it/v3/schemas", "x_content_type_options": "nosniff"}
2025-08-27 15:03:37,888 p=20016 u=root n=ansible INFO| ...ignoring
2025-08-27 15:03:37,892 p=20016 u=root n=ansible INFO| TASK [master1_create_cluster : Create a new Kubernetes RKE2 custom cluster via Rancher API] *****************************************************************************************************************************************************************************
2025-08-27 15:03:38,211 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it -> localhost]
2025-08-27 15:03:38,219 p=20016 u=root n=ansible INFO| TASK [master1_create_cluster : Show cluster creation result] ************************************************************************************************************************************************************************************************************
2025-08-27 15:03:38,250 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it] => {
    "create_cluster_result.json": {
        "apiVersion": "provisioning.cattle.io/v1",
        "id": "fleet-default/iacdevops",
        "kind": "Cluster",
        "links": {
            "patch": "https://ansible.app.iac-svil.almaviva.it/v1/provisioning.cattle.io.clusters/fleet-default/iacdevops",
            "remove": "https://ansible.app.iac-svil.almaviva.it/v1/provisioning.cattle.io.clusters/fleet-default/iacdevops",
            "self": "https://ansible.app.iac-svil.almaviva.it/v1/provisioning.cattle.io.clusters/fleet-default/iacdevops",
            "update": "https://ansible.app.iac-svil.almaviva.it/v1/provisioning.cattle.io.clusters/fleet-default/iacdevops",
            "view": "https://ansible.app.iac-svil.almaviva.it/apis/provisioning.cattle.io/v1/namespaces/fleet-default/clusters/iacdevops"
        },
        "metadata": {
            "annotations": {
                "field.cattle.io/creatorId": "user-jjp6z"
            },
            "creationTimestamp": "2025-08-27T13:03:38Z",
            "fields": [
                "iacdevops",
                null,
                null
            ],
            "generation": 1,
            "managedFields": [
                {
                    "apiVersion": "provisioning.cattle.io/v1",
                    "fieldsType": "FieldsV1",
                    "fieldsV1": {
                        "f:spec": {
                            ".": {},
                            "f:defaultClusterRoleForProjectMembers": {},
                            "f:defaultPodSecurityAdmissionConfigurationTemplateName": {},
                            "f:enableNetworkPolicy": {},
                            "f:kubernetesVersion": {},
                            "f:rkeConfig": {
                                ".": {},
                                "f:etcd": {
                                    ".": {},
                                    "f:disableSnapshots": {},
                                    "f:snapshotRetention": {},
                                    "f:snapshotScheduleCron": {}
                                },
                                "f:machineGlobalConfig": {
                                    ".": {},
                                    "f:cni": {},
                                    "f:disable-kube-proxy": {},
                                    "f:etcd-expose-metrics": {}
                                },
                                "f:machineSelectorConfig": {},
                                "f:registries": {},
                                "f:upgradeStrategy": {
                                    ".": {},
                                    "f:controlPlaneConcurrency": {},
                                    "f:controlPlaneDrainOptions": {
                                        ".": {},
                                        "f:enabled": {},
                                        "f:force": {},
                                        "f:ignoreDaemonSets": {},
                                        "f:timeout": {}
                                    },
                                    "f:workerConcurrency": {},
                                    "f:workerDrainOptions": {
                                        ".": {},
                                        "f:enabled": {},
                                        "f:force": {},
                                        "f:ignoreDaemonSets": {},
                                        "f:timeout": {}
                                    }
                                }
                            }
                        }
                    },
                    "manager": "rancher",
                    "operation": "Update",
                    "time": "2025-08-27T13:03:38Z"
                }
            ],
            "name": "iacdevops",
            "namespace": "fleet-default",
            "relationships": null,
            "resourceVersion": "6178",
            "state": {
                "error": false,
                "message": "Resource is current",
                "name": "active",
                "transitioning": false
            },
            "uid": "d1c6c2a4-97ba-4204-94b8-fd276aa63832"
        },
        "spec": {
            "defaultClusterRoleForProjectMembers": "",
            "defaultPodSecurityAdmissionConfigurationTemplateName": "",
            "enableNetworkPolicy": false,
            "kubernetesVersion": "v1.31.11+rke2r1",
            "rkeConfig": {
                "etcd": {
                    "disableSnapshots": false,
                    "snapshotRetention": 5,
                    "snapshotScheduleCron": "0 */5 * * *"
                },
                "machineGlobalConfig": {
                    "cni": "canal",
                    "disable-kube-proxy": false,
                    "etcd-expose-metrics": false
                },
                "machineSelectorConfig": [
                    {
                        "config": {
                            "protect-kernel-defaults": false
                        }
                    }
                ],
                "registries": {},
                "upgradeStrategy": {
                    "controlPlaneConcurrency": "1",
                    "controlPlaneDrainOptions": {
                        "enabled": false,
                        "force": false,
                        "ignoreDaemonSets": true,
                        "timeout": 120
                    },
                    "workerConcurrency": "1",
                    "workerDrainOptions": {
                        "enabled": false,
                        "force": false,
                        "ignoreDaemonSets": true,
                        "timeout": 120
                    }
                }
            }
        },
        "type": "provisioning.cattle.io.cluster"
    }
}
2025-08-27 15:03:38,258 p=20016 u=root n=ansible INFO| TASK [master1_create_cluster : Wait for cluster to be provisioned] ******************************************************************************************************************************************************************************************************
2025-08-27 15:03:38,661 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it -> localhost]
2025-08-27 15:03:38,666 p=20016 u=root n=ansible INFO| TASK [master1_create_cluster : Get cluster ID from legacy API for registration token] ***********************************************************************************************************************************************************************************
2025-08-27 15:03:39,047 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it -> localhost]
2025-08-27 15:03:39,053 p=20016 u=root n=ansible INFO| TASK [master1_create_cluster : Debug cluster info] **********************************************************************************************************************************************************************************************************************
2025-08-27 15:03:39,071 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it] => {
    "legacy_cluster_info.json": {
        "actions": {
            "createFromTemplate": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?action=createFromTemplate"
        },
        "createTypes": {
            "cluster": "https://ansible.app.iac-svil.almaviva.it/v3/clusters"
        },
        "data": [
            {
                "actions": {
                    "generateKubeconfig": "https://ansible.app.iac-svil.almaviva.it/v3/clusters/c-m-rpjjcwlv?action=generateKubeconfig",
                    "importYaml": "https://ansible.app.iac-svil.almaviva.it/v3/clusters/c-m-rpjjcwlv?action=importYaml"
                },
                "agentImage": "",
                "agentImageOverride": "",
                "aksStatus": {
                    "privateRequiresTunnel": null,
                    "rbacEnabled": null,
                    "type": "/v3/schemas/aksStatus",
                    "upstreamSpec": null
                },
                "annotations": {
                    "authz.management.cattle.io/creator-role-bindings": "{\"created\":[\"cluster-owner\"],\"required\":[\"cluster-owner\"]}",
                    "authz.management.cattle.io/initial-sync": "true",
                    "objectset.rio.cattle.io/applied": "H4sIAAAAAAAA/4xS3WrcTAx9lQ9d2072Lz+G76KEXoRCGmhpr+UZeVe745lBI3tJQ969jO0kS0JCr8zIOtI5R+cRMPIvksTBQw0detxSR14rg6qOKg5nwwoKOLC3UMON65OSQAEdKVpUhPoR0PugqBx8ys+WydmTAUYINchtHtAnknK/jxd/oIDQ7MloIq2EwwmAc6eZVpUjmj7pDkdPUm6HA9QQJQycxbDfnkpYFP99Y2//f+X/+TSPHUENjMbSEGL6J0CKaDKqdURaWmqxdwrFR6TQduw5qaBSFqzS08fdr6cpn52xnKLDh/dknwoYXePgf3JHSbGLUPveuQIcNuTGM30kaIdpBzWsGsTFyq5xfXmx2Vxd4fl6tWht22BzjXZB6+vmcrPcNHnbzMCUXSlxvzdHN+RyimTGfGzJ622HW/o+kAjb3AwFoE9HkkwmM55U/SAjpHPNUjLCUad0wlhgIfvlZSDUIOjNjuRs/pbjtnpYVotFtTzB9Lp7Czn0DZUYucRed/VwXi2rdUZMxt69DwF5bBzdkR6DHO6DY/MAdYsuUTGd/XeQw5iDGfw2C9zFIEr2JviWt9mczOH5Bdk19kri0b0MdsGgm5ObVXz1Ngb2mtETITv3PhVwZG/DMd0LtSRknwM//3/6GwAA//8mMjmc8QMAAA",
                    "objectset.rio.cattle.io/id": "cluster-create",
                    "objectset.rio.cattle.io/owner-gvk": "provisioning.cattle.io/v1, Kind=Cluster",
                    "objectset.rio.cattle.io/owner-name": "iacdevops",
                    "objectset.rio.cattle.io/owner-namespace": "fleet-default",
                    "provisioning.cattle.io/administrated": "true",
                    "provisioning.cattle.io/management-cluster-display-name": "iacdevops"
                },
                "answers": {
                    "clusterId": null,
                    "projectId": null,
                    "type": "/v3/schemas/answer"
                },
                "appliedEnableNetworkPolicy": false,
                "appliedSpec": {
                    "agentImageOverride": "",
                    "answers": {
                        "clusterId": null,
                        "projectId": null,
                        "type": "/v3/schemas/answer"
                    },
                    "clusterSecrets": {
                        "type": "/v3/schemas/clusterSecrets"
                    },
                    "clusterTemplateId": null,
                    "clusterTemplateRevisionId": null,
                    "defaultClusterRoleForProjectMembers": null,
                    "description": "",
                    "desiredAgentImage": "",
                    "desiredAuthImage": "",
                    "displayName": "",
                    "dockerRootDir": "/var/lib/docker",
                    "enableNetworkPolicy": null,
                    "internal": false,
                    "localClusterAuthEndpoint": {
                        "enabled": false,
                        "type": "/v3/schemas/localClusterAuthEndpoint"
                    },
                    "type": "/v3/schemas/clusterSpec",
                    "windowsPreferedCluster": false
                },
                "authImage": "",
                "baseType": "cluster",
                "capabilities": {
                    "loadBalancerCapabilities": {
                        "healthCheckSupported": false,
                        "type": "/v3/schemas/loadBalancerCapabilities"
                    },
                    "nodePoolScalingSupported": false,
                    "type": "/v3/schemas/capabilities"
                },
                "clusterSecrets": {
                    "type": "/v3/schemas/clusterSecrets"
                },
                "clusterTemplateId": null,
                "clusterTemplateRevisionId": null,
                "conditions": [
                    {
                        "lastUpdateTime": "2025-08-27T13:03:38Z",
                        "status": "True",
                        "type": "BackingNamespaceCreated"
                    },
                    {
                        "lastUpdateTime": "2025-08-27T13:03:38Z",
                        "status": "True",
                        "type": "DefaultProjectCreated"
                    },
                    {
                        "lastUpdateTime": "2025-08-27T13:03:38Z",
                        "status": "True",
                        "type": "SystemProjectCreated"
                    },
                    {
                        "lastUpdateTime": "2025-08-27T13:03:38Z",
                        "status": "True",
                        "type": "InitialRolesPopulated"
                    }
                ],
                "created": "2025-08-27T13:03:38Z",
                "createdTS": 1756299818000,
                "creatorId": "user-jjp6z",
                "defaultClusterRoleForProjectMembers": null,
                "description": "",
                "desiredAgentImage": "rancher/rancher-agent:v2.11.2",
                "desiredAuthImage": "rancher/kube-api-auth:v0.2.4",
                "dockerRootDir": "/var/lib/docker",
                "driver": "",
                "eksStatus": {
                    "generatedNodeRole": "",
                    "managedLaunchTemplateID": "",
                    "managedLaunchTemplateVersions": null,
                    "privateRequiresTunnel": null,
                    "securityGroups": null,
                    "subnets": null,
                    "type": "/v3/schemas/eksStatus",
                    "upstreamSpec": null,
                    "virtualNetwork": ""
                },
                "enableNetworkPolicy": false,
                "fleetWorkspaceName": "fleet-default",
                "gkeStatus": {
                    "privateRequiresTunnel": null,
                    "type": "/v3/schemas/gkeStatus",
                    "upstreamSpec": null
                },
                "id": "c-m-rpjjcwlv",
                "importedConfig": {
                    "type": "/v3/schemas/importedConfig"
                },
                "internal": false,
                "istioEnabled": false,
                "labels": {
                    "objectset.rio.cattle.io/hash": "3baa13d4a4765588a0431fdfbab9ad1e49b7525b"
                },
                "links": {
                    "apiServices": "https://ansible.app.iac-svil.almaviva.it/v3/clusters/c-m-rpjjcwlv/apiservices",
                    "clusterRegistrationTokens": "https://ansible.app.iac-svil.almaviva.it/v3/clusters/c-m-rpjjcwlv/clusterregistrationtokens",
                    "clusterRoleTemplateBindings": "https://ansible.app.iac-svil.almaviva.it/v3/clusters/c-m-rpjjcwlv/clusterroletemplatebindings",
                    "etcdBackups": "https://ansible.app.iac-svil.almaviva.it/v3/clusters/c-m-rpjjcwlv/etcdbackups",
                    "namespaces": "https://ansible.app.iac-svil.almaviva.it/v3/clusters/c-m-rpjjcwlv/namespaces",
                    "nodePools": "https://ansible.app.iac-svil.almaviva.it/v3/clusters/c-m-rpjjcwlv/nodepools",
                    "nodes": "https://ansible.app.iac-svil.almaviva.it/v3/clusters/c-m-rpjjcwlv/nodes",
                    "persistentVolumes": "https://ansible.app.iac-svil.almaviva.it/v3/clusters/c-m-rpjjcwlv/persistentvolumes",
                    "projects": "https://ansible.app.iac-svil.almaviva.it/v3/clusters/c-m-rpjjcwlv/projects",
                    "remove": "https://ansible.app.iac-svil.almaviva.it/v3/clusters/c-m-rpjjcwlv",
                    "self": "https://ansible.app.iac-svil.almaviva.it/v3/clusters/c-m-rpjjcwlv",
                    "shell": "wss://ansible.app.iac-svil.almaviva.it/v3/clusters/c-m-rpjjcwlv?shell=true",
                    "storageClasses": "https://ansible.app.iac-svil.almaviva.it/v3/clusters/c-m-rpjjcwlv/storageclasses",
                    "subscribe": "https://ansible.app.iac-svil.almaviva.it/v3/clusters/c-m-rpjjcwlv/subscribe",
                    "tokens": "https://ansible.app.iac-svil.almaviva.it/v3/clusters/c-m-rpjjcwlv/tokens",
                    "update": "https://ansible.app.iac-svil.almaviva.it/v3/clusters/c-m-rpjjcwlv"
                },
                "linuxWorkerCount": 0,
                "localClusterAuthEndpoint": {
                    "enabled": false,
                    "type": "/v3/schemas/localClusterAuthEndpoint"
                },
                "name": "iacdevops",
                "nodeCount": 0,
                "nodeVersion": 0,
                "provider": "",
                "state": "active",
                "transitioning": "no",
                "transitioningMessage": "",
                "type": "cluster",
                "uuid": "6453bfe8-608d-46b2-a4ed-13e1f1743255",
                "windowsPreferedCluster": false,
                "windowsWorkerCount": 0
            }
        ],
        "filters": {
            "aadClientCertSecret": null,
            "aadClientSecret": null,
            "agentImage": null,
            "agentImageOverride": null,
            "apiEndpoint": null,
            "appliedEnableNetworkPolicy": null,
            "authImage": null,
            "caCert": null,
            "clusterTemplateId": null,
            "clusterTemplateRevisionId": null,
            "created": null,
            "creatorId": null,
            "currentCisRunName": null,
            "defaultClusterRoleForProjectMembers": null,
            "defaultPodSecurityAdmissionConfigurationTemplateName": null,
            "description": null,
            "desiredAgentImage": null,
            "desiredAuthImage": null,
            "dockerRootDir": null,
            "driver": null,
            "enableNetworkPolicy": null,
            "fleetWorkspaceName": null,
            "id": null,
            "internal": null,
            "istioEnabled": null,
            "linuxWorkerCount": null,
            "name": [
                {
                    "modifier": "eq",
                    "value": "iacdevops"
                }
            ],
            "nodeCount": null,
            "nodeVersion": null,
            "openStackSecret": null,
            "privateRegistrySecret": null,
            "provider": null,
            "removed": null,
            "s3CredentialSecret": null,
            "serviceAccountTokenSecret": null,
            "state": null,
            "transitioning": null,
            "transitioningMessage": null,
            "uuid": null,
            "virtualCenterSecret": null,
            "vsphereSecret": null,
            "weavePasswordSecret": null,
            "windowsPreferedCluster": null,
            "windowsWorkerCount": null
        },
        "links": {
            "self": "https://ansible.app.iac-svil.almaviva.it/v3/clusters"
        },
        "pagination": {
            "limit": 1000,
            "total": 1
        },
        "resourceType": "cluster",
        "sort": {
            "links": {
                "aadClientCertSecret": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&sort=aadClientCertSecret",
                "aadClientSecret": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&sort=aadClientSecret",
                "agentImage": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&sort=agentImage",
                "agentImageOverride": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&sort=agentImageOverride",
                "apiEndpoint": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&sort=apiEndpoint",
                "authImage": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&sort=authImage",
                "caCert": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&sort=caCert",
                "currentCisRunName": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&sort=currentCisRunName",
                "defaultPodSecurityAdmissionConfigurationTemplateName": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&sort=defaultPodSecurityAdmissionConfigurationTemplateName",
                "description": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&sort=description",
                "desiredAgentImage": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&sort=desiredAgentImage",
                "desiredAuthImage": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&sort=desiredAuthImage",
                "dockerRootDir": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&sort=dockerRootDir",
                "driver": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&sort=driver",
                "fleetWorkspaceName": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&sort=fleetWorkspaceName",
                "openStackSecret": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&sort=openStackSecret",
                "privateRegistrySecret": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&sort=privateRegistrySecret",
                "provider": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&sort=provider",
                "s3CredentialSecret": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&sort=s3CredentialSecret",
                "serviceAccountTokenSecret": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&sort=serviceAccountTokenSecret",
                "state": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&sort=state",
                "transitioning": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&sort=transitioning",
                "transitioningMessage": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&sort=transitioningMessage",
                "uuid": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&sort=uuid",
                "virtualCenterSecret": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&sort=virtualCenterSecret",
                "vsphereSecret": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&sort=vsphereSecret",
                "weavePasswordSecret": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&sort=weavePasswordSecret"
            },
            "order": "asc",
            "reverse": "https://ansible.app.iac-svil.almaviva.it/v3/clusters?name=iacdevops&order=desc"
        },
        "type": "collection"
    }
}
2025-08-27 15:03:39,075 p=20016 u=root n=ansible INFO| TASK [master1_create_cluster : Get or create cluster registration token] ************************************************************************************************************************************************************************************************
2025-08-27 15:03:39,405 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it -> localhost]
2025-08-27 15:03:39,411 p=20016 u=root n=ansible INFO| TASK [master1_create_cluster : Wait for registration token to be ready] *************************************************************************************************************************************************************************************************
2025-08-27 15:03:39,726 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it -> localhost]
2025-08-27 15:03:39,732 p=20016 u=root n=ansible INFO| TASK [master1_create_cluster : Show cluster status] *********************************************************************************************************************************************************************************************************************
2025-08-27 15:03:39,744 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it] => {
    "msg": "Cluster creato con successo!\nNome: iacdevops\nID: iacdevops\nNamespace: fleet-default\nKubernetes Version: v1.31.11+rke2r1\n"
}
2025-08-27 15:03:39,751 p=20016 u=root n=ansible INFO| TASK [master1_create_cluster : Debug registration token] ****************************************************************************************************************************************************************************************************************
2025-08-27 15:03:39,764 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it] => {
    "token_list.json.data[0]": {
        "annotations": {},
        "baseType": "clusterRegistrationToken",
        "clusterId": "c-m-rpjjcwlv",
        "command": "kubectl apply -f https://ansible.app.iac-svil.almaviva.it/v3/import/szbr5zq9ls2kd5s77cr5g42sw8d4kkpshtn2f6zlwkszsnd5g2jg5s_c-m-rpjjcwlv.yaml",
        "created": "2025-08-27T13:03:39Z",
        "createdTS": 1756299819000,
        "creatorId": "user-jjp6z",
        "id": "c-m-rpjjcwlv:crt-f7xs2",
        "insecureCommand": "curl --insecure -sfL https://ansible.app.iac-svil.almaviva.it/v3/import/szbr5zq9ls2kd5s77cr5g42sw8d4kkpshtn2f6zlwkszsnd5g2jg5s_c-m-rpjjcwlv.yaml | kubectl apply -f -",
        "insecureNodeCommand": " curl --insecure -fL https://ansible.app.iac-svil.almaviva.it/system-agent-install.sh | sudo  sh -s - --server https://ansible.app.iac-svil.almaviva.it --label 'cattle.io/os=linux' --token szbr5zq9ls2kd5s77cr5g42sw8d4kkpshtn2f6zlwkszsnd5g2jg5s --ca-checksum 538463ba320a67b4723aebc71f8d798ce0416a6ff29ecf895661e0f7ac274349",
        "insecureWindowsNodeCommand": " curl.exe --insecure -fL https://ansible.app.iac-svil.almaviva.it/wins-agent-install.ps1 -o install.ps1; Set-ExecutionPolicy Bypass -Scope Process -Force; ./install.ps1 -Server https://ansible.app.iac-svil.almaviva.it -Label 'cattle.io/os=windows' -Token szbr5zq9ls2kd5s77cr5g42sw8d4kkpshtn2f6zlwkszsnd5g2jg5s -Worker -CaChecksum 538463ba320a67b4723aebc71f8d798ce0416a6ff29ecf895661e0f7ac274349",
        "labels": {
            "cattle.io/creator": "norman"
        },
        "links": {
            "remove": "https://ansible.app.iac-svil.almaviva.it/v3/clusterRegistrationTokens/c-m-rpjjcwlv:crt-f7xs2",
            "self": "https://ansible.app.iac-svil.almaviva.it/v3/clusterRegistrationTokens/c-m-rpjjcwlv:crt-f7xs2",
            "update": "https://ansible.app.iac-svil.almaviva.it/v3/clusterRegistrationTokens/c-m-rpjjcwlv:crt-f7xs2"
        },
        "manifestUrl": "https://ansible.app.iac-svil.almaviva.it/v3/import/szbr5zq9ls2kd5s77cr5g42sw8d4kkpshtn2f6zlwkszsnd5g2jg5s_c-m-rpjjcwlv.yaml",
        "name": "crt-f7xs2",
        "namespaceId": null,
        "nodeCommand": " curl -fL https://ansible.app.iac-svil.almaviva.it/system-agent-install.sh | sudo  sh -s - --server https://ansible.app.iac-svil.almaviva.it --label 'cattle.io/os=linux' --token szbr5zq9ls2kd5s77cr5g42sw8d4kkpshtn2f6zlwkszsnd5g2jg5s --ca-checksum 538463ba320a67b4723aebc71f8d798ce0416a6ff29ecf895661e0f7ac274349",
        "state": "active",
        "token": "szbr5zq9ls2kd5s77cr5g42sw8d4kkpshtn2f6zlwkszsnd5g2jg5s",
        "transitioning": "no",
        "transitioningMessage": "",
        "type": "clusterRegistrationToken",
        "uuid": "a15dfc0c-51cd-4c30-95b0-a54b4c4bff03",
        "windowsNodeCommand": " curl.exe -fL https://ansible.app.iac-svil.almaviva.it/wins-agent-install.ps1 -o install.ps1; Set-ExecutionPolicy Bypass -Scope Process -Force; ./install.ps1 -Server https://ansible.app.iac-svil.almaviva.it -Label 'cattle.io/os=windows' -Token szbr5zq9ls2kd5s77cr5g42sw8d4kkpshtn2f6zlwkszsnd5g2jg5s -Worker -CaChecksum 538463ba320a67b4723aebc71f8d798ce0416a6ff29ecf895661e0f7ac274349"
    }
}
2025-08-27 15:03:39,769 p=20016 u=root n=ansible INFO| TASK [master1_create_cluster : Extract registration details] ************************************************************************************************************************************************************************************************************
2025-08-27 15:03:39,795 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 15:03:39,800 p=20016 u=root n=ansible INFO| TASK [master1_create_cluster : Extract token from nodeCommand] **********************************************************************************************************************************************************************************************************
2025-08-27 15:03:39,827 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 15:03:39,832 p=20016 u=root n=ansible INFO| TASK [master1_create_cluster : Debug extracted values] ******************************************************************************************************************************************************************************************************************
2025-08-27 15:03:39,844 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it] => {
    "msg": "Rancher Host: ansible.app.iac-svil.almaviva.it\nCluster Token: szbr5zq9ls2kd5s77cr5g42sw8d4kkpshtn2f6zlwkszsnd5g2jg5s\nNode Command:  curl -fL https://ansible.app.iac-svil.almaviva.it/system-agent-install.sh | sudo  sh -s - --server https://ansible.app.iac-svil.almaviva.it --label 'cattle.io/os=linux' --token szbr5zq9ls2kd5s77cr5g42sw8d4kkpshtn2f6zlwkszsnd5g2jg5s --ca-checksum 538463ba320a67b4723aebc71f8d798ce0416a6ff29ecf895661e0f7ac274349\n"
}
2025-08-27 15:03:39,849 p=20016 u=root n=ansible INFO| TASK [master1_create_cluster : Install system-agent and join nodes as managers] *****************************************************************************************************************************************************************************************
2025-08-27 15:03:40,839 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it -> rke2-manager2(10.205.166.217)] => (item=rke2-manager2)
2025-08-27 15:03:40,845 p=20016 u=root n=ansible INFO| TASK [master1_create_cluster : Install system-agent and join nodes as workers] ******************************************************************************************************************************************************************************************
2025-08-27 15:03:41,729 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it -> rke2-worker1(10.205.166.218)] => (item=rke2-worker1)
2025-08-27 15:03:42,593 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it -> rke2-worker2(10.205.166.214)] => (item=rke2-worker2)
2025-08-27 15:03:43,498 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it -> rke2-worker3(10.205.166.215)] => (item=rke2-worker3)
2025-08-27 15:03:43,505 p=20016 u=root n=ansible INFO| TASK [master1_create_cluster : Wait for manager nodes join to complete] *************************************************************************************************************************************************************************************************
2025-08-27 15:03:43,811 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it -> rke2-manager2(10.205.166.217)] => (item={'failed': 0, 'started': 1, 'finished': 0, 'ansible_job_id': 'j478519967843.22002', 'results_file': '/root/.ansible_async/j478519967843.22002', 'changed': True, 'item': 'rke2-manager2', 'ansible_loop_var': 'item'})
2025-08-27 15:03:43,817 p=20016 u=root n=ansible INFO| TASK [master1_create_cluster : Wait for worker nodes join to complete] **************************************************************************************************************************************************************************************************
2025-08-27 15:03:44,056 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it -> rke2-worker1(10.205.166.218)] => (item={'failed': 0, 'started': 1, 'finished': 0, 'ansible_job_id': 'j200090488053.21237', 'results_file': '/root/.ansible_async/j200090488053.21237', 'changed': True, 'item': 'rke2-worker1', 'ansible_loop_var': 'item'})
2025-08-27 15:03:44,296 p=20016 u=root n=ansible INFO| FAILED - RETRYING: [cornelio.app.iac-svil.almaviva.it -> rke2-worker2]: Wait for worker nodes join to complete (120 retries left).
2025-08-27 15:04:04,520 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it -> rke2-worker2(10.205.166.214)] => (item={'failed': 0, 'started': 1, 'finished': 0, 'ansible_job_id': 'j366583591004.20966', 'results_file': '/root/.ansible_async/j366583591004.20966', 'changed': True, 'item': 'rke2-worker2', 'ansible_loop_var': 'item'})
2025-08-27 15:04:04,753 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it -> rke2-worker3(10.205.166.215)] => (item={'failed': 0, 'started': 1, 'finished': 0, 'ansible_job_id': 'j470111294503.21357', 'results_file': '/root/.ansible_async/j470111294503.21357', 'changed': True, 'item': 'rke2-worker3', 'ansible_loop_var': 'item'})
2025-08-27 15:04:04,759 p=20016 u=root n=ansible INFO| TASK [master1_create_cluster : Show manager nodes join results] *********************************************************************************************************************************************************************************************************
2025-08-27 15:04:04,781 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it] => (item={'started': 1, 'finished': 1, 'stdout': '[INFO]  Label: cattle.io/os=linux\n[INFO]  Role requested: etcd\n[INFO]  Role requested: controlplane\n[INFO]  CA strict verification is set to true\n[INFO]  Using default agent configuration directory /etc/rancher/agent\n[INFO]  Using default agent var directory /var/lib/rancher/agent\n[INFO]  Successfully downloaded CA certificate\n[INFO]  Value from https://ansible.app.iac-svil.almaviva.it/cacerts is an x509 certificate\n[INFO]  Successfully tested Rancher connection\n[INFO]  Downloading rancher-system-agent binary from https://ansible.app.iac-svil.almaviva.it/assets/rancher-system-agent-amd64\n[INFO]  Successfully downloaded the rancher-system-agent binary.\n[INFO]  Downloading rancher-system-agent-uninstall.sh script from https://ansible.app.iac-svil.almaviva.it/assets/system-agent-uninstall.sh\n[INFO]  Successfully downloaded the rancher-system-agent-uninstall.sh script.\n[INFO]  Generating Cattle ID\n[INFO]  Cattle ID was already detected as f49d8e34127d0160bc40d7183e9dffad6d5b3f738a73e0e3c5ee7c118f40655. Not generating a new one.\n[INFO]  Successfully downloaded Rancher connection information\n[INFO]  systemd: Creating service file\n[INFO]  Creating environment file /etc/systemd/system/rancher-system-agent.env\n[INFO]  Enabling rancher-system-agent.service\n[INFO]  Starting/restarting rancher-system-agent.service', 'stderr': '  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 33702    0 33702    0     0  3787k      0 --:--:-- --:--:-- --:--:-- 4114k', 'stdout_lines': ['[INFO]  Label: cattle.io/os=linux', '[INFO]  Role requested: etcd', '[INFO]  Role requested: controlplane', '[INFO]  CA strict verification is set to true', '[INFO]  Using default agent configuration directory /etc/rancher/agent', '[INFO]  Using default agent var directory /var/lib/rancher/agent', '[INFO]  Successfully downloaded CA certificate', '[INFO]  Value from https://ansible.app.iac-svil.almaviva.it/cacerts is an x509 certificate', '[INFO]  Successfully tested Rancher connection', '[INFO]  Downloading rancher-system-agent binary from https://ansible.app.iac-svil.almaviva.it/assets/rancher-system-agent-amd64', '[INFO]  Successfully downloaded the rancher-system-agent binary.', '[INFO]  Downloading rancher-system-agent-uninstall.sh script from https://ansible.app.iac-svil.almaviva.it/assets/system-agent-uninstall.sh', '[INFO]  Successfully downloaded the rancher-system-agent-uninstall.sh script.', '[INFO]  Generating Cattle ID', '[INFO]  Cattle ID was already detected as f49d8e34127d0160bc40d7183e9dffad6d5b3f738a73e0e3c5ee7c118f40655. Not generating a new one.', '[INFO]  Successfully downloaded Rancher connection information', '[INFO]  systemd: Creating service file', '[INFO]  Creating environment file /etc/systemd/system/rancher-system-agent.env', '[INFO]  Enabling rancher-system-agent.service', '[INFO]  Starting/restarting rancher-system-agent.service'], 'stderr_lines': ['  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current', '                                 Dload  Upload   Total   Spent    Left  Speed', '', '  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0', '100 33702    0 33702    0     0  3787k      0 --:--:-- --:--:-- --:--:-- 4114k'], 'ansible_job_id': 'j478519967843.22002', 'results_file': '/root/.ansible_async/j478519967843.22002', 'changed': True, 'rc': 0, 'cmd': "# Download and install system-agent for manager nodes\ncurl --insecure -fL https://ansible.app.iac-svil.almaviva.it/system-agent-install.sh | sh -s -  --server https://ansible.app.iac-svil.almaviva.it  --label 'cattle.io/os=linux'  --token szbr5zq9ls2kd5s77cr5g42sw8d4kkpshtn2f6zlwkszsnd5g2jg5s  --etcd --controlplane\n", 'start': '2025-08-27 15:03:40.952267', 'end': '2025-08-27 15:03:42.433747', 'delta': '0:00:01.481480', 'msg': '', 'invocation': {'module_args': {'executable': '/bin/bash', '_raw_params': "# Download and install system-agent for manager nodes\ncurl --insecure -fL https://ansible.app.iac-svil.almaviva.it/system-agent-install.sh | sh -s -  --server https://ansible.app.iac-svil.almaviva.it  --label 'cattle.io/os=linux'  --token szbr5zq9ls2kd5s77cr5g42sw8d4kkpshtn2f6zlwkszsnd5g2jg5s  --etcd --controlplane\n", '_uses_shell': True, 'expand_argument_vars': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'creates': None, 'removes': None, 'stdin': None}}, 'failed': False, 'attempts': 1, 'item': {'failed': 0, 'started': 1, 'finished': 0, 'ansible_job_id': 'j478519967843.22002', 'results_file': '/root/.ansible_async/j478519967843.22002', 'changed': True, 'item': 'rke2-manager2', 'ansible_loop_var': 'item'}, 'ansible_loop_var': 'item'}) => {
    "msg": "Manager Node: {'failed': 0, 'started': 1, 'finished': 0, 'ansible_job_id': 'j478519967843.22002', 'results_file': '/root/.ansible_async/j478519967843.22002', 'changed': True, 'item': 'rke2-manager2', 'ansible_loop_var': 'item'}\nStatus: Success\nOutput: [INFO]  Label: cattle.io/os=linux\n[INFO]  Role requested: etcd\n[INFO]  Role requested: controlplane\n[INFO]  CA strict verification is set to true\n[INFO]  Using default agent configuration directory /etc/rancher/agent\n[INFO]  Using default agent var directory /var/lib/rancher/agent\n[INFO]  Successfully downloaded CA certificate\n[INFO]  Value from https://ansible.app.iac-svil.almaviva.it/cacerts is an x509 certificate\n[INFO]  Successfully tested Rancher connection\n[INFO]  Downloading rancher-system-agent binary from https://ansible.app.iac-svil.almaviva.it/assets/rancher-system-agent-amd64\n[INFO]  Successfully downloaded the rancher-system-agent binary.\n[INFO]  Downloading rancher-system-agent-uninstall.sh script from https://ansible.app.iac-svil.almaviva.it/assets/system-agent-uninstall.sh\n[INFO]  Successfully downloaded the rancher-system-agent-uninstall.sh script.\n[INFO]  Generating Cattle ID\n[INFO]  Cattle ID was already detected as f49d8e34127d0160bc40d7183e9dffad6d5b3f738a73e0e3c5ee7c118f40655. Not generating a new one.\n[INFO]  Successfully downloaded Rancher connection information\n[INFO]  systemd: Creating service file\n[INFO]  Creating environment file /etc/systemd/system/rancher-system-agent.env\n[INFO]  Enabling rancher-system-agent.service\n[INFO]  Starting/restarting rancher-system-agent.service\nError:   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 33702    0 33702    0     0  3787k      0 --:--:-- --:--:-- --:--:-- 4114k\n"
}
2025-08-27 15:04:04,787 p=20016 u=root n=ansible INFO| TASK [master1_create_cluster : Show worker nodes join results] **********************************************************************************************************************************************************************************************************
2025-08-27 15:04:04,810 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it] => (item={'started': 1, 'finished': 1, 'stdout': '[INFO]  Label: cattle.io/os=linux\n[INFO]  Role requested: worker\n[INFO]  CA strict verification is set to true\n[INFO]  Using default agent configuration directory /etc/rancher/agent\n[INFO]  Using default agent var directory /var/lib/rancher/agent\n[INFO]  Successfully downloaded CA certificate\n[INFO]  Value from https://ansible.app.iac-svil.almaviva.it/cacerts is an x509 certificate\n[INFO]  Successfully tested Rancher connection\n[INFO]  Downloading rancher-system-agent binary from https://ansible.app.iac-svil.almaviva.it/assets/rancher-system-agent-amd64\n[INFO]  Successfully downloaded the rancher-system-agent binary.\n[INFO]  Downloading rancher-system-agent-uninstall.sh script from https://ansible.app.iac-svil.almaviva.it/assets/system-agent-uninstall.sh\n[INFO]  Successfully downloaded the rancher-system-agent-uninstall.sh script.\n[INFO]  Generating Cattle ID\n[INFO]  Cattle ID was already detected as 2ac6b2c9c008d545f9ccc4285b1dfe0e164d46be760d5ef5a4ad6b5c16e5d02. Not generating a new one.\n[INFO]  Successfully downloaded Rancher connection information\n[INFO]  systemd: Creating service file\n[INFO]  Creating environment file /etc/systemd/system/rancher-system-agent.env\n[INFO]  Enabling rancher-system-agent.service\n[INFO]  Starting/restarting rancher-system-agent.service', 'stderr': '  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 33702    0 33702    0     0  3779k      0 --:--:-- --:--:-- --:--:-- 4114k', 'stdout_lines': ['[INFO]  Label: cattle.io/os=linux', '[INFO]  Role requested: worker', '[INFO]  CA strict verification is set to true', '[INFO]  Using default agent configuration directory /etc/rancher/agent', '[INFO]  Using default agent var directory /var/lib/rancher/agent', '[INFO]  Successfully downloaded CA certificate', '[INFO]  Value from https://ansible.app.iac-svil.almaviva.it/cacerts is an x509 certificate', '[INFO]  Successfully tested Rancher connection', '[INFO]  Downloading rancher-system-agent binary from https://ansible.app.iac-svil.almaviva.it/assets/rancher-system-agent-amd64', '[INFO]  Successfully downloaded the rancher-system-agent binary.', '[INFO]  Downloading rancher-system-agent-uninstall.sh script from https://ansible.app.iac-svil.almaviva.it/assets/system-agent-uninstall.sh', '[INFO]  Successfully downloaded the rancher-system-agent-uninstall.sh script.', '[INFO]  Generating Cattle ID', '[INFO]  Cattle ID was already detected as 2ac6b2c9c008d545f9ccc4285b1dfe0e164d46be760d5ef5a4ad6b5c16e5d02. Not generating a new one.', '[INFO]  Successfully downloaded Rancher connection information', '[INFO]  systemd: Creating service file', '[INFO]  Creating environment file /etc/systemd/system/rancher-system-agent.env', '[INFO]  Enabling rancher-system-agent.service', '[INFO]  Starting/restarting rancher-system-agent.service'], 'stderr_lines': ['  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current', '                                 Dload  Upload   Total   Spent    Left  Speed', '', '  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0', '100 33702    0 33702    0     0  3779k      0 --:--:-- --:--:-- --:--:-- 4114k'], 'ansible_job_id': 'j200090488053.21237', 'results_file': '/root/.ansible_async/j200090488053.21237', 'changed': True, 'rc': 0, 'cmd': "# Download and install system-agent for worker nodes\ncurl --insecure -fL https://ansible.app.iac-svil.almaviva.it/system-agent-install.sh | sh -s -  --server https://ansible.app.iac-svil.almaviva.it  --label 'cattle.io/os=linux'  --token szbr5zq9ls2kd5s77cr5g42sw8d4kkpshtn2f6zlwkszsnd5g2jg5s  --worker\n", 'start': '2025-08-27 15:03:41.842588', 'end': '2025-08-27 15:03:43.292372', 'delta': '0:00:01.449784', 'msg': '', 'invocation': {'module_args': {'executable': '/bin/bash', '_raw_params': "# Download and install system-agent for worker nodes\ncurl --insecure -fL https://ansible.app.iac-svil.almaviva.it/system-agent-install.sh | sh -s -  --server https://ansible.app.iac-svil.almaviva.it  --label 'cattle.io/os=linux'  --token szbr5zq9ls2kd5s77cr5g42sw8d4kkpshtn2f6zlwkszsnd5g2jg5s  --worker\n", '_uses_shell': True, 'expand_argument_vars': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'creates': None, 'removes': None, 'stdin': None}}, 'failed': False, 'attempts': 1, 'item': {'failed': 0, 'started': 1, 'finished': 0, 'ansible_job_id': 'j200090488053.21237', 'results_file': '/root/.ansible_async/j200090488053.21237', 'changed': True, 'item': 'rke2-worker1', 'ansible_loop_var': 'item'}, 'ansible_loop_var': 'item'}) => {
    "msg": "Worker Node: {'failed': 0, 'started': 1, 'finished': 0, 'ansible_job_id': 'j200090488053.21237', 'results_file': '/root/.ansible_async/j200090488053.21237', 'changed': True, 'item': 'rke2-worker1', 'ansible_loop_var': 'item'}\nStatus: Success\nOutput: [INFO]  Label: cattle.io/os=linux\n[INFO]  Role requested: worker\n[INFO]  CA strict verification is set to true\n[INFO]  Using default agent configuration directory /etc/rancher/agent\n[INFO]  Using default agent var directory /var/lib/rancher/agent\n[INFO]  Successfully downloaded CA certificate\n[INFO]  Value from https://ansible.app.iac-svil.almaviva.it/cacerts is an x509 certificate\n[INFO]  Successfully tested Rancher connection\n[INFO]  Downloading rancher-system-agent binary from https://ansible.app.iac-svil.almaviva.it/assets/rancher-system-agent-amd64\n[INFO]  Successfully downloaded the rancher-system-agent binary.\n[INFO]  Downloading rancher-system-agent-uninstall.sh script from https://ansible.app.iac-svil.almaviva.it/assets/system-agent-uninstall.sh\n[INFO]  Successfully downloaded the rancher-system-agent-uninstall.sh script.\n[INFO]  Generating Cattle ID\n[INFO]  Cattle ID was already detected as 2ac6b2c9c008d545f9ccc4285b1dfe0e164d46be760d5ef5a4ad6b5c16e5d02. Not generating a new one.\n[INFO]  Successfully downloaded Rancher connection information\n[INFO]  systemd: Creating service file\n[INFO]  Creating environment file /etc/systemd/system/rancher-system-agent.env\n[INFO]  Enabling rancher-system-agent.service\n[INFO]  Starting/restarting rancher-system-agent.service\nError:   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 33702    0 33702    0     0  3779k      0 --:--:-- --:--:-- --:--:-- 4114k\n"
}
2025-08-27 15:04:04,820 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it] => (item={'started': 1, 'finished': 1, 'stdout': '[INFO]  Label: cattle.io/os=linux\n[INFO]  Role requested: worker\n[INFO]  CA strict verification is set to true\n[INFO]  Using default agent configuration directory /etc/rancher/agent\n[INFO]  Using default agent var directory /var/lib/rancher/agent\n[INFO]  Successfully downloaded CA certificate\n[INFO]  Value from https://ansible.app.iac-svil.almaviva.it/cacerts is an x509 certificate\n[INFO]  Successfully tested Rancher connection\n[INFO]  Downloading rancher-system-agent binary from https://ansible.app.iac-svil.almaviva.it/assets/rancher-system-agent-amd64\n[INFO]  Successfully downloaded the rancher-system-agent binary.\n[INFO]  Downloading rancher-system-agent-uninstall.sh script from https://ansible.app.iac-svil.almaviva.it/assets/system-agent-uninstall.sh\n[INFO]  Successfully downloaded the rancher-system-agent-uninstall.sh script.\n[INFO]  Generating Cattle ID\n[INFO]  Cattle ID was already detected as 09cada8227ce28abebaf5ec6d7f432b172a86a86ae8051f9c80fe609c3662a2. Not generating a new one.\n[INFO]  Successfully downloaded Rancher connection information\n[INFO]  systemd: Creating service file\n[INFO]  Creating environment file /etc/systemd/system/rancher-system-agent.env\n[INFO]  Enabling rancher-system-agent.service\n[INFO]  Starting/restarting rancher-system-agent.service', 'stderr': '  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 33702    0 33702    0     0  3695k      0 --:--:-- --:--:-- --:--:-- 4114k', 'stdout_lines': ['[INFO]  Label: cattle.io/os=linux', '[INFO]  Role requested: worker', '[INFO]  CA strict verification is set to true', '[INFO]  Using default agent configuration directory /etc/rancher/agent', '[INFO]  Using default agent var directory /var/lib/rancher/agent', '[INFO]  Successfully downloaded CA certificate', '[INFO]  Value from https://ansible.app.iac-svil.almaviva.it/cacerts is an x509 certificate', '[INFO]  Successfully tested Rancher connection', '[INFO]  Downloading rancher-system-agent binary from https://ansible.app.iac-svil.almaviva.it/assets/rancher-system-agent-amd64', '[INFO]  Successfully downloaded the rancher-system-agent binary.', '[INFO]  Downloading rancher-system-agent-uninstall.sh script from https://ansible.app.iac-svil.almaviva.it/assets/system-agent-uninstall.sh', '[INFO]  Successfully downloaded the rancher-system-agent-uninstall.sh script.', '[INFO]  Generating Cattle ID', '[INFO]  Cattle ID was already detected as 09cada8227ce28abebaf5ec6d7f432b172a86a86ae8051f9c80fe609c3662a2. Not generating a new one.', '[INFO]  Successfully downloaded Rancher connection information', '[INFO]  systemd: Creating service file', '[INFO]  Creating environment file /etc/systemd/system/rancher-system-agent.env', '[INFO]  Enabling rancher-system-agent.service', '[INFO]  Starting/restarting rancher-system-agent.service'], 'stderr_lines': ['  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current', '                                 Dload  Upload   Total   Spent    Left  Speed', '', '  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0', '100 33702    0 33702    0     0  3695k      0 --:--:-- --:--:-- --:--:-- 4114k'], 'ansible_job_id': 'j366583591004.20966', 'results_file': '/root/.ansible_async/j366583591004.20966', 'changed': True, 'rc': 0, 'cmd': "# Download and install system-agent for worker nodes\ncurl --insecure -fL https://ansible.app.iac-svil.almaviva.it/system-agent-install.sh | sh -s -  --server https://ansible.app.iac-svil.almaviva.it  --label 'cattle.io/os=linux'  --token szbr5zq9ls2kd5s77cr5g42sw8d4kkpshtn2f6zlwkszsnd5g2jg5s  --worker\n", 'start': '2025-08-27 15:03:42.708747', 'end': '2025-08-27 15:03:44.358378', 'delta': '0:00:01.649631', 'msg': '', 'invocation': {'module_args': {'executable': '/bin/bash', '_raw_params': "# Download and install system-agent for worker nodes\ncurl --insecure -fL https://ansible.app.iac-svil.almaviva.it/system-agent-install.sh | sh -s -  --server https://ansible.app.iac-svil.almaviva.it  --label 'cattle.io/os=linux'  --token szbr5zq9ls2kd5s77cr5g42sw8d4kkpshtn2f6zlwkszsnd5g2jg5s  --worker\n", '_uses_shell': True, 'expand_argument_vars': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'creates': None, 'removes': None, 'stdin': None}}, 'failed': False, 'attempts': 2, 'item': {'failed': 0, 'started': 1, 'finished': 0, 'ansible_job_id': 'j366583591004.20966', 'results_file': '/root/.ansible_async/j366583591004.20966', 'changed': True, 'item': 'rke2-worker2', 'ansible_loop_var': 'item'}, 'ansible_loop_var': 'item'}) => {
    "msg": "Worker Node: {'failed': 0, 'started': 1, 'finished': 0, 'ansible_job_id': 'j366583591004.20966', 'results_file': '/root/.ansible_async/j366583591004.20966', 'changed': True, 'item': 'rke2-worker2', 'ansible_loop_var': 'item'}\nStatus: Success\nOutput: [INFO]  Label: cattle.io/os=linux\n[INFO]  Role requested: worker\n[INFO]  CA strict verification is set to true\n[INFO]  Using default agent configuration directory /etc/rancher/agent\n[INFO]  Using default agent var directory /var/lib/rancher/agent\n[INFO]  Successfully downloaded CA certificate\n[INFO]  Value from https://ansible.app.iac-svil.almaviva.it/cacerts is an x509 certificate\n[INFO]  Successfully tested Rancher connection\n[INFO]  Downloading rancher-system-agent binary from https://ansible.app.iac-svil.almaviva.it/assets/rancher-system-agent-amd64\n[INFO]  Successfully downloaded the rancher-system-agent binary.\n[INFO]  Downloading rancher-system-agent-uninstall.sh script from https://ansible.app.iac-svil.almaviva.it/assets/system-agent-uninstall.sh\n[INFO]  Successfully downloaded the rancher-system-agent-uninstall.sh script.\n[INFO]  Generating Cattle ID\n[INFO]  Cattle ID was already detected as 09cada8227ce28abebaf5ec6d7f432b172a86a86ae8051f9c80fe609c3662a2. Not generating a new one.\n[INFO]  Successfully downloaded Rancher connection information\n[INFO]  systemd: Creating service file\n[INFO]  Creating environment file /etc/systemd/system/rancher-system-agent.env\n[INFO]  Enabling rancher-system-agent.service\n[INFO]  Starting/restarting rancher-system-agent.service\nError:   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 33702    0 33702    0     0  3695k      0 --:--:-- --:--:-- --:--:-- 4114k\n"
}
2025-08-27 15:04:04,827 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it] => (item={'started': 1, 'finished': 1, 'stdout': '[INFO]  Label: cattle.io/os=linux\n[INFO]  Role requested: worker\n[INFO]  CA strict verification is set to true\n[INFO]  Using default agent configuration directory /etc/rancher/agent\n[INFO]  Using default agent var directory /var/lib/rancher/agent\n[INFO]  Successfully downloaded CA certificate\n[INFO]  Value from https://ansible.app.iac-svil.almaviva.it/cacerts is an x509 certificate\n[INFO]  Successfully tested Rancher connection\n[INFO]  Downloading rancher-system-agent binary from https://ansible.app.iac-svil.almaviva.it/assets/rancher-system-agent-amd64\n[INFO]  Successfully downloaded the rancher-system-agent binary.\n[INFO]  Downloading rancher-system-agent-uninstall.sh script from https://ansible.app.iac-svil.almaviva.it/assets/system-agent-uninstall.sh\n[INFO]  Successfully downloaded the rancher-system-agent-uninstall.sh script.\n[INFO]  Generating Cattle ID\n[INFO]  Cattle ID was already detected as b732e812a832943db5f8e4730653be860e81f88e3c8bfbecfbaa389a38df204. Not generating a new one.\n[INFO]  Successfully downloaded Rancher connection information\n[INFO]  systemd: Creating service file\n[INFO]  Creating environment file /etc/systemd/system/rancher-system-agent.env\n[INFO]  Enabling rancher-system-agent.service\n[INFO]  Starting/restarting rancher-system-agent.service', 'stderr': '  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 33702    0 33702    0     0  2378k      0 --:--:-- --:--:-- --:--:-- 2531k', 'stdout_lines': ['[INFO]  Label: cattle.io/os=linux', '[INFO]  Role requested: worker', '[INFO]  CA strict verification is set to true', '[INFO]  Using default agent configuration directory /etc/rancher/agent', '[INFO]  Using default agent var directory /var/lib/rancher/agent', '[INFO]  Successfully downloaded CA certificate', '[INFO]  Value from https://ansible.app.iac-svil.almaviva.it/cacerts is an x509 certificate', '[INFO]  Successfully tested Rancher connection', '[INFO]  Downloading rancher-system-agent binary from https://ansible.app.iac-svil.almaviva.it/assets/rancher-system-agent-amd64', '[INFO]  Successfully downloaded the rancher-system-agent binary.', '[INFO]  Downloading rancher-system-agent-uninstall.sh script from https://ansible.app.iac-svil.almaviva.it/assets/system-agent-uninstall.sh', '[INFO]  Successfully downloaded the rancher-system-agent-uninstall.sh script.', '[INFO]  Generating Cattle ID', '[INFO]  Cattle ID was already detected as b732e812a832943db5f8e4730653be860e81f88e3c8bfbecfbaa389a38df204. Not generating a new one.', '[INFO]  Successfully downloaded Rancher connection information', '[INFO]  systemd: Creating service file', '[INFO]  Creating environment file /etc/systemd/system/rancher-system-agent.env', '[INFO]  Enabling rancher-system-agent.service', '[INFO]  Starting/restarting rancher-system-agent.service'], 'stderr_lines': ['  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current', '                                 Dload  Upload   Total   Spent    Left  Speed', '', '  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0', '100 33702    0 33702    0     0  2378k      0 --:--:-- --:--:-- --:--:-- 2531k'], 'ansible_job_id': 'j470111294503.21357', 'results_file': '/root/.ansible_async/j470111294503.21357', 'changed': True, 'rc': 0, 'cmd': "# Download and install system-agent for worker nodes\ncurl --insecure -fL https://ansible.app.iac-svil.almaviva.it/system-agent-install.sh | sh -s -  --server https://ansible.app.iac-svil.almaviva.it  --label 'cattle.io/os=linux'  --token szbr5zq9ls2kd5s77cr5g42sw8d4kkpshtn2f6zlwkszsnd5g2jg5s  --worker\n", 'start': '2025-08-27 15:03:43.612781', 'end': '2025-08-27 15:03:45.135472', 'delta': '0:00:01.522691', 'msg': '', 'invocation': {'module_args': {'executable': '/bin/bash', '_raw_params': "# Download and install system-agent for worker nodes\ncurl --insecure -fL https://ansible.app.iac-svil.almaviva.it/system-agent-install.sh | sh -s -  --server https://ansible.app.iac-svil.almaviva.it  --label 'cattle.io/os=linux'  --token szbr5zq9ls2kd5s77cr5g42sw8d4kkpshtn2f6zlwkszsnd5g2jg5s  --worker\n", '_uses_shell': True, 'expand_argument_vars': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'creates': None, 'removes': None, 'stdin': None}}, 'failed': False, 'attempts': 1, 'item': {'failed': 0, 'started': 1, 'finished': 0, 'ansible_job_id': 'j470111294503.21357', 'results_file': '/root/.ansible_async/j470111294503.21357', 'changed': True, 'item': 'rke2-worker3', 'ansible_loop_var': 'item'}, 'ansible_loop_var': 'item'}) => {
    "msg": "Worker Node: {'failed': 0, 'started': 1, 'finished': 0, 'ansible_job_id': 'j470111294503.21357', 'results_file': '/root/.ansible_async/j470111294503.21357', 'changed': True, 'item': 'rke2-worker3', 'ansible_loop_var': 'item'}\nStatus: Success\nOutput: [INFO]  Label: cattle.io/os=linux\n[INFO]  Role requested: worker\n[INFO]  CA strict verification is set to true\n[INFO]  Using default agent configuration directory /etc/rancher/agent\n[INFO]  Using default agent var directory /var/lib/rancher/agent\n[INFO]  Successfully downloaded CA certificate\n[INFO]  Value from https://ansible.app.iac-svil.almaviva.it/cacerts is an x509 certificate\n[INFO]  Successfully tested Rancher connection\n[INFO]  Downloading rancher-system-agent binary from https://ansible.app.iac-svil.almaviva.it/assets/rancher-system-agent-amd64\n[INFO]  Successfully downloaded the rancher-system-agent binary.\n[INFO]  Downloading rancher-system-agent-uninstall.sh script from https://ansible.app.iac-svil.almaviva.it/assets/system-agent-uninstall.sh\n[INFO]  Successfully downloaded the rancher-system-agent-uninstall.sh script.\n[INFO]  Generating Cattle ID\n[INFO]  Cattle ID was already detected as b732e812a832943db5f8e4730653be860e81f88e3c8bfbecfbaa389a38df204. Not generating a new one.\n[INFO]  Successfully downloaded Rancher connection information\n[INFO]  systemd: Creating service file\n[INFO]  Creating environment file /etc/systemd/system/rancher-system-agent.env\n[INFO]  Enabling rancher-system-agent.service\n[INFO]  Starting/restarting rancher-system-agent.service\nError:   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 33702    0 33702    0     0  2378k      0 --:--:-- --:--:-- --:--:-- 2531k\n"
}
2025-08-27 15:04:04,833 p=20016 u=root n=ansible INFO| TASK [master1_create_cluster : Verify nodes are registered in Rancher] **************************************************************************************************************************************************************************************************
2025-08-27 15:04:05,126 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it -> localhost]
2025-08-27 15:04:05,134 p=20016 u=root n=ansible INFO| TASK [master1_create_cluster : Show registered nodes] *******************************************************************************************************************************************************************************************************************
2025-08-27 15:04:05,167 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it] => {
    "msg": "Nodes registered in cluster:\n"
}
2025-08-27 15:04:05,173 p=20016 u=root n=ansible INFO| PLAY [Pausa dopo creazione cluster] *************************************************************************************************************************************************************************************************************************************
2025-08-27 15:04:05,178 p=20016 u=root n=ansible INFO| TASK [Attendi 180 secondi prima setup kubectl] **************************************************************************************************************************************************************************************************************************
2025-08-27 15:04:05,186 p=20016 u=root n=ansible INFO| Pausing for 180 seconds
2025-08-27 15:04:05,186 p=20016 u=root n=ansible INFO| (ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
2025-08-27 15:04:05,186 p=20016 u=root n=ansible INFO| [Attendi 180 secondi prima setup kubectl]
Attendendo 60 secondi dopo creazione cluster...:
2025-08-27 15:07:05,188 p=20016 u=root n=ansible INFO| ok: [localhost]
2025-08-27 15:07:05,191 p=20016 u=root n=ansible INFO| PLAY [new_managers] *****************************************************************************************************************************************************************************************************************************************************
2025-08-27 15:07:05,198 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Copy kubectl binary to system path] ***************************************************************************************************************************************************************************************************************
2025-08-27 15:07:06,907 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:07:06,913 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Create .kube directory for root] ******************************************************************************************************************************************************************************************************************
2025-08-27 15:07:07,157 p=20016 u=root n=ansible INFO| [0;31m--- before[0m
[0;31m[0m[0;32m+++ after[0m
[0;32m[0m[0;36m@@ -1,4 +1,4 @@[0m
[0;36m[0m {
     "path": "/root/.kube",
[0;31m-    "state": "absent"[0m
[0;31m[0m[0;32m+    "state": "directory"[0m
[0;32m[0m }


2025-08-27 15:07:07,157 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:07:07,162 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Copy RKE2 kubeconfig to root .kube directory] *****************************************************************************************************************************************************************************************************
2025-08-27 15:07:07,390 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:07:07,396 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Verify kubectl is working] ************************************************************************************************************************************************************************************************************************
2025-08-27 15:07:07,689 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:07:07,694 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Show kubectl test result] *************************************************************************************************************************************************************************************************************************
2025-08-27 15:07:07,706 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "Kubectl test result:\nNAME            STATUS   ROLES                       AGE     VERSION\nrke2-manager2   Ready    control-plane,etcd,master   2m53s   v1.31.11+rke2r1\n"
}
2025-08-27 15:07:07,711 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Wait 60 seconds before applying DNS fix] **********************************************************************************************************************************************************************************************************
2025-08-27 15:07:07,721 p=20016 u=root n=ansible INFO| Pausing for 60 seconds
2025-08-27 15:07:07,721 p=20016 u=root n=ansible INFO| (ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
2025-08-27 15:08:07,726 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:08:07,733 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Wait for cattle-system namespace to exist] ********************************************************************************************************************************************************************************************************
2025-08-27 15:08:08,603 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:08:08,608 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Show namespace status] ****************************************************************************************************************************************************************************************************************************
2025-08-27 15:08:08,620 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "Cattle-system namespace: Found\n"
}
2025-08-27 15:08:08,627 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Wait for cattle-cluster-agent deployment to exist] ************************************************************************************************************************************************************************************************
2025-08-27 15:08:08,921 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:08:08,952 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Show deployment availability] *********************************************************************************************************************************************************************************************************************
2025-08-27 15:08:08,964 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "Cattle-cluster-agent deployment: Found\n"
}
2025-08-27 15:08:08,969 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Check current cattle-cluster-agent deployment] ****************************************************************************************************************************************************************************************************
2025-08-27 15:08:09,261 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:08:09,266 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Show current deployment (before changes)] *********************************************************************************************************************************************************************************************************
2025-08-27 15:08:09,283 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "current_deployment.stdout": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: \"1\"\n    management.cattle.io/scale-available: \"2\"\n    objectset.rio.cattle.io/applied: H4sIAAAAAAAA/8RW72/iOBD9V07+nFBIKD8i7Yeoze6htmxFYKXVqkKDPQFfjZ2znXRRxf9+chLa0NL2Vnen+wRJZt68Gb95ySOBnH9DbbiSJCKQ5+as7BGP3HPJSEQuMRdqt0VpiUe2aIGBBRI9EpBSWbBcSeMutyBhjS6uQ8FagR2uzgwFgT6UwAWsBJKIBMQjavUHUmvQdjRXrWju6r3zXD1I1P66vCcRuQ9N60nZ83674pJ9ihlT8kMICVvHhYrCWNQ+rOvuPk4yOVCXeV+s0Dc7Y3FL9h4RsEJRTeEtiA2Yjat4TkPA7jnAgGZD7PdYsOoGAY4BaD+jNAuHIRtlfQd6IFlh+C+5ttk0Ic98TI7UsTEokFql6/OxdHP9RBTy/C3wvUOwGiyudy5UKyG4XC9yBhZrqJ9poddIop7nLhaydcLdvUfsLne8ZkeJ7j5uc3EAaUlJ/G1aTWOQZVxyW9GTimHcus41Zqg1sstCc7lO6QZZ4XhM1lI93U5+Ii1spfkfhxyUFJ9GlfzMNRpTy/vHI7nHHYmqYr5WAjtOAlqiRePOlypptRK5AIlOSTlqqAZPJk6OJYgCHQ6xukByt7/be+QB+XpjSdTrdvfev8fB/19JbMEd2X9d/XmxXu3wL5Xd33lE458F/wW51IpLm9Wao97WzN6hu0ILL+akzDHZqbIv+D5wydSDqSjfOfHnisXS8n+sdcUOEI780/6lL83idC9uQ98b88ndrcduVa6EWu+uKqDjeWyUsZXjvVBF1blTNnCJuiaCsqx+G4e8iOfz62T5OYnni1mSHtiQiOB2hYwhe2aT808ZCINeJhBt879+dbHGQP0iX2tg6FNFnCiPq0zS5ewqadWoIE4EpsnsWzJrBW6szU10dgbS8JXADuR5hwP1TclFB8QWSl5Ch9sTWBfx8uL35OIqXdy0AM/DUX8QriAMujAYrvrDIARc0WEvG7HheESx2+8NYJBlwRhpNhqfDwY97GZDoMGwH/bHpwpdL9L5Eetqa14HXo3S5U08jb8klx8HN6jLWfJlks5n31sZp6JnyWUynU/i6+U0vmmP+iAtjQyl5SCMzwajsBt0x93Bm0ew/JbM0snXaQuoDDq9Xic4dbzTdB5fXy8Xi0m7MYbQgyxjfjYO0e8HuPJH4WDkAxuHYwbD3jikJ9G+zJI0XU5ul5dfb+JJm4MxgucdfqSxdD6bXMwd4cnn76/meucRvgX32iUaJN2gPmt+6y2LDm01cbeFELdKcOq2bZJNlb3VaJ4/H1pfQBrXvPHtUoliizeqkLbxNff3Fqz7fjl7fQTk1ZfK0TONwL5KsSORa8JZmUcM6pJTjCl10NN2NnEuIZy3PNkOZhlSW1lkbWQu6P2XEFrKqrm2chtv/Dj5+DXq+NYjMW3POdmrQarROv9kmEEh7I1iSKIw6B6eTd/KPhKyM739/q8AAAD//1B1BXWZCwAA\n    objectset.rio.cattle.io/id: \"\"\n    objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon\n    objectset.rio.cattle.io/owner-name: cluster-agent\n    objectset.rio.cattle.io/owner-namespace: kube-system\n  creationTimestamp: \"2025-08-27T13:04:15Z\"\n  generation: 1\n  labels:\n    objectset.rio.cattle.io/hash: c5c3ae05aa6cf7e41d2b022e9aac4fccf373d8f4\n  name: cattle-cluster-agent\n  namespace: cattle-system\n  resourceVersion: \"987\"\n  uid: cfe82288-7bdf-4da2-b955-f9f8ed07444f\nspec:\n  progressDeadlineSeconds: 600\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: cattle-cluster-agent\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: cattle-cluster-agent\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - preference:\n              matchExpressions:\n              - key: node-role.kubernetes.io/controlplane\n                operator: In\n                values:\n                - \"true\"\n            weight: 100\n          - preference:\n              matchExpressions:\n              - key: node-role.kubernetes.io/control-plane\n                operator: In\n                values:\n                - \"true\"\n            weight: 100\n          - preference:\n              matchExpressions:\n              - key: node-role.kubernetes.io/master\n                operator: In\n                values:\n                - \"true\"\n            weight: 100\n          - preference:\n              matchExpressions:\n              - key: cattle.io/cluster-agent\n                operator: In\n                values:\n                - \"true\"\n            weight: 1\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/os\n                operator: NotIn\n                values:\n                - windows\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - cattle-cluster-agent\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      containers:\n      - env:\n        - name: CATTLE_FEATURES\n          value: embedded-cluster-api=false,fleet=false,managed-system-upgrade-controller=false,multi-cluster-management=false,multi-cluster-management-agent=true,provisioningprebootstrap=false,provisioningv2=false,rke2=false,ui-sql-cache=false\n        - name: CATTLE_IS_RKE\n          value: \"false\"\n        - name: CATTLE_SERVER\n          value: https://ansible.app.iac-svil.almaviva.it\n        - name: CATTLE_CA_CHECKSUM\n          value: 538463ba320a67b4723aebc71f8d798ce0416a6ff29ecf895661e0f7ac274349\n        - name: CATTLE_CLUSTER\n          value: \"true\"\n        - name: CATTLE_K8S_MANAGED\n          value: \"true\"\n        - name: CATTLE_CLUSTER_REGISTRY\n        - name: CATTLE_CREDENTIAL_NAME\n          value: cattle-credentials-d683020906\n        - name: CATTLE_SERVER_VERSION\n          value: v2.11.2\n        - name: CATTLE_INSTALL_UUID\n          value: dea1affd-f93e-42eb-8368-ad939da7193c\n        - name: CATTLE_INGRESS_IP_DOMAIN\n          value: sslip.io\n        - name: STRICT_VERIFY\n          value: \"true\"\n        image: rancher/rancher-agent:v2.11.2\n        imagePullPolicy: IfNotPresent\n        name: cluster-register\n        resources: {}\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n        volumeMounts:\n        - mountPath: /cattle-credentials\n          name: cattle-credentials\n          readOnly: true\n      dnsPolicy: ClusterFirst\n      restartPolicy: Always\n      schedulerName: default-scheduler\n      securityContext: {}\n      serviceAccount: cattle\n      serviceAccountName: cattle\n      terminationGracePeriodSeconds: 30\n      tolerations:\n      - effect: NoExecute\n        key: node-role.kubernetes.io/etcd\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n      volumes:\n      - name: cattle-credentials\n        secret:\n          defaultMode: 320\n          secretName: cattle-credentials-d683020906\nstatus:\n  conditions:\n  - lastTransitionTime: \"2025-08-27T13:04:18Z\"\n    lastUpdateTime: \"2025-08-27T13:05:15Z\"\n    message: ReplicaSet \"cattle-cluster-agent-6c5cfb747c\" has successfully progressed.\n    reason: NewReplicaSetAvailable\n    status: \"True\"\n    type: Progressing\n  - lastTransitionTime: \"2025-08-27T13:05:18Z\"\n    lastUpdateTime: \"2025-08-27T13:05:18Z\"\n    message: Deployment does not have minimum availability.\n    reason: MinimumReplicasUnavailable\n    status: \"False\"\n    type: Available\n  observedGeneration: 1\n  replicas: 1\n  unavailableReplicas: 1\n  updatedReplicas: 1"
}
2025-08-27 15:08:09,288 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Apply DNS fix to cattle-cluster-agent deployment] *************************************************************************************************************************************************************************************************
2025-08-27 15:08:09,608 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:08:09,614 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Show patch result] ********************************************************************************************************************************************************************************************************************************
2025-08-27 15:08:09,632 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "Patch applied: Yes\nOutput: deployment.apps/cattle-cluster-agent patched\nError: Warning: spec.template.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].key: beta.kubernetes.io/os is deprecated since v1.14; use \"kubernetes.io/os\" instead\nWarning: spec.template.spec.affinity.nodeAffinity.preferredDuringSchedulingIgnoredDuringExecution[2].preference.matchExpressions[0].key: node-role.kubernetes.io/master is use \"node-role.kubernetes.io/control-plane\" instead\n"
}
2025-08-27 15:08:09,637 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Wait for deployment rollout to complete] **********************************************************************************************************************************************************************************************************
2025-08-27 15:08:10,407 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:08:10,413 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Show rollout status] ******************************************************************************************************************************************************************************************************************************
2025-08-27 15:08:10,430 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "rollout_status.stdout": "Waiting for deployment \"cattle-cluster-agent\" rollout to finish: 1 old replicas are pending termination...\nWaiting for deployment \"cattle-cluster-agent\" rollout to finish: 1 old replicas are pending termination...\nWaiting for deployment \"cattle-cluster-agent\" rollout to finish: 1 old replicas are pending termination...\ndeployment \"cattle-cluster-agent\" successfully rolled out"
}
2025-08-27 15:08:10,435 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Verify final deployment configuration] ************************************************************************************************************************************************************************************************************
2025-08-27 15:08:10,793 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:08:10,798 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Show final configuration] *************************************************************************************************************************************************************************************************************************
2025-08-27 15:08:10,816 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "Final hostAliases configuration:\n      hostAliases:\n      - hostnames:\n        - ansible.app.iac-svil.almaviva.it\n        ip: 10.205.166.216\n      restartPolicy: Always\n      schedulerName: default-scheduler\n      securityContext: {}\n      serviceAccount: cattle\n      serviceAccountName: cattle\n      terminationGracePeriodSeconds: 30\n      tolerations:\n"
}
2025-08-27 15:08:10,821 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Check cattle-cluster-agent pods status] ***********************************************************************************************************************************************************************************************************
2025-08-27 15:08:11,148 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:08:11,153 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Show pods status] *********************************************************************************************************************************************************************************************************************************
2025-08-27 15:08:11,170 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "Cattle cluster agent pods:\nNAME                                    READY   STATUS    RESTARTS   AGE\ncattle-cluster-agent-6c5cfb747c-55mn6   0/1     Error     4          3m53s\ncattle-cluster-agent-7c6fbb45c4-fn28k   1/1     Running   0          2s\n"
}
2025-08-27 15:08:11,174 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Show summary] *************************************************************************************************************************************************************************************************************************************
2025-08-27 15:08:11,195 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "=== KUBECTL SETUP SUMMARY ===\n✅ Kubectl configured: Yes\n✅ Kubeconfig copied: Yes\n🔍 Cattle-system namespace: Found\n🔍 Cattle-cluster-agent deployment: Found\n🔧 DNS Fix applied: Yes\n\n"
}
2025-08-27 15:08:11,203 p=20016 u=root n=ansible INFO| PLAY [new_managers] *****************************************************************************************************************************************************************************************************************************************************
2025-08-27 15:08:11,209 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Copy kubectl binary to system path] ***************************************************************************************************************************************************************************************************************
2025-08-27 15:08:11,910 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:08:11,916 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Create .kube directory for root] ******************************************************************************************************************************************************************************************************************
2025-08-27 15:08:12,152 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:08:12,158 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Copy RKE2 kubeconfig to root .kube directory] *****************************************************************************************************************************************************************************************************
2025-08-27 15:08:12,391 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:08:12,397 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Verify kubectl is working] ************************************************************************************************************************************************************************************************************************
2025-08-27 15:08:12,690 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:08:12,696 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Show kubectl test result] *************************************************************************************************************************************************************************************************************************
2025-08-27 15:08:12,707 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "Kubectl test result:\nNAME            STATUS   ROLES                       AGE     VERSION\nrke2-manager2   Ready    control-plane,etcd,master   3m58s   v1.31.11+rke2r1\n"
}
2025-08-27 15:08:12,712 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Wait 60 seconds before applying DNS fix] **********************************************************************************************************************************************************************************************************
2025-08-27 15:08:12,720 p=20016 u=root n=ansible INFO| Pausing for 60 seconds
2025-08-27 15:08:12,721 p=20016 u=root n=ansible INFO| (ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
2025-08-27 15:09:12,724 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:09:12,729 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Wait for cattle-system namespace to exist] ********************************************************************************************************************************************************************************************************
2025-08-27 15:09:14,239 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:09:14,244 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Show namespace status] ****************************************************************************************************************************************************************************************************************************
2025-08-27 15:09:14,257 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "Cattle-system namespace: Found\n"
}
2025-08-27 15:09:14,261 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Wait for cattle-cluster-agent deployment to exist] ************************************************************************************************************************************************************************************************
2025-08-27 15:09:14,711 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:09:14,716 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Show deployment availability] *********************************************************************************************************************************************************************************************************************
2025-08-27 15:09:14,729 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "Cattle-cluster-agent deployment: Found\n"
}
2025-08-27 15:09:14,734 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Check current cattle-cluster-agent deployment] ****************************************************************************************************************************************************************************************************
2025-08-27 15:09:15,149 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:09:15,156 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Show current deployment (before changes)] *********************************************************************************************************************************************************************************************************
2025-08-27 15:09:15,174 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "current_deployment.stdout": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: \"3\"\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"metadata\":{\"annotations\":{\"management.cattle.io/scale-available\":\"2\"},\"name\":\"cattle-cluster-agent\",\"namespace\":\"cattle-system\"},\"spec\":{\"selector\":{\"matchLabels\":{\"app\":\"cattle-cluster-agent\"}},\"strategy\":{\"rollingUpdate\":{\"maxSurge\":1,\"maxUnavailable\":0},\"type\":\"RollingUpdate\"},\"template\":{\"metadata\":{\"labels\":{\"app\":\"cattle-cluster-agent\"}},\"spec\":{\"affinity\":{\"nodeAffinity\":{\"preferredDuringSchedulingIgnoredDuringExecution\":[{\"preference\":{\"matchExpressions\":[{\"key\":\"node-role.kubernetes.io/controlplane\",\"operator\":\"In\",\"values\":[\"true\"]}]},\"weight\":100},{\"preference\":{\"matchExpressions\":[{\"key\":\"node-role.kubernetes.io/control-plane\",\"operator\":\"In\",\"values\":[\"true\"]}]},\"weight\":100},{\"preference\":{\"matchExpressions\":[{\"key\":\"node-role.kubernetes.io/master\",\"operator\":\"In\",\"values\":[\"true\"]}]},\"weight\":100},{\"preference\":{\"matchExpressions\":[{\"key\":\"cattle.io/cluster-agent\",\"operator\":\"In\",\"values\":[\"true\"]}]},\"weight\":1}],\"requiredDuringSchedulingIgnoredDuringExecution\":{\"nodeSelectorTerms\":[{\"matchExpressions\":[{\"key\":\"beta.kubernetes.io/os\",\"operator\":\"NotIn\",\"values\":[\"windows\"]}]}]}},\"podAntiAffinity\":{\"preferredDuringSchedulingIgnoredDuringExecution\":[{\"podAffinityTerm\":{\"labelSelector\":{\"matchExpressions\":[{\"key\":\"app\",\"operator\":\"In\",\"values\":[\"cattle-cluster-agent\"]}]},\"topologyKey\":\"kubernetes.io/hostname\"},\"weight\":100}]}},\"containers\":[{\"env\":[{\"name\":\"CATTLE_FEATURES\",\"value\":\"embedded-cluster-api=false,fleet=false,managed-system-upgrade-controller=false,multi-cluster-management=false,multi-cluster-management-agent=true,provisioningprebootstrap=false,provisioningv2=false,rke2=false,ui-sql-cache=false\"},{\"name\":\"CATTLE_IS_RKE\",\"value\":\"false\"},{\"name\":\"CATTLE_SERVER\",\"value\":\"https://ansible.app.iac-svil.almaviva.it\"},{\"name\":\"CATTLE_CA_CHECKSUM\",\"value\":\"538463ba320a67b4723aebc71f8d798ce0416a6ff29ecf895661e0f7ac274349\"},{\"name\":\"CATTLE_CLUSTER\",\"value\":\"true\"},{\"name\":\"CATTLE_K8S_MANAGED\",\"value\":\"true\"},{\"name\":\"CATTLE_CLUSTER_REGISTRY\",\"value\":\"\"},{\"name\":\"CATTLE_CREDENTIAL_NAME\",\"value\":\"cattle-credentials-173217425b\"},{\"name\":\"CATTLE_SERVER_VERSION\",\"value\":\"v2.11.2\"},{\"name\":\"CATTLE_INSTALL_UUID\",\"value\":\"dea1affd-f93e-42eb-8368-ad939da7193c\"},{\"name\":\"CATTLE_INGRESS_IP_DOMAIN\",\"value\":\"sslip.io\"},{\"name\":\"STRICT_VERIFY\",\"value\":\"true\"}],\"image\":\"rancher/rancher-agent:v2.11.2\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"cluster-register\",\"volumeMounts\":[{\"mountPath\":\"/cattle-credentials\",\"name\":\"cattle-credentials\",\"readOnly\":true}]}],\"serviceAccountName\":\"cattle\",\"tolerations\":[{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/control-plane\"},{\"effect\":\"NoExecute\",\"key\":\"node-role.kubernetes.io/etcd\"}],\"volumes\":[{\"name\":\"cattle-credentials\",\"secret\":{\"defaultMode\":320,\"secretName\":\"cattle-credentials-173217425b\"}}]}}}}\n    management.cattle.io/scale-available: \"2\"\n    objectset.rio.cattle.io/applied: H4sIAAAAAAAA/8RW72/iOBD9V07+nFBIKD8i7Yeoze6htmxFYKXVqkKDPQFfjZ2znXRRxf9+chLa0NL2Vnen+wRJZt68Gb95ySOBnH9DbbiSJCKQ5+as7BGP3HPJSEQuMRdqt0VpiUe2aIGBBRI9EpBSWbBcSeMutyBhjS6uQ8FagR2uzgwFgT6UwAWsBJKIBMQjavUHUmvQdjRXrWju6r3zXD1I1P66vCcRuQ9N60nZ83674pJ9ihlT8kMICVvHhYrCWNQ+rOvuPk4yOVCXeV+s0Dc7Y3FL9h4RsEJRTeEtiA2Yjat4TkPA7jnAgGZD7PdYsOoGAY4BaD+jNAuHIRtlfQd6IFlh+C+5ttk0Ic98TI7UsTEokFql6/OxdHP9RBTy/C3wvUOwGiyudy5UKyG4XC9yBhZrqJ9poddIop7nLhaydcLdvUfsLne8ZkeJ7j5uc3EAaUlJ/G1aTWOQZVxyW9GTimHcus41Zqg1sstCc7lO6QZZ4XhM1lI93U5+Ii1spfkfhxyUFJ9GlfzMNRpTy/vHI7nHHYmqYr5WAjtOAlqiRePOlypptRK5AIlOSTlqqAZPJk6OJYgCHQ6xukByt7/be+QB+XpjSdTrdvfev8fB/19JbMEd2X9d/XmxXu3wL5Xd33lE458F/wW51IpLm9Wao97WzN6hu0ILL+akzDHZqbIv+D5wydSDqSjfOfHnisXS8n+sdcUOEI780/6lL83idC9uQ98b88ndrcduVa6EWu+uKqDjeWyUsZXjvVBF1blTNnCJuiaCsqx+G4e8iOfz62T5OYnni1mSHtiQiOB2hYwhe2aT808ZCINeJhBt879+dbHGQP0iX2tg6FNFnCiPq0zS5ewqadWoIE4EpsnsWzJrBW6szU10dgbS8JXADuR5hwP1TclFB8QWSl5Ch9sTWBfx8uL35OIqXdy0AM/DUX8QriAMujAYrvrDIARc0WEvG7HheESx2+8NYJBlwRhpNhqfDwY97GZDoMGwH/bHpwpdL9L5Eetqa14HXo3S5U08jb8klx8HN6jLWfJlks5n31sZp6JnyWUynU/i6+U0vmmP+iAtjQyl5SCMzwajsBt0x93Bm0ew/JbM0snXaQuoDDq9Xic4dbzTdB5fXy8Xi0m7MYbQgyxjfjYO0e8HuPJH4WDkAxuHYwbD3jikJ9G+zJI0XU5ul5dfb+JJm4MxgucdfqSxdD6bXMwd4cnn76/meucRvgX32iUaJN2gPmt+6y2LDm01cbeFELdKcOq2bZJNlb3VaJ4/H1pfQBrXvPHtUoliizeqkLbxNff3Fqz7fjl7fQTk1ZfK0TONwL5KsSORa8JZmUcM6pJTjCl10NN2NnEuIZy3PNkOZhlSW1lkbWQu6P2XEFrKqrm2chtv/Dj5+DXq+NYjMW3POdmrQarROv9kmEEh7I1iSKIw6B6eTd/KPhKyM739/q8AAAD//1B1BXWZCwAA\n    objectset.rio.cattle.io/id: \"\"\n    objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon\n    objectset.rio.cattle.io/owner-name: cluster-agent\n    objectset.rio.cattle.io/owner-namespace: kube-system\n  creationTimestamp: \"2025-08-27T13:04:15Z\"\n  generation: 3\n  labels:\n    objectset.rio.cattle.io/hash: c5c3ae05aa6cf7e41d2b022e9aac4fccf373d8f4\n  name: cattle-cluster-agent\n  namespace: cattle-system\n  resourceVersion: \"2010\"\n  uid: cfe82288-7bdf-4da2-b955-f9f8ed07444f\nspec:\n  progressDeadlineSeconds: 600\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: cattle-cluster-agent\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: cattle-cluster-agent\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - preference:\n              matchExpressions:\n              - key: node-role.kubernetes.io/controlplane\n                operator: In\n                values:\n                - \"true\"\n            weight: 100\n          - preference:\n              matchExpressions:\n              - key: node-role.kubernetes.io/control-plane\n                operator: In\n                values:\n                - \"true\"\n            weight: 100\n          - preference:\n              matchExpressions:\n              - key: node-role.kubernetes.io/master\n                operator: In\n                values:\n                - \"true\"\n            weight: 100\n          - preference:\n              matchExpressions:\n              - key: cattle.io/cluster-agent\n                operator: In\n                values:\n                - \"true\"\n            weight: 1\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/os\n                operator: NotIn\n                values:\n                - windows\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - cattle-cluster-agent\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      containers:\n      - env:\n        - name: CATTLE_FEATURES\n          value: embedded-cluster-api=false,fleet=false,managed-system-upgrade-controller=false,multi-cluster-management=false,multi-cluster-management-agent=true,provisioningprebootstrap=false,provisioningv2=false,rke2=false,ui-sql-cache=false\n        - name: CATTLE_IS_RKE\n          value: \"false\"\n        - name: CATTLE_SERVER\n          value: https://ansible.app.iac-svil.almaviva.it\n        - name: CATTLE_CA_CHECKSUM\n          value: 538463ba320a67b4723aebc71f8d798ce0416a6ff29ecf895661e0f7ac274349\n        - name: CATTLE_CLUSTER\n          value: \"true\"\n        - name: CATTLE_K8S_MANAGED\n          value: \"true\"\n        - name: CATTLE_CLUSTER_REGISTRY\n        - name: CATTLE_CREDENTIAL_NAME\n          value: cattle-credentials-173217425b\n        - name: CATTLE_SERVER_VERSION\n          value: v2.11.2\n        - name: CATTLE_INSTALL_UUID\n          value: dea1affd-f93e-42eb-8368-ad939da7193c\n        - name: CATTLE_INGRESS_IP_DOMAIN\n          value: sslip.io\n        - name: STRICT_VERIFY\n          value: \"true\"\n        image: rancher/rancher-agent:v2.11.2\n        imagePullPolicy: IfNotPresent\n        name: cluster-register\n        resources: {}\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n        volumeMounts:\n        - mountPath: /cattle-credentials\n          name: cattle-credentials\n          readOnly: true\n      dnsPolicy: ClusterFirst\n      hostAliases:\n      - hostnames:\n        - ansible.app.iac-svil.almaviva.it\n        ip: 10.205.166.216\n      restartPolicy: Always\n      schedulerName: default-scheduler\n      securityContext: {}\n      serviceAccount: cattle\n      serviceAccountName: cattle\n      terminationGracePeriodSeconds: 30\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n      - effect: NoExecute\n        key: node-role.kubernetes.io/etcd\n      volumes:\n      - name: cattle-credentials\n        secret:\n          defaultMode: 320\n          secretName: cattle-credentials-173217425b\nstatus:\n  availableReplicas: 1\n  conditions:\n  - lastTransitionTime: \"2025-08-27T13:08:10Z\"\n    lastUpdateTime: \"2025-08-27T13:08:10Z\"\n    message: Deployment has minimum availability.\n    reason: MinimumReplicasAvailable\n    status: \"True\"\n    type: Available\n  - lastTransitionTime: \"2025-08-27T13:04:18Z\"\n    lastUpdateTime: \"2025-08-27T13:08:19Z\"\n    message: ReplicaSet \"cattle-cluster-agent-d6994f9d\" has successfully progressed.\n    reason: NewReplicaSetAvailable\n    status: \"True\"\n    type: Progressing\n  observedGeneration: 3\n  readyReplicas: 1\n  replicas: 1\n  updatedReplicas: 1"
}
2025-08-27 15:09:15,178 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Apply DNS fix to cattle-cluster-agent deployment] *************************************************************************************************************************************************************************************************
2025-08-27 15:09:15,505 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:09:15,511 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Show patch result] ********************************************************************************************************************************************************************************************************************************
2025-08-27 15:09:15,529 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "Patch applied: Yes\nOutput: deployment.apps/cattle-cluster-agent patched (no change)\nError: \n"
}
2025-08-27 15:09:15,534 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Wait for deployment rollout to complete] **********************************************************************************************************************************************************************************************************
2025-08-27 15:09:15,850 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:09:15,855 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Show rollout status] ******************************************************************************************************************************************************************************************************************************
2025-08-27 15:09:15,872 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "rollout_status.stdout": "deployment \"cattle-cluster-agent\" successfully rolled out"
}
2025-08-27 15:09:15,876 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Verify final deployment configuration] ************************************************************************************************************************************************************************************************************
2025-08-27 15:09:16,171 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:09:16,177 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Show final configuration] *************************************************************************************************************************************************************************************************************************
2025-08-27 15:09:16,194 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "Final hostAliases configuration:\n      hostAliases:\n      - hostnames:\n        - ansible.app.iac-svil.almaviva.it\n        ip: 10.205.166.216\n      restartPolicy: Always\n      schedulerName: default-scheduler\n      securityContext: {}\n      serviceAccount: cattle\n      serviceAccountName: cattle\n      terminationGracePeriodSeconds: 30\n      tolerations:\n"
}
2025-08-27 15:09:16,199 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Check cattle-cluster-agent pods status] ***********************************************************************************************************************************************************************************************************
2025-08-27 15:09:16,525 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:09:16,536 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Show pods status] *********************************************************************************************************************************************************************************************************************************
2025-08-27 15:09:16,562 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "Cattle cluster agent pods:\nNAME                                  READY   STATUS    RESTARTS   AGE\ncattle-cluster-agent-d6994f9d-w9glp   1/1     Running   0          59s\n"
}
2025-08-27 15:09:16,572 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Show summary] *************************************************************************************************************************************************************************************************************************************
2025-08-27 15:09:16,599 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "=== KUBECTL SETUP SUMMARY ===\n✅ Kubectl configured: Yes\n✅ Kubeconfig copied: Yes\n🔍 Cattle-system namespace: Found\n🔍 Cattle-cluster-agent deployment: Found\n🔧 DNS Fix applied: Yes\n\n"
}
2025-08-27 15:09:16,609 p=20016 u=root n=ansible INFO| PLAY [Pausa prima NFS server] *******************************************************************************************************************************************************************************************************************************************
2025-08-27 15:09:16,616 p=20016 u=root n=ansible INFO| TASK [Attendi 60 secondi prima installazione NFS] ***********************************************************************************************************************************************************************************************************************
2025-08-27 15:09:16,624 p=20016 u=root n=ansible INFO| Pausing for 60 seconds
2025-08-27 15:09:16,624 p=20016 u=root n=ansible INFO| (ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
2025-08-27 15:09:16,624 p=20016 u=root n=ansible INFO| [Attendi 60 secondi prima installazione NFS]
Attendendo 60 secondi prima installazione NFS...:
2025-08-27 15:10:16,627 p=20016 u=root n=ansible INFO| ok: [localhost]
2025-08-27 15:10:16,630 p=20016 u=root n=ansible INFO| PLAY [nfs_server] *******************************************************************************************************************************************************************************************************************************************************
2025-08-27 15:10:16,638 p=20016 u=root n=ansible INFO| TASK [Gathering Facts] **************************************************************************************************************************************************************************************************************************************************
2025-08-27 15:10:18,135 p=20016 u=root n=ansible INFO| ok: [nfs-server-01]
2025-08-27 15:10:18,139 p=20016 u=root n=ansible INFO| TASK [nfs : Installing NFS] *********************************************************************************************************************************************************************************************************************************************
2025-08-27 15:10:18,149 p=20016 u=root n=ansible INFO| ok: [nfs-server-01] => {
    "msg": "Installing NFS"
}
2025-08-27 15:10:18,154 p=20016 u=root n=ansible INFO| TASK [nfs : Ubuntu + single mode] ***************************************************************************************************************************************************************************************************************************************
2025-08-27 15:10:18,170 p=20016 u=root n=ansible INFO| included: /root/IAC/COMPLETO-8-server/roles/nfs/tasks/nfs_ubuntu_single.yml for nfs-server-01
2025-08-27 15:10:18,174 p=20016 u=root n=ansible INFO| TASK [nfs : Installing nfs-common] **************************************************************************************************************************************************************************************************************************************
2025-08-27 15:10:30,992 p=20016 u=root n=ansible INFO| The following additional packages will be installed:
  libnfsidmap1 rpcbind
Suggested packages:
  watchdog
The following NEW packages will be installed:
  libnfsidmap1 nfs-common nfs-kernel-server rpcbind
0 upgraded, 4 newly installed, 0 to remove and 52 not upgraded.
2025-08-27 15:10:30,993 p=20016 u=root n=ansible INFO| changed: [nfs-server-01]
2025-08-27 15:10:30,998 p=20016 u=root n=ansible INFO| TASK [nfs : Creating directory] *****************************************************************************************************************************************************************************************************************************************
2025-08-27 15:10:31,238 p=20016 u=root n=ansible INFO| ok: [nfs-server-01] => (item=/share/backup)
2025-08-27 15:10:31,471 p=20016 u=root n=ansible INFO| ok: [nfs-server-01] => (item=/share/k8s)
2025-08-27 15:10:31,487 p=20016 u=root n=ansible INFO| TASK [nfs : Debug nfs_clients variable] *********************************************************************************************************************************************************************************************************************************
2025-08-27 15:10:31,506 p=20016 u=root n=ansible INFO| ok: [nfs-server-01] => {
    "nfs_clients": [
        "10.205.166.216",
        "10.205.166.218",
        "10.205.166.217",
        "10.205.166.214",
        "10.205.166.215"
    ]
}
2025-08-27 15:10:31,516 p=20016 u=root n=ansible INFO| TASK [nfs : Generate exports file from template] ************************************************************************************************************************************************************************************************************************
2025-08-27 15:10:32,480 p=20016 u=root n=ansible INFO| [0;31m--- before: /etc/exports[0m
[0;31m[0m[0;32m+++ after: /root/.ansible/tmp/ansible-local-20016_chgpspt/tmpp0t5skiv/exports.j2[0m
[0;32m[0m[0;36m@@ -1,10 +1,3 @@[0m
[0;36m[0m[0;31m-# /etc/exports: the access control list for filesystems which may be exported[0m
[0;31m[0m[0;31m-#		to NFS clients.  See exports(5).[0m
[0;31m[0m[0;31m-#[0m
[0;31m[0m[0;31m-# Example for NFSv2 and NFSv3:[0m
[0;31m[0m[0;31m-# /srv/homes       hostname1(rw,sync,no_subtree_check) hostname2(ro,sync,no_subtree_check)[0m
[0;31m[0m[0;31m-#[0m
[0;31m[0m[0;31m-# Example for NFSv4:[0m
[0;31m[0m[0;31m-# /srv/nfs4        gss/krb5i(rw,sync,fsid=0,crossmnt,no_subtree_check)[0m
[0;31m[0m[0;31m-# /srv/nfs4/homes  gss/krb5i(rw,sync,no_subtree_check)[0m
[0;31m[0m[0;31m-#[0m
[0;31m[0m[0;32m+# NFS exports managed by Ansible[0m
[0;32m[0m[0;32m+/share/backup 10.205.166.216(rw,sync,no_subtree_check,no_root_squash) 10.205.166.218(rw,sync,no_subtree_check,no_root_squash) 10.205.166.217(rw,sync,no_subtree_check,no_root_squash) 10.205.166.214(rw,sync,no_subtree_check,no_root_squash) 10.205.166.215(rw,sync,no_subtree_check,no_root_squash)[0m
[0;32m[0m[0;32m+/share/k8s 10.205.166.216(rw,sync,no_subtree_check,no_root_squash) 10.205.166.218(rw,sync,no_subtree_check,no_root_squash) 10.205.166.217(rw,sync,no_subtree_check,no_root_squash) 10.205.166.214(rw,sync,no_subtree_check,no_root_squash) 10.205.166.215(rw,sync,no_subtree_check,no_root_squash)[0m
[0;32m[0m

2025-08-27 15:10:32,481 p=20016 u=root n=ansible INFO| changed: [nfs-server-01]
2025-08-27 15:10:32,490 p=20016 u=root n=ansible INFO| TASK [nfs : Show exports file content] **********************************************************************************************************************************************************************************************************************************
2025-08-27 15:10:32,741 p=20016 u=root n=ansible INFO| changed: [nfs-server-01]
2025-08-27 15:10:32,748 p=20016 u=root n=ansible INFO| TASK [nfs : Display exports content] ************************************************************************************************************************************************************************************************************************************
2025-08-27 15:10:32,761 p=20016 u=root n=ansible INFO| ok: [nfs-server-01] => {
    "exports_check.stdout_lines": [
        "# NFS exports managed by Ansible",
        "/share/backup 10.205.166.216(rw,sync,no_subtree_check,no_root_squash) 10.205.166.218(rw,sync,no_subtree_check,no_root_squash) 10.205.166.217(rw,sync,no_subtree_check,no_root_squash) 10.205.166.214(rw,sync,no_subtree_check,no_root_squash) 10.205.166.215(rw,sync,no_subtree_check,no_root_squash)",
        "/share/k8s 10.205.166.216(rw,sync,no_subtree_check,no_root_squash) 10.205.166.218(rw,sync,no_subtree_check,no_root_squash) 10.205.166.217(rw,sync,no_subtree_check,no_root_squash) 10.205.166.214(rw,sync,no_subtree_check,no_root_squash) 10.205.166.215(rw,sync,no_subtree_check,no_root_squash)"
    ]
}
2025-08-27 15:10:32,765 p=20016 u=root n=ansible INFO| TASK [nfs : Restart and enable NFS services] ****************************************************************************************************************************************************************************************************************************
2025-08-27 15:10:33,399 p=20016 u=root n=ansible INFO| changed: [nfs-server-01]
2025-08-27 15:10:33,405 p=20016 u=root n=ansible INFO| TASK [nfs : Ubuntu + cluster mode] **************************************************************************************************************************************************************************************************************************************
2025-08-27 15:10:33,414 p=20016 u=root n=ansible INFO| skipping: [nfs-server-01]
2025-08-27 15:10:33,419 p=20016 u=root n=ansible INFO| TASK [nfs : RedHat + single mode] ***************************************************************************************************************************************************************************************************************************************
2025-08-27 15:10:33,428 p=20016 u=root n=ansible INFO| skipping: [nfs-server-01]
2025-08-27 15:10:33,433 p=20016 u=root n=ansible INFO| TASK [nfs : RedHat + cluster mode] **************************************************************************************************************************************************************************************************************************************
2025-08-27 15:10:33,441 p=20016 u=root n=ansible INFO| skipping: [nfs-server-01]
2025-08-27 15:10:33,452 p=20016 u=root n=ansible INFO| RUNNING HANDLER [nfs : reload exports] **********************************************************************************************************************************************************************************************************************************
2025-08-27 15:10:33,681 p=20016 u=root n=ansible INFO| changed: [nfs-server-01]
2025-08-27 15:10:33,684 p=20016 u=root n=ansible INFO| PLAY [Install NFS client prerequisites on all cluster nodes] ************************************************************************************************************************************************************************************************************
2025-08-27 15:10:33,690 p=20016 u=root n=ansible INFO| TASK [Gathering Facts] **************************************************************************************************************************************************************************************************************************************************
2025-08-27 15:10:35,502 p=20016 u=root n=ansible INFO| ok: [rke2-worker1]
2025-08-27 15:10:35,550 p=20016 u=root n=ansible INFO| ok: [rke2-worker3]
2025-08-27 15:10:35,912 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:10:36,313 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 15:10:36,582 p=20016 u=root n=ansible INFO| ok: [rke2-worker2]
2025-08-27 15:10:36,588 p=20016 u=root n=ansible INFO| TASK [nfs_client_setup : Detect OS family] ******************************************************************************************************************************************************************************************************************************
2025-08-27 15:10:37,064 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 15:10:37,077 p=20016 u=root n=ansible INFO| ok: [rke2-worker1]
2025-08-27 15:10:37,081 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:10:37,090 p=20016 u=root n=ansible INFO| ok: [rke2-worker2]
2025-08-27 15:10:37,104 p=20016 u=root n=ansible INFO| ok: [rke2-worker3]
2025-08-27 15:10:37,111 p=20016 u=root n=ansible INFO| TASK [nfs_client_setup : Install NFS utils on RedHat/CentOS/Rocky/AlmaLinux] ********************************************************************************************************************************************************************************************
2025-08-27 15:10:37,127 p=20016 u=root n=ansible INFO| skipping: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 15:10:37,140 p=20016 u=root n=ansible INFO| skipping: [rke2-manager2]
2025-08-27 15:10:37,151 p=20016 u=root n=ansible INFO| skipping: [rke2-worker1]
2025-08-27 15:10:37,164 p=20016 u=root n=ansible INFO| skipping: [rke2-worker2]
2025-08-27 15:10:37,171 p=20016 u=root n=ansible INFO| skipping: [rke2-worker3]
2025-08-27 15:10:37,177 p=20016 u=root n=ansible INFO| TASK [nfs_client_setup : Install NFS utils on Debian/Ubuntu (without update_cache first)] *******************************************************************************************************************************************************************************
2025-08-27 15:10:38,273 p=20016 u=root n=ansible INFO| ok: [rke2-worker1]
2025-08-27 15:10:38,360 p=20016 u=root n=ansible INFO| ok: [rke2-worker2]
2025-08-27 15:10:38,380 p=20016 u=root n=ansible INFO| ok: [rke2-worker3]
2025-08-27 15:10:38,398 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:10:38,472 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 15:10:38,480 p=20016 u=root n=ansible INFO| TASK [nfs_client_setup : Remove problematic PostgreSQL repository] ******************************************************************************************************************************************************************************************************
2025-08-27 15:10:38,496 p=20016 u=root n=ansible INFO| skipping: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 15:10:38,509 p=20016 u=root n=ansible INFO| skipping: [rke2-manager2]
2025-08-27 15:10:38,521 p=20016 u=root n=ansible INFO| skipping: [rke2-worker1]
2025-08-27 15:10:38,534 p=20016 u=root n=ansible INFO| skipping: [rke2-worker2]
2025-08-27 15:10:38,540 p=20016 u=root n=ansible INFO| skipping: [rke2-worker3]
2025-08-27 15:10:38,547 p=20016 u=root n=ansible INFO| TASK [nfs_client_setup : Clean apt cache] *******************************************************************************************************************************************************************************************************************************
2025-08-27 15:10:38,561 p=20016 u=root n=ansible INFO| skipping: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 15:10:38,573 p=20016 u=root n=ansible INFO| skipping: [rke2-manager2]
2025-08-27 15:10:38,585 p=20016 u=root n=ansible INFO| skipping: [rke2-worker1]
2025-08-27 15:10:38,597 p=20016 u=root n=ansible INFO| skipping: [rke2-worker2]
2025-08-27 15:10:38,603 p=20016 u=root n=ansible INFO| skipping: [rke2-worker3]
2025-08-27 15:10:38,610 p=20016 u=root n=ansible INFO| TASK [nfs_client_setup : Update apt cache with cleaned repos] ***********************************************************************************************************************************************************************************************************
2025-08-27 15:10:38,627 p=20016 u=root n=ansible INFO| skipping: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 15:10:38,640 p=20016 u=root n=ansible INFO| skipping: [rke2-manager2]
2025-08-27 15:10:38,653 p=20016 u=root n=ansible INFO| skipping: [rke2-worker1]
2025-08-27 15:10:38,665 p=20016 u=root n=ansible INFO| skipping: [rke2-worker2]
2025-08-27 15:10:38,672 p=20016 u=root n=ansible INFO| skipping: [rke2-worker3]
2025-08-27 15:10:38,679 p=20016 u=root n=ansible INFO| TASK [nfs_client_setup : Retry installing NFS utils] ********************************************************************************************************************************************************************************************************************
2025-08-27 15:10:38,693 p=20016 u=root n=ansible INFO| skipping: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 15:10:38,705 p=20016 u=root n=ansible INFO| skipping: [rke2-manager2]
2025-08-27 15:10:38,718 p=20016 u=root n=ansible INFO| skipping: [rke2-worker1]
2025-08-27 15:10:38,730 p=20016 u=root n=ansible INFO| skipping: [rke2-worker2]
2025-08-27 15:10:38,736 p=20016 u=root n=ansible INFO| skipping: [rke2-worker3]
2025-08-27 15:10:38,744 p=20016 u=root n=ansible INFO| TASK [nfs_client_setup : Install NFS utils on SUSE/openSUSE] ************************************************************************************************************************************************************************************************************
2025-08-27 15:10:38,759 p=20016 u=root n=ansible INFO| skipping: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 15:10:38,771 p=20016 u=root n=ansible INFO| skipping: [rke2-manager2]
2025-08-27 15:10:38,785 p=20016 u=root n=ansible INFO| skipping: [rke2-worker1]
2025-08-27 15:10:38,797 p=20016 u=root n=ansible INFO| skipping: [rke2-worker2]
2025-08-27 15:10:38,803 p=20016 u=root n=ansible INFO| skipping: [rke2-worker3]
2025-08-27 15:10:38,810 p=20016 u=root n=ansible INFO| TASK [nfs_client_setup : Start and enable rpcbind service] **************************************************************************************************************************************************************************************************************
2025-08-27 15:10:39,325 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:10:39,330 p=20016 u=root n=ansible INFO| ok: [rke2-worker1]
2025-08-27 15:10:39,352 p=20016 u=root n=ansible INFO| ok: [rke2-worker3]
2025-08-27 15:10:39,355 p=20016 u=root n=ansible INFO| ok: [rke2-worker2]
2025-08-27 15:10:39,358 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 15:10:39,365 p=20016 u=root n=ansible INFO| TASK [nfs_client_setup : Test NFS mount capability] *********************************************************************************************************************************************************************************************************************
2025-08-27 15:10:39,668 p=20016 u=root n=ansible INFO| changed: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 15:10:39,676 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:10:39,717 p=20016 u=root n=ansible INFO| changed: [rke2-worker1]
2025-08-27 15:10:39,740 p=20016 u=root n=ansible INFO| changed: [rke2-worker2]
2025-08-27 15:10:39,743 p=20016 u=root n=ansible INFO| changed: [rke2-worker3]
2025-08-27 15:10:39,751 p=20016 u=root n=ansible INFO| TASK [nfs_client_setup : Display available NFS exports] *****************************************************************************************************************************************************************************************************************
2025-08-27 15:10:39,783 p=20016 u=root n=ansible INFO| ok: [cornelio.app.iac-svil.almaviva.it] => {
    "msg": "NFS server 10.205.166.219 exports:\nExport list for 10.205.166.219:\n/share/k8s    10.205.166.215,10.205.166.214,10.205.166.217,10.205.166.218,10.205.166.216\n/share/backup 10.205.166.215,10.205.166.214,10.205.166.217,10.205.166.218,10.205.166.216\n"
}
2025-08-27 15:10:39,798 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "NFS server 10.205.166.219 exports:\nExport list for 10.205.166.219:\n/share/k8s    10.205.166.215,10.205.166.214,10.205.166.217,10.205.166.218,10.205.166.216\n/share/backup 10.205.166.215,10.205.166.214,10.205.166.217,10.205.166.218,10.205.166.216\n"
}
2025-08-27 15:10:39,813 p=20016 u=root n=ansible INFO| ok: [rke2-worker1] => {
    "msg": "NFS server 10.205.166.219 exports:\nExport list for 10.205.166.219:\n/share/k8s    10.205.166.215,10.205.166.214,10.205.166.217,10.205.166.218,10.205.166.216\n/share/backup 10.205.166.215,10.205.166.214,10.205.166.217,10.205.166.218,10.205.166.216\n"
}
2025-08-27 15:10:39,814 p=20016 u=root n=ansible INFO| ok: [rke2-worker2] => {
    "msg": "NFS server 10.205.166.219 exports:\nExport list for 10.205.166.219:\n/share/k8s    10.205.166.215,10.205.166.214,10.205.166.217,10.205.166.218,10.205.166.216\n/share/backup 10.205.166.215,10.205.166.214,10.205.166.217,10.205.166.218,10.205.166.216\n"
}
2025-08-27 15:10:39,822 p=20016 u=root n=ansible INFO| ok: [rke2-worker3] => {
    "msg": "NFS server 10.205.166.219 exports:\nExport list for 10.205.166.219:\n/share/k8s    10.205.166.215,10.205.166.214,10.205.166.217,10.205.166.218,10.205.166.216\n/share/backup 10.205.166.215,10.205.166.214,10.205.166.217,10.205.166.218,10.205.166.216\n"
}
2025-08-27 15:10:39,829 p=20016 u=root n=ansible INFO| TASK [nfs_client_setup : Warn if NFS server not reachable] **************************************************************************************************************************************************************************************************************
2025-08-27 15:10:39,844 p=20016 u=root n=ansible INFO| skipping: [cornelio.app.iac-svil.almaviva.it]
2025-08-27 15:10:39,856 p=20016 u=root n=ansible INFO| skipping: [rke2-manager2]
2025-08-27 15:10:39,867 p=20016 u=root n=ansible INFO| skipping: [rke2-worker1]
2025-08-27 15:10:39,882 p=20016 u=root n=ansible INFO| skipping: [rke2-worker2]
2025-08-27 15:10:39,888 p=20016 u=root n=ansible INFO| skipping: [rke2-worker3]
2025-08-27 15:10:39,911 p=20016 u=root n=ansible INFO| PLAY [Install NFS Subdir External Provisioner] **************************************************************************************************************************************************************************************************************************
2025-08-27 15:10:39,919 p=20016 u=root n=ansible INFO| TASK [Gathering Facts] **************************************************************************************************************************************************************************************************************************************************
2025-08-27 15:10:40,831 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:10:40,836 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Ensure /root/.kube/config exists (fail early if not)] *******************************************************************************************************************************************************************************************
2025-08-27 15:10:41,099 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:10:41,105 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Fail if kubeconfig not found (kubectl must be configured on control node)] **********************************************************************************************************************************************************************
2025-08-27 15:10:41,115 p=20016 u=root n=ansible INFO| skipping: [rke2-manager2]
2025-08-27 15:10:41,121 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Ensure helm is installed (fast check)] **********************************************************************************************************************************************************************************************************
2025-08-27 15:10:41,353 p=20016 u=root n=ansible INFO| fatal: [rke2-manager2]: FAILED! => {"changed": false, "cmd": "helm version --short", "msg": "[Errno 2] No such file or directory: b'helm'", "rc": 2, "stderr": "", "stderr_lines": [], "stdout": "", "stdout_lines": []}
2025-08-27 15:10:41,353 p=20016 u=root n=ansible INFO| ...ignoring
2025-08-27 15:10:41,359 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Create temp dir for helm] ***********************************************************************************************************************************************************************************************************************
2025-08-27 15:10:41,598 p=20016 u=root n=ansible INFO| [0;31m--- before[0m
[0;31m[0m[0;32m+++ after[0m
[0;32m[0m[0;36m@@ -1,4 +1,4 @@[0m
[0;36m[0m {
     "path": "/tmp/helm_install",
[0;31m-    "state": "absent"[0m
[0;31m[0m[0;32m+    "state": "directory"[0m
[0;32m[0m }


2025-08-27 15:10:41,599 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:10:41,605 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Download Helm tarball] **************************************************************************************************************************************************************************************************************************
2025-08-27 15:10:42,550 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:10:42,557 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Extract helm and move binary] *******************************************************************************************************************************************************************************************************************
2025-08-27 15:10:44,043 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:10:44,049 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Move helm binary to /usr/local/bin] *************************************************************************************************************************************************************************************************************
2025-08-27 15:10:44,500 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:10:44,507 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Add Helm repo for nfs-subdir-external-provisioner] **********************************************************************************************************************************************************************************************
2025-08-27 15:10:44,818 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:10:44,827 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Update Helm repos] ******************************************************************************************************************************************************************************************************************************
2025-08-27 15:10:45,735 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:10:45,741 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Ensure Helm repo is reachable (search)] *********************************************************************************************************************************************************************************************************
2025-08-27 15:10:46,674 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:10:46,680 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Set KUBECONFIG environment for kubectl/helm interactions] ***************************************************************************************************************************************************************************************
2025-08-27 15:10:46,691 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:10:46,699 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Process each NFS export] ************************************************************************************************************************************************************************************************************************
2025-08-27 15:10:46,720 p=20016 u=root n=ansible INFO| included: /root/IAC/COMPLETO-8-server/roles/nfs_provisioner/tasks/process_nfs_export.yml for rke2-manager2 => (item=/share/k8s)
2025-08-27 15:10:46,723 p=20016 u=root n=ansible INFO| included: /root/IAC/COMPLETO-8-server/roles/nfs_provisioner/tasks/process_nfs_export.yml for rke2-manager2 => (item=/share/backup)
2025-08-27 15:10:46,728 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Check if namespace exists] **********************************************************************************************************************************************************************************************************************
2025-08-27 15:10:47,073 p=20016 u=root n=ansible INFO| fatal: [rke2-manager2]: FAILED! => {"changed": true, "cmd": ["kubectl", "get", "namespace", "nfs-provisioner-k8s"], "delta": "0:00:00.093896", "end": "2025-08-27 15:10:47.041544", "msg": "non-zero return code", "rc": 1, "start": "2025-08-27 15:10:46.947648", "stderr": "Error from server (NotFound): namespaces \"nfs-provisioner-k8s\" not found", "stderr_lines": ["Error from server (NotFound): namespaces \"nfs-provisioner-k8s\" not found"], "stdout": "", "stdout_lines": []}
2025-08-27 15:10:47,073 p=20016 u=root n=ansible INFO| ...ignoring
2025-08-27 15:10:47,079 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Create namespace for provisioner] ***************************************************************************************************************************************************************************************************************
2025-08-27 15:10:47,411 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:10:47,425 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Build temporary values file for helm] ***********************************************************************************************************************************************************************************************************
2025-08-27 15:10:48,117 p=20016 u=root n=ansible INFO| [0;31m--- before[0m
[0;31m[0m[0;32m+++ after: /tmp/nfs-subdir-external-provisioner-k8s-values.yaml[0m
[0;32m[0m[0;36m@@ -0,0 +1,9 @@[0m
[0;36m[0m[0;32m+# Generated by Ansible: values for nfs-subdir-external-provisioner-k8s[0m
[0;32m[0m[0;32m+nfs:[0m
[0;32m[0m[0;32m+  server: "10.205.166.219"[0m
[0;32m[0m[0;32m+  path: "/share/k8s"[0m
[0;32m[0m[0;32m+# Try to set storageClass name and whether it becomes the default[0m
[0;32m[0m[0;32m+storageClass:[0m
[0;32m[0m[0;32m+  name: "nfs-k8s"[0m
[0;32m[0m[0;32m+persistence:[0m
[0;32m[0m[0;32m+  defaultClass: false[0m
[0;32m[0m

2025-08-27 15:10:48,118 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:10:48,124 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Helm upgrade --install provisioner] *************************************************************************************************************************************************************************************************************
2025-08-27 15:10:52,948 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:10:52,955 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Wait for provisioner deployment to be ready] ****************************************************************************************************************************************************************************************************
2025-08-27 15:10:53,277 p=20016 u=root n=ansible INFO| fatal: [rke2-manager2]: FAILED! => {"changed": true, "cmd": ["kubectl", "-n", "nfs-provisioner-k8s", "rollout", "status", "deploy", "nfs-client-provisioner", "--timeout=15m"], "delta": "0:00:00.071704", "end": "2025-08-27 15:10:53.247173", "failed_when_result": true, "msg": "non-zero return code", "rc": 1, "start": "2025-08-27 15:10:53.175469", "stderr": "Error from server (NotFound): deployments.apps \"nfs-client-provisioner\" not found", "stderr_lines": ["Error from server (NotFound): deployments.apps \"nfs-client-provisioner\" not found"], "stdout": "", "stdout_lines": []}
2025-08-27 15:10:53,278 p=20016 u=root n=ansible INFO| ...ignoring
2025-08-27 15:10:53,284 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Print quick status] *****************************************************************************************************************************************************************************************************************************
2025-08-27 15:10:53,317 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "Release nfs-subdir-external-provisioner-k8s installed in namespace nfs-provisioner-k8s (export: /share/k8s). \nStorageClass name suggested: nfs-k8s. \nHelm status rc=0 rollout_rc=1\n"
}
2025-08-27 15:10:53,324 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Check if namespace exists] **********************************************************************************************************************************************************************************************************************
2025-08-27 15:10:53,642 p=20016 u=root n=ansible INFO| fatal: [rke2-manager2]: FAILED! => {"changed": true, "cmd": ["kubectl", "get", "namespace", "nfs-provisioner-backup"], "delta": "0:00:00.064216", "end": "2025-08-27 15:10:53.612552", "msg": "non-zero return code", "rc": 1, "start": "2025-08-27 15:10:53.548336", "stderr": "Error from server (NotFound): namespaces \"nfs-provisioner-backup\" not found", "stderr_lines": ["Error from server (NotFound): namespaces \"nfs-provisioner-backup\" not found"], "stdout": "", "stdout_lines": []}
2025-08-27 15:10:53,642 p=20016 u=root n=ansible INFO| ...ignoring
2025-08-27 15:10:53,649 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Create namespace for provisioner] ***************************************************************************************************************************************************************************************************************
2025-08-27 15:10:53,978 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:10:53,984 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Build temporary values file for helm] ***********************************************************************************************************************************************************************************************************
2025-08-27 15:10:54,647 p=20016 u=root n=ansible INFO| [0;31m--- before[0m
[0;31m[0m[0;32m+++ after: /tmp/nfs-subdir-external-provisioner-backup-values.yaml[0m
[0;32m[0m[0;36m@@ -0,0 +1,9 @@[0m
[0;36m[0m[0;32m+# Generated by Ansible: values for nfs-subdir-external-provisioner-backup[0m
[0;32m[0m[0;32m+nfs:[0m
[0;32m[0m[0;32m+  server: "10.205.166.219"[0m
[0;32m[0m[0;32m+  path: "/share/backup"[0m
[0;32m[0m[0;32m+# Try to set storageClass name and whether it becomes the default[0m
[0;32m[0m[0;32m+storageClass:[0m
[0;32m[0m[0;32m+  name: "nfs-backup"[0m
[0;32m[0m[0;32m+persistence:[0m
[0;32m[0m[0;32m+  defaultClass: false[0m
[0;32m[0m

2025-08-27 15:10:54,648 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:10:54,656 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Helm upgrade --install provisioner] *************************************************************************************************************************************************************************************************************
2025-08-27 15:10:59,321 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:10:59,328 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Wait for provisioner deployment to be ready] ****************************************************************************************************************************************************************************************************
2025-08-27 15:10:59,655 p=20016 u=root n=ansible INFO| fatal: [rke2-manager2]: FAILED! => {"changed": true, "cmd": ["kubectl", "-n", "nfs-provisioner-backup", "rollout", "status", "deploy", "nfs-client-provisioner", "--timeout=15m"], "delta": "0:00:00.064428", "end": "2025-08-27 15:10:59.624235", "failed_when_result": true, "msg": "non-zero return code", "rc": 1, "start": "2025-08-27 15:10:59.559807", "stderr": "Error from server (NotFound): deployments.apps \"nfs-client-provisioner\" not found", "stderr_lines": ["Error from server (NotFound): deployments.apps \"nfs-client-provisioner\" not found"], "stdout": "", "stdout_lines": []}
2025-08-27 15:10:59,655 p=20016 u=root n=ansible INFO| ...ignoring
2025-08-27 15:10:59,662 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Print quick status] *****************************************************************************************************************************************************************************************************************************
2025-08-27 15:10:59,695 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "Release nfs-subdir-external-provisioner-backup installed in namespace nfs-provisioner-backup (export: /share/backup). \nStorageClass name suggested: nfs-backup. \nHelm status rc=0 rollout_rc=1\n"
}
2025-08-27 15:10:59,702 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Show all StorageClasses] ************************************************************************************************************************************************************************************************************************
2025-08-27 15:11:00,006 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:11:00,013 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Filter NFS StorageClasses] **********************************************************************************************************************************************************************************************************************
2025-08-27 15:11:00,306 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:11:00,313 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Display all storageclasses] *********************************************************************************************************************************************************************************************************************
2025-08-27 15:11:00,325 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "sc_list.stdout_lines": [
        "NAME         PROVISIONER                                            RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE",
        "nfs-backup   cluster.local/nfs-subdir-external-provisioner-backup   Delete          Immediate           true                   4s",
        "nfs-k8s      cluster.local/nfs-subdir-external-provisioner-k8s      Delete          Immediate           true                   11s"
    ]
}
2025-08-27 15:11:00,330 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Display NFS storageclasses found] ***************************************************************************************************************************************************************************************************************
2025-08-27 15:11:00,342 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "nfs_sc_list.stdout_lines": [
        "nfs-backup   cluster.local/nfs-subdir-external-provisioner-backup   Delete          Immediate           true                   5s",
        "nfs-k8s      cluster.local/nfs-subdir-external-provisioner-k8s      Delete          Immediate           true                   12s"
    ]
}
2025-08-27 15:11:00,348 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Apply test claim and pod] ***********************************************************************************************************************************************************************************************************************
2025-08-27 15:11:00,371 p=20016 u=root n=ansible INFO| skipping: [rke2-manager2]
2025-08-27 15:11:00,377 p=20016 u=root n=ansible INFO| TASK [nfs_provisioner : Wait for test pod to be running] ****************************************************************************************************************************************************************************************************************
2025-08-27 15:11:00,401 p=20016 u=root n=ansible INFO| skipping: [rke2-manager2]
2025-08-27 15:11:00,409 p=20016 u=root n=ansible INFO| PLAY [new_managers] *****************************************************************************************************************************************************************************************************************************************************
2025-08-27 15:11:00,417 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Copy kubectl binary to system path] ***************************************************************************************************************************************************************************************************************
2025-08-27 15:11:01,279 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:11:01,288 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Create .kube directory for root] ******************************************************************************************************************************************************************************************************************
2025-08-27 15:11:01,539 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:11:01,549 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Copy RKE2 kubeconfig to root .kube directory] *****************************************************************************************************************************************************************************************************
2025-08-27 15:11:01,818 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:11:01,824 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Verify kubectl is working] ************************************************************************************************************************************************************************************************************************
2025-08-27 15:11:02,149 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:11:02,163 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Show kubectl test result] *************************************************************************************************************************************************************************************************************************
2025-08-27 15:11:02,182 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "Kubectl test result:\nNAME            STATUS   ROLES                       AGE     VERSION\nrke2-manager2   Ready    control-plane,etcd,master   6m48s   v1.31.11+rke2r1\nrke2-worker1    Ready    worker                      100s    v1.31.11+rke2r1\nrke2-worker2    Ready    worker                      101s    v1.31.11+rke2r1\nrke2-worker3    Ready    worker                      100s    v1.31.11+rke2r1\n"
}
2025-08-27 15:11:02,194 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Wait 60 seconds before applying DNS fix] **********************************************************************************************************************************************************************************************************
2025-08-27 15:11:02,208 p=20016 u=root n=ansible INFO| Pausing for 60 seconds
2025-08-27 15:11:02,208 p=20016 u=root n=ansible INFO| (ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
2025-08-27 15:12:02,211 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:12:02,217 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Wait for cattle-system namespace to exist] ********************************************************************************************************************************************************************************************************
2025-08-27 15:12:03,261 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:12:03,268 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Show namespace status] ****************************************************************************************************************************************************************************************************************************
2025-08-27 15:12:03,281 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "Cattle-system namespace: Found\n"
}
2025-08-27 15:12:03,287 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Wait for cattle-cluster-agent deployment to exist] ************************************************************************************************************************************************************************************************
2025-08-27 15:12:03,592 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:12:03,599 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Show deployment availability] *********************************************************************************************************************************************************************************************************************
2025-08-27 15:12:03,612 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "Cattle-cluster-agent deployment: Found\n"
}
2025-08-27 15:12:03,619 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Check current cattle-cluster-agent deployment] ****************************************************************************************************************************************************************************************************
2025-08-27 15:12:03,917 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:12:03,924 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Show current deployment (before changes)] *********************************************************************************************************************************************************************************************************
2025-08-27 15:12:03,942 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "current_deployment.stdout": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: \"3\"\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"metadata\":{\"annotations\":{\"management.cattle.io/scale-available\":\"2\"},\"name\":\"cattle-cluster-agent\",\"namespace\":\"cattle-system\"},\"spec\":{\"selector\":{\"matchLabels\":{\"app\":\"cattle-cluster-agent\"}},\"strategy\":{\"rollingUpdate\":{\"maxSurge\":1,\"maxUnavailable\":0},\"type\":\"RollingUpdate\"},\"template\":{\"metadata\":{\"labels\":{\"app\":\"cattle-cluster-agent\"}},\"spec\":{\"affinity\":{\"nodeAffinity\":{\"preferredDuringSchedulingIgnoredDuringExecution\":[{\"preference\":{\"matchExpressions\":[{\"key\":\"node-role.kubernetes.io/controlplane\",\"operator\":\"In\",\"values\":[\"true\"]}]},\"weight\":100},{\"preference\":{\"matchExpressions\":[{\"key\":\"node-role.kubernetes.io/control-plane\",\"operator\":\"In\",\"values\":[\"true\"]}]},\"weight\":100},{\"preference\":{\"matchExpressions\":[{\"key\":\"node-role.kubernetes.io/master\",\"operator\":\"In\",\"values\":[\"true\"]}]},\"weight\":100},{\"preference\":{\"matchExpressions\":[{\"key\":\"cattle.io/cluster-agent\",\"operator\":\"In\",\"values\":[\"true\"]}]},\"weight\":1}],\"requiredDuringSchedulingIgnoredDuringExecution\":{\"nodeSelectorTerms\":[{\"matchExpressions\":[{\"key\":\"beta.kubernetes.io/os\",\"operator\":\"NotIn\",\"values\":[\"windows\"]}]}]}},\"podAntiAffinity\":{\"preferredDuringSchedulingIgnoredDuringExecution\":[{\"podAffinityTerm\":{\"labelSelector\":{\"matchExpressions\":[{\"key\":\"app\",\"operator\":\"In\",\"values\":[\"cattle-cluster-agent\"]}]},\"topologyKey\":\"kubernetes.io/hostname\"},\"weight\":100}]}},\"containers\":[{\"env\":[{\"name\":\"CATTLE_FEATURES\",\"value\":\"embedded-cluster-api=false,fleet=false,managed-system-upgrade-controller=false,multi-cluster-management=false,multi-cluster-management-agent=true,provisioningprebootstrap=false,provisioningv2=false,rke2=false,ui-sql-cache=false\"},{\"name\":\"CATTLE_IS_RKE\",\"value\":\"false\"},{\"name\":\"CATTLE_SERVER\",\"value\":\"https://ansible.app.iac-svil.almaviva.it\"},{\"name\":\"CATTLE_CA_CHECKSUM\",\"value\":\"538463ba320a67b4723aebc71f8d798ce0416a6ff29ecf895661e0f7ac274349\"},{\"name\":\"CATTLE_CLUSTER\",\"value\":\"true\"},{\"name\":\"CATTLE_K8S_MANAGED\",\"value\":\"true\"},{\"name\":\"CATTLE_CLUSTER_REGISTRY\",\"value\":\"\"},{\"name\":\"CATTLE_CREDENTIAL_NAME\",\"value\":\"cattle-credentials-173217425b\"},{\"name\":\"CATTLE_SERVER_VERSION\",\"value\":\"v2.11.2\"},{\"name\":\"CATTLE_INSTALL_UUID\",\"value\":\"dea1affd-f93e-42eb-8368-ad939da7193c\"},{\"name\":\"CATTLE_INGRESS_IP_DOMAIN\",\"value\":\"sslip.io\"},{\"name\":\"STRICT_VERIFY\",\"value\":\"true\"}],\"image\":\"rancher/rancher-agent:v2.11.2\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"cluster-register\",\"volumeMounts\":[{\"mountPath\":\"/cattle-credentials\",\"name\":\"cattle-credentials\",\"readOnly\":true}]}],\"serviceAccountName\":\"cattle\",\"tolerations\":[{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/control-plane\"},{\"effect\":\"NoExecute\",\"key\":\"node-role.kubernetes.io/etcd\"}],\"volumes\":[{\"name\":\"cattle-credentials\",\"secret\":{\"defaultMode\":320,\"secretName\":\"cattle-credentials-173217425b\"}}]}}}}\n    management.cattle.io/scale-available: \"2\"\n    objectset.rio.cattle.io/applied: H4sIAAAAAAAA/8RW72/iOBD9V07+nFBIKD8i7Yeoze6htmxFYKXVqkKDPQFfjZ2znXRRxf9+chLa0NL2Vnen+wRJZt68Gb95ySOBnH9DbbiSJCKQ5+as7BGP3HPJSEQuMRdqt0VpiUe2aIGBBRI9EpBSWbBcSeMutyBhjS6uQ8FagR2uzgwFgT6UwAWsBJKIBMQjavUHUmvQdjRXrWju6r3zXD1I1P66vCcRuQ9N60nZ83674pJ9ihlT8kMICVvHhYrCWNQ+rOvuPk4yOVCXeV+s0Dc7Y3FL9h4RsEJRTeEtiA2Yjat4TkPA7jnAgGZD7PdYsOoGAY4BaD+jNAuHIRtlfQd6IFlh+C+5ttk0Ic98TI7UsTEokFql6/OxdHP9RBTy/C3wvUOwGiyudy5UKyG4XC9yBhZrqJ9poddIop7nLhaydcLdvUfsLne8ZkeJ7j5uc3EAaUlJ/G1aTWOQZVxyW9GTimHcus41Zqg1sstCc7lO6QZZ4XhM1lI93U5+Ii1spfkfhxyUFJ9GlfzMNRpTy/vHI7nHHYmqYr5WAjtOAlqiRePOlypptRK5AIlOSTlqqAZPJk6OJYgCHQ6xukByt7/be+QB+XpjSdTrdvfev8fB/19JbMEd2X9d/XmxXu3wL5Xd33lE458F/wW51IpLm9Wao97WzN6hu0ILL+akzDHZqbIv+D5wydSDqSjfOfHnisXS8n+sdcUOEI780/6lL83idC9uQ98b88ndrcduVa6EWu+uKqDjeWyUsZXjvVBF1blTNnCJuiaCsqx+G4e8iOfz62T5OYnni1mSHtiQiOB2hYwhe2aT808ZCINeJhBt879+dbHGQP0iX2tg6FNFnCiPq0zS5ewqadWoIE4EpsnsWzJrBW6szU10dgbS8JXADuR5hwP1TclFB8QWSl5Ch9sTWBfx8uL35OIqXdy0AM/DUX8QriAMujAYrvrDIARc0WEvG7HheESx2+8NYJBlwRhpNhqfDwY97GZDoMGwH/bHpwpdL9L5Eetqa14HXo3S5U08jb8klx8HN6jLWfJlks5n31sZp6JnyWUynU/i6+U0vmmP+iAtjQyl5SCMzwajsBt0x93Bm0ew/JbM0snXaQuoDDq9Xic4dbzTdB5fXy8Xi0m7MYbQgyxjfjYO0e8HuPJH4WDkAxuHYwbD3jikJ9G+zJI0XU5ul5dfb+JJm4MxgucdfqSxdD6bXMwd4cnn76/meucRvgX32iUaJN2gPmt+6y2LDm01cbeFELdKcOq2bZJNlb3VaJ4/H1pfQBrXvPHtUoliizeqkLbxNff3Fqz7fjl7fQTk1ZfK0TONwL5KsSORa8JZmUcM6pJTjCl10NN2NnEuIZy3PNkOZhlSW1lkbWQu6P2XEFrKqrm2chtv/Dj5+DXq+NYjMW3POdmrQarROv9kmEEh7I1iSKIw6B6eTd/KPhKyM739/q8AAAD//1B1BXWZCwAA\n    objectset.rio.cattle.io/id: \"\"\n    objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon\n    objectset.rio.cattle.io/owner-name: cluster-agent\n    objectset.rio.cattle.io/owner-namespace: kube-system\n  creationTimestamp: \"2025-08-27T13:04:15Z\"\n  generation: 4\n  labels:\n    objectset.rio.cattle.io/hash: c5c3ae05aa6cf7e41d2b022e9aac4fccf373d8f4\n  name: cattle-cluster-agent\n  namespace: cattle-system\n  resourceVersion: \"2563\"\n  uid: cfe82288-7bdf-4da2-b955-f9f8ed07444f\nspec:\n  progressDeadlineSeconds: 600\n  replicas: 2\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: cattle-cluster-agent\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: cattle-cluster-agent\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - preference:\n              matchExpressions:\n              - key: node-role.kubernetes.io/controlplane\n                operator: In\n                values:\n                - \"true\"\n            weight: 100\n          - preference:\n              matchExpressions:\n              - key: node-role.kubernetes.io/control-plane\n                operator: In\n                values:\n                - \"true\"\n            weight: 100\n          - preference:\n              matchExpressions:\n              - key: node-role.kubernetes.io/master\n                operator: In\n                values:\n                - \"true\"\n            weight: 100\n          - preference:\n              matchExpressions:\n              - key: cattle.io/cluster-agent\n                operator: In\n                values:\n                - \"true\"\n            weight: 1\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/os\n                operator: NotIn\n                values:\n                - windows\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - cattle-cluster-agent\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      containers:\n      - env:\n        - name: CATTLE_FEATURES\n          value: embedded-cluster-api=false,fleet=false,managed-system-upgrade-controller=false,multi-cluster-management=false,multi-cluster-management-agent=true,provisioningprebootstrap=false,provisioningv2=false,rke2=false,ui-sql-cache=false\n        - name: CATTLE_IS_RKE\n          value: \"false\"\n        - name: CATTLE_SERVER\n          value: https://ansible.app.iac-svil.almaviva.it\n        - name: CATTLE_CA_CHECKSUM\n          value: 538463ba320a67b4723aebc71f8d798ce0416a6ff29ecf895661e0f7ac274349\n        - name: CATTLE_CLUSTER\n          value: \"true\"\n        - name: CATTLE_K8S_MANAGED\n          value: \"true\"\n        - name: CATTLE_CLUSTER_REGISTRY\n        - name: CATTLE_CREDENTIAL_NAME\n          value: cattle-credentials-173217425b\n        - name: CATTLE_SERVER_VERSION\n          value: v2.11.2\n        - name: CATTLE_INSTALL_UUID\n          value: dea1affd-f93e-42eb-8368-ad939da7193c\n        - name: CATTLE_INGRESS_IP_DOMAIN\n          value: sslip.io\n        - name: STRICT_VERIFY\n          value: \"true\"\n        image: rancher/rancher-agent:v2.11.2\n        imagePullPolicy: IfNotPresent\n        name: cluster-register\n        resources: {}\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n        volumeMounts:\n        - mountPath: /cattle-credentials\n          name: cattle-credentials\n          readOnly: true\n      dnsPolicy: ClusterFirst\n      hostAliases:\n      - hostnames:\n        - ansible.app.iac-svil.almaviva.it\n        ip: 10.205.166.216\n      restartPolicy: Always\n      schedulerName: default-scheduler\n      securityContext: {}\n      serviceAccount: cattle\n      serviceAccountName: cattle\n      terminationGracePeriodSeconds: 30\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n      - effect: NoExecute\n        key: node-role.kubernetes.io/etcd\n      volumes:\n      - name: cattle-credentials\n        secret:\n          defaultMode: 320\n          secretName: cattle-credentials-173217425b\nstatus:\n  availableReplicas: 2\n  conditions:\n  - lastTransitionTime: \"2025-08-27T13:04:18Z\"\n    lastUpdateTime: \"2025-08-27T13:08:19Z\"\n    message: ReplicaSet \"cattle-cluster-agent-d6994f9d\" has successfully progressed.\n    reason: NewReplicaSetAvailable\n    status: \"True\"\n    type: Progressing\n  - lastTransitionTime: \"2025-08-27T13:09:23Z\"\n    lastUpdateTime: \"2025-08-27T13:09:23Z\"\n    message: Deployment has minimum availability.\n    reason: MinimumReplicasAvailable\n    status: \"True\"\n    type: Available\n  observedGeneration: 4\n  readyReplicas: 2\n  replicas: 2\n  updatedReplicas: 2"
}
2025-08-27 15:12:03,948 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Apply DNS fix to cattle-cluster-agent deployment] *************************************************************************************************************************************************************************************************
2025-08-27 15:12:04,269 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:12:04,278 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Show patch result] ********************************************************************************************************************************************************************************************************************************
2025-08-27 15:12:04,297 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "Patch applied: Yes\nOutput: deployment.apps/cattle-cluster-agent patched (no change)\nError: \n"
}
2025-08-27 15:12:04,304 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Wait for deployment rollout to complete] **********************************************************************************************************************************************************************************************************
2025-08-27 15:12:04,616 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:12:04,623 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Show rollout status] ******************************************************************************************************************************************************************************************************************************
2025-08-27 15:12:04,641 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "rollout_status.stdout": "deployment \"cattle-cluster-agent\" successfully rolled out"
}
2025-08-27 15:12:04,648 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Verify final deployment configuration] ************************************************************************************************************************************************************************************************************
2025-08-27 15:12:04,960 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:12:04,966 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Show final configuration] *************************************************************************************************************************************************************************************************************************
2025-08-27 15:12:04,985 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "Final hostAliases configuration:\n      hostAliases:\n      - hostnames:\n        - ansible.app.iac-svil.almaviva.it\n        ip: 10.205.166.216\n      restartPolicy: Always\n      schedulerName: default-scheduler\n      securityContext: {}\n      serviceAccount: cattle\n      serviceAccountName: cattle\n      terminationGracePeriodSeconds: 30\n      tolerations:\n"
}
2025-08-27 15:12:04,991 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Check cattle-cluster-agent pods status] ***********************************************************************************************************************************************************************************************************
2025-08-27 15:12:05,292 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:12:05,299 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Show pods status] *********************************************************************************************************************************************************************************************************************************
2025-08-27 15:12:05,317 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "Cattle cluster agent pods:\nNAME                                  READY   STATUS    RESTARTS   AGE\ncattle-cluster-agent-d6994f9d-pslf7   1/1     Running   0          2m44s\ncattle-cluster-agent-d6994f9d-w9glp   1/1     Running   0          3m48s\n"
}
2025-08-27 15:12:05,323 p=20016 u=root n=ansible INFO| TASK [kubectl_setup : Show summary] *************************************************************************************************************************************************************************************************************************************
2025-08-27 15:12:05,342 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": "=== KUBECTL SETUP SUMMARY ===\n✅ Kubectl configured: Yes\n✅ Kubeconfig copied: Yes\n🔍 Cattle-system namespace: Found\n🔍 Cattle-cluster-agent deployment: Found\n🔧 DNS Fix applied: Yes\n\n"
}
2025-08-27 15:12:05,353 p=20016 u=root n=ansible INFO| TASK [install_pgadmin : Ensure /root/.kube/config exists (fail early if not)] *******************************************************************************************************************************************************************************************
2025-08-27 15:12:05,600 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:12:05,606 p=20016 u=root n=ansible INFO| TASK [install_pgadmin : Fail if kubeconfig not found (kubectl must be configured on target host)] ***********************************************************************************************************************************************************************
2025-08-27 15:12:05,616 p=20016 u=root n=ansible INFO| skipping: [rke2-manager2]
2025-08-27 15:12:05,622 p=20016 u=root n=ansible INFO| TASK [install_pgadmin : Set KUBECONFIG environment for kubectl interactions] ********************************************************************************************************************************************************************************************
2025-08-27 15:12:05,633 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:12:05,642 p=20016 u=root n=ansible INFO| TASK [install_pgadmin : Check that storageclass exists (warning only)] **************************************************************************************************************************************************************************************************
2025-08-27 15:12:05,940 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:12:05,947 p=20016 u=root n=ansible INFO| TASK [install_pgadmin : Warn if storageclass not found] *****************************************************************************************************************************************************************************************************************
2025-08-27 15:12:05,957 p=20016 u=root n=ansible INFO| skipping: [rke2-manager2]
2025-08-27 15:12:05,963 p=20016 u=root n=ansible INFO| TASK [install_pgadmin : Ensure temporary manifest directory] ************************************************************************************************************************************************************************************************************
2025-08-27 15:12:06,198 p=20016 u=root n=ansible INFO| [0;31m--- before[0m
[0;31m[0m[0;32m+++ after[0m
[0;32m[0m[0;36m@@ -1,4 +1,4 @@[0m
[0;36m[0m {
     "path": "/tmp/pgadmin_manifests",
[0;31m-    "state": "absent"[0m
[0;31m[0m[0;32m+    "state": "directory"[0m
[0;32m[0m }


2025-08-27 15:12:06,198 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:12:06,205 p=20016 u=root n=ansible INFO| TASK [install_pgadmin : Create pgAdmin namespace manifest] **************************************************************************************************************************************************************************************************************
2025-08-27 15:12:06,843 p=20016 u=root n=ansible INFO| [0;31m--- before[0m
[0;31m[0m[0;32m+++ after: /tmp/pgadmin_manifests/pgadmin-namespace.yaml[0m
[0;32m[0m[0;36m@@ -0,0 +1,4 @@[0m
[0;36m[0m[0;32m+apiVersion: v1[0m
[0;32m[0m[0;32m+kind: Namespace[0m
[0;32m[0m[0;32m+metadata:[0m
[0;32m[0m[0;32m+  name: pgadmin[0m
[0;32m[0m

2025-08-27 15:12:06,843 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:12:06,849 p=20016 u=root n=ansible INFO| TASK [install_pgadmin : Apply pgAdmin namespace] ************************************************************************************************************************************************************************************************************************
2025-08-27 15:12:07,225 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:12:07,232 p=20016 u=root n=ansible INFO| TASK [install_pgadmin : Render pgAdmin Deployment manifest] *************************************************************************************************************************************************************************************************************
2025-08-27 15:12:07,892 p=20016 u=root n=ansible INFO| [0;31m--- before[0m
[0;31m[0m[0;32m+++ after: /root/.ansible/tmp/ansible-local-20016_chgpspt/tmphrqaqgjy/pgadmin-deployment.yaml.j2[0m
[0;32m[0m[0;36m@@ -0,0 +1,48 @@[0m
[0;36m[0m[0;32m+apiVersion: apps/v1[0m
[0;32m[0m[0;32m+kind: Deployment[0m
[0;32m[0m[0;32m+metadata:[0m
[0;32m[0m[0;32m+  name: pgadmin[0m
[0;32m[0m[0;32m+  namespace: pgadmin[0m
[0;32m[0m[0;32m+  labels:[0m
[0;32m[0m[0;32m+    app: pgadmin[0m
[0;32m[0m[0;32m+spec:[0m
[0;32m[0m[0;32m+  replicas: 1[0m
[0;32m[0m[0;32m+  selector:[0m
[0;32m[0m[0;32m+    matchLabels:[0m
[0;32m[0m[0;32m+      app: pgadmin[0m
[0;32m[0m[0;32m+  template:[0m
[0;32m[0m[0;32m+    metadata:[0m
[0;32m[0m[0;32m+      labels:[0m
[0;32m[0m[0;32m+        app: pgadmin[0m
[0;32m[0m[0;32m+    spec:[0m
[0;32m[0m[0;32m+      containers:[0m
[0;32m[0m[0;32m+        - name: pgadmin[0m
[0;32m[0m[0;32m+          image: dpage/pgadmin4:8.8[0m
[0;32m[0m[0;32m+          env:[0m
[0;32m[0m[0;32m+            - name: PGADMIN_DEFAULT_EMAIL[0m
[0;32m[0m[0;32m+              value: "admin@example.local"[0m
[0;32m[0m[0;32m+            - name: PGADMIN_DEFAULT_PASSWORD[0m
[0;32m[0m[0;32m+              value: "Cambiami123!"[0m
[0;32m[0m[0;32m+          ports:[0m
[0;32m[0m[0;32m+            - containerPort: 80[0m
[0;32m[0m[0;32m+          volumeMounts:[0m
[0;32m[0m[0;32m+            - name: pgadmin-data[0m
[0;32m[0m[0;32m+              mountPath: /var/lib/pgadmin[0m
[0;32m[0m[0;32m+      volumes:[0m
[0;32m[0m[0;32m+        - name: pgadmin-data[0m
[0;32m[0m[0;32m+          persistentVolumeClaim:[0m
[0;32m[0m[0;32m+            claimName: pgadmin-pvc[0m
[0;32m[0m[0;32m+---[0m
[0;32m[0m[0;32m+apiVersion: v1[0m
[0;32m[0m[0;32m+kind: PersistentVolumeClaim[0m
[0;32m[0m[0;32m+metadata:[0m
[0;32m[0m[0;32m+  name: pgadmin-pvc[0m
[0;32m[0m[0;32m+  namespace: pgadmin[0m
[0;32m[0m[0;32m+spec:[0m
[0;32m[0m[0;32m+  accessModes:[0m
[0;32m[0m[0;32m+    - ReadWriteMany[0m
[0;32m[0m[0;32m+  resources:[0m
[0;32m[0m[0;32m+    requests:[0m
[0;32m[0m[0;32m+      storage: 2Gi[0m
[0;32m[0m[0;32m+  storageClassName: nfs-k8s[0m
[0;32m[0m[0;32m+[0m
[0;32m[0m

2025-08-27 15:12:07,892 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:12:07,899 p=20016 u=root n=ansible INFO| TASK [install_pgadmin : Render pgAdmin Service manifest] ****************************************************************************************************************************************************************************************************************
2025-08-27 15:12:08,536 p=20016 u=root n=ansible INFO| [0;31m--- before[0m
[0;31m[0m[0;32m+++ after: /root/.ansible/tmp/ansible-local-20016_chgpspt/tmp4vysb_62/pgadmin-service.yaml.j2[0m
[0;32m[0m[0;36m@@ -0,0 +1,14 @@[0m
[0;36m[0m[0;32m+apiVersion: v1[0m
[0;32m[0m[0;32m+kind: Service[0m
[0;32m[0m[0;32m+metadata:[0m
[0;32m[0m[0;32m+  name: pgadmin[0m
[0;32m[0m[0;32m+  namespace: pgadmin[0m
[0;32m[0m[0;32m+spec:[0m
[0;32m[0m[0;32m+  selector:[0m
[0;32m[0m[0;32m+    app: pgadmin[0m
[0;32m[0m[0;32m+  ports:[0m
[0;32m[0m[0;32m+    - protocol: TCP[0m
[0;32m[0m[0;32m+      port: 80[0m
[0;32m[0m[0;32m+      targetPort: 80[0m
[0;32m[0m[0;32m+  type: ClusterIP[0m
[0;32m[0m[0;32m+[0m
[0;32m[0m

2025-08-27 15:12:08,537 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:12:08,543 p=20016 u=root n=ansible INFO| TASK [install_pgadmin : Render pgAdmin Ingress manifest] ****************************************************************************************************************************************************************************************************************
2025-08-27 15:12:09,188 p=20016 u=root n=ansible INFO| [0;31m--- before[0m
[0;31m[0m[0;32m+++ after: /root/.ansible/tmp/ansible-local-20016_chgpspt/tmpb4_yqs16/pgadmin-ingress.yaml.j2[0m
[0;32m[0m[0;36m@@ -0,0 +1,26 @@[0m
[0;36m[0m[0;32m+apiVersion: networking.k8s.io/v1[0m
[0;32m[0m[0;32m+kind: Ingress[0m
[0;32m[0m[0;32m+metadata:[0m
[0;32m[0m[0;32m+  name: pgadmin[0m
[0;32m[0m[0;32m+  namespace: pgadmin[0m
[0;32m[0m[0;32m+  annotations:[0m
[0;32m[0m[0;32m+    nginx.ingress.kubernetes.io/ssl-redirect: "true"[0m
[0;32m[0m[0;32m+spec:[0m
[0;32m[0m[0;32m+  ingressClassName: nginx[0m
[0;32m[0m[0;32m+  rules:[0m
[0;32m[0m[0;32m+    - host: pgadmin.app.iac-svil.almaviva.it[0m
[0;32m[0m[0;32m+      http:[0m
[0;32m[0m[0;32m+        paths:[0m
[0;32m[0m[0;32m+          - path: /[0m
[0;32m[0m[0;32m+            pathType: Prefix[0m
[0;32m[0m[0;32m+            backend:[0m
[0;32m[0m[0;32m+              service:[0m
[0;32m[0m[0;32m+                name: pgadmin[0m
[0;32m[0m[0;32m+                port:[0m
[0;32m[0m[0;32m+                  number: 80[0m
[0;32m[0m[0;32m+  # TLS: assumes secret pgadmin-tls esista o venga creato separatamente[0m
[0;32m[0m[0;32m+  tls:[0m
[0;32m[0m[0;32m+    - hosts:[0m
[0;32m[0m[0;32m+        - pgadmin.app.iac-svil.almaviva.it[0m
[0;32m[0m[0;32m+      secretName: pgadmin-tls[0m
[0;32m[0m[0;32m+[0m
[0;32m[0m

2025-08-27 15:12:09,189 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:12:09,195 p=20016 u=root n=ansible INFO| TASK [install_pgadmin : Apply pgAdmin manifests] ************************************************************************************************************************************************************************************************************************
2025-08-27 15:12:09,794 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:12:09,800 p=20016 u=root n=ansible INFO| TASK [install_pgadmin : Wait for pgAdmin deployment rollout] ************************************************************************************************************************************************************************************************************
2025-08-27 15:12:22,122 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:12:22,130 p=20016 u=root n=ansible INFO| PLAY [Install and configure Nginx] **************************************************************************************************************************************************************************************************************************************
2025-08-27 15:12:22,136 p=20016 u=root n=ansible INFO| TASK [Gathering Facts] **************************************************************************************************************************************************************************************************************************************************
2025-08-27 15:12:23,619 p=20016 u=root n=ansible INFO| ok: [nginx]
2025-08-27 15:12:23,623 p=20016 u=root n=ansible INFO| TASK [nginx_install : Update apt cache] *********************************************************************************************************************************************************************************************************************************
2025-08-27 15:12:29,935 p=20016 u=root n=ansible INFO| changed: [nginx]
2025-08-27 15:12:29,941 p=20016 u=root n=ansible INFO| TASK [nginx_install : Install Nginx] ************************************************************************************************************************************************************************************************************************************
2025-08-27 15:12:30,668 p=20016 u=root n=ansible INFO| ok: [nginx]
2025-08-27 15:12:30,674 p=20016 u=root n=ansible INFO| TASK [nginx_install : Ensure Nginx service is stopped before configuration] *********************************************************************************************************************************************************************************************
2025-08-27 15:12:31,096 p=20016 u=root n=ansible INFO| changed: [nginx]
2025-08-27 15:12:31,102 p=20016 u=root n=ansible INFO| TASK [nginx_install : Create Nginx certificate directory] ***************************************************************************************************************************************************************************************************************
2025-08-27 15:12:31,333 p=20016 u=root n=ansible INFO| ok: [nginx]
2025-08-27 15:12:31,339 p=20016 u=root n=ansible INFO| TASK [nginx_install : Copy SSL certificate to remote server] ************************************************************************************************************************************************************************************************************
2025-08-27 15:12:31,791 p=20016 u=root n=ansible INFO| ok: [nginx]
2025-08-27 15:12:31,797 p=20016 u=root n=ansible INFO| TASK [nginx_install : Copy SSL private key to remote server] ************************************************************************************************************************************************************************************************************
2025-08-27 15:12:32,224 p=20016 u=root n=ansible INFO| ok: [nginx]
2025-08-27 15:12:32,230 p=20016 u=root n=ansible INFO| TASK [nginx_install : Create Nginx conf.d directory] ********************************************************************************************************************************************************************************************************************
2025-08-27 15:12:32,461 p=20016 u=root n=ansible INFO| ok: [nginx]
2025-08-27 15:12:32,467 p=20016 u=root n=ansible INFO| TASK [nginx_install : Create Nginx conf-http.d directory] ***************************************************************************************************************************************************************************************************************
2025-08-27 15:12:32,697 p=20016 u=root n=ansible INFO| ok: [nginx]
2025-08-27 15:12:32,703 p=20016 u=root n=ansible INFO| TASK [nginx_install : Backup original nginx.conf] ***********************************************************************************************************************************************************************************************************************
2025-08-27 15:12:32,930 p=20016 u=root n=ansible INFO| ok: [nginx]
2025-08-27 15:12:32,936 p=20016 u=root n=ansible INFO| TASK [nginx_install : Deploy Nginx configuration from template] *********************************************************************************************************************************************************************************************************
2025-08-27 15:12:33,367 p=20016 u=root n=ansible INFO| ok: [nginx]
2025-08-27 15:12:33,375 p=20016 u=root n=ansible INFO| TASK [nginx_install : Test Nginx configuration] *************************************************************************************************************************************************************************************************************************
2025-08-27 15:12:33,608 p=20016 u=root n=ansible INFO| ok: [nginx]
2025-08-27 15:12:33,614 p=20016 u=root n=ansible INFO| TASK [nginx_install : Display Nginx test result] ************************************************************************************************************************************************************************************************************************
2025-08-27 15:12:33,627 p=20016 u=root n=ansible INFO| ok: [nginx] => {
    "nginx_test.stdout_lines": []
}
2025-08-27 15:12:33,632 p=20016 u=root n=ansible INFO| TASK [nginx_install : Enable and start Nginx service] *******************************************************************************************************************************************************************************************************************
2025-08-27 15:12:34,063 p=20016 u=root n=ansible INFO| changed: [nginx]
2025-08-27 15:12:34,069 p=20016 u=root n=ansible INFO| TASK [nginx_install : Check if Nginx is running] ************************************************************************************************************************************************************************************************************************
2025-08-27 15:12:34,451 p=20016 u=root n=ansible INFO| ok: [nginx]
2025-08-27 15:12:34,457 p=20016 u=root n=ansible INFO| TASK [nginx_install : Display Nginx status] *****************************************************************************************************************************************************************************************************************************
2025-08-27 15:12:34,470 p=20016 u=root n=ansible INFO| ok: [nginx] => {
    "msg": "Nginx is active"
}
2025-08-27 15:12:34,479 p=20016 u=root n=ansible INFO| TASK [Display completion message] ***************************************************************************************************************************************************************************************************************************************
2025-08-27 15:12:34,493 p=20016 u=root n=ansible INFO| ok: [nginx] => {
    "msg": [
        "Nginx installation and configuration completed successfully!",
        "Server configured for: test.app.iac-svil.almaviva.it",
        "SSL Certificate: /etc/nginx/cert/fullchain.pem",
        "SSL Key: /etc/nginx/cert/privkey.pem",
        "Remember to copy your SSL certificates to /etc/nginx/cert/"
    ]
}
2025-08-27 15:12:34,496 p=20016 u=root n=ansible INFO| PLAY [Install Prometheus Monitoring Stack] ******************************************************************************************************************************************************************************************************************************
2025-08-27 15:12:34,504 p=20016 u=root n=ansible INFO| TASK [install_prometheus : Create monitoring namespace] *****************************************************************************************************************************************************************************************************************
2025-08-27 15:12:34,866 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:12:34,873 p=20016 u=root n=ansible INFO| TASK [install_prometheus : Check if Helm repo already exists] ***********************************************************************************************************************************************************************************************************
2025-08-27 15:12:35,176 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:12:35,182 p=20016 u=root n=ansible INFO| TASK [install_prometheus : Add Prometheus Community Helm repository] ****************************************************************************************************************************************************************************************************
2025-08-27 15:12:35,192 p=20016 u=root n=ansible INFO| skipping: [rke2-manager2]
2025-08-27 15:12:35,201 p=20016 u=root n=ansible INFO| TASK [install_prometheus : Update Helm repositories] ********************************************************************************************************************************************************************************************************************
2025-08-27 15:12:36,076 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:12:36,083 p=20016 u=root n=ansible INFO| TASK [install_prometheus : Generate Prometheus values file] *************************************************************************************************************************************************************************************************************
2025-08-27 15:12:36,744 p=20016 u=root n=ansible INFO| [0;31m--- before[0m
[0;31m[0m[0;32m+++ after: /root/.ansible/tmp/ansible-local-20016_chgpspt/tmp__9o93zd/prometheus-values.yaml.j2[0m
[0;32m[0m[0;36m@@ -0,0 +1,110 @@[0m
[0;36m[0m[0;32m+# Prometheus Configuration Values[0m
[0;32m[0m[0;32m+prometheus:[0m
[0;32m[0m[0;32m+  prometheusSpec:[0m
[0;32m[0m[0;32m+    scrapeInterval: "1m"[0m
[0;32m[0m[0;32m+    evaluationInterval: "1m"[0m
[0;32m[0m[0;32m+    retention: "15d"[0m
[0;32m[0m[0;32m+    retentionSize: "2GiB"[0m
[0;32m[0m[0;32m+    resources:[0m
[0;32m[0m[0;32m+      requests:[0m
[0;32m[0m[0;32m+        cpu: "1000m"[0m
[0;32m[0m[0;32m+        memory: "2Gi"[0m
[0;32m[0m[0;32m+      limits:[0m
[0;32m[0m[0;32m+        cpu: "2000m"[0m
[0;32m[0m[0;32m+        memory: "4Gi"[0m
[0;32m[0m[0;32m+    storageSpec:[0m
[0;32m[0m[0;32m+      volumeClaimTemplate:[0m
[0;32m[0m[0;32m+        spec:[0m
[0;32m[0m[0;32m+          storageClassName: "nfs-k8s"[0m
[0;32m[0m[0;32m+          accessModes: ["ReadWriteOnce"][0m
[0;32m[0m[0;32m+          resources:[0m
[0;32m[0m[0;32m+            requests:[0m
[0;32m[0m[0;32m+              storage: "5Gi"[0m
[0;32m[0m[0;32m+    serviceMonitorNamespaceSelector: {}[0m
[0;32m[0m[0;32m+    serviceMonitorSelector: {}[0m
[0;32m[0m[0;32m+    ruleNamespaceSelector: {}[0m
[0;32m[0m[0;32m+    ruleSelector: {}[0m
[0;32m[0m[0;32m+    enableAdminAPI: true[0m
[0;32m[0m[0;32m+[0m
[0;32m[0m[0;32m+kubeStateMetrics:[0m
[0;32m[0m[0;32m+  enabled: true[0m
[0;32m[0m[0;32m+[0m
[0;32m[0m[0;32m+nodeExporter:[0m
[0;32m[0m[0;32m+  enabled: true[0m
[0;32m[0m[0;32m+[0m
[0;32m[0m[0;32m+grafana:[0m
[0;32m[0m[0;32m+  enabled: true[0m
[0;32m[0m[0;32m+  adminPassword: "SecureGrafanaPassword123!"[0m
[0;32m[0m[0;32m+  persistence:[0m
[0;32m[0m[0;32m+    enabled: true[0m
[0;32m[0m[0;32m+    size: "3Gi"[0m
[0;32m[0m[0;32m+    storageClassName: "nfs-k8s"[0m
[0;32m[0m[0;32m+  ingress:[0m
[0;32m[0m[0;32m+    enabled: false[0m
[0;32m[0m[0;32m+    # Se vuoi abilitare l'ingress direttamente[0m
[0;32m[0m[0;32m+    # enabled: true[0m
[0;32m[0m[0;32m+    # hosts:[0m
[0;32m[0m[0;32m+    #   - grafana.app.iac-svil.almaviva.it[0m
[0;32m[0m[0;32m+    # tls:[0m
[0;32m[0m[0;32m+    #   - secretName: grafana-tls[0m
[0;32m[0m[0;32m+    #     hosts:[0m
[0;32m[0m[0;32m+    #       - grafana.app.iac-svil.almaviva.it[0m
[0;32m[0m[0;32m+[0m
[0;32m[0m[0;32m+alertmanager:[0m
[0;32m[0m[0;32m+  enabled: true[0m
[0;32m[0m[0;32m+  alertmanagerSpec:[0m
[0;32m[0m[0;32m+    storage:[0m
[0;32m[0m[0;32m+      volumeClaimTemplate:[0m
[0;32m[0m[0;32m+        spec:[0m
[0;32m[0m[0;32m+          storageClassName: "nfs-k8s"[0m
[0;32m[0m[0;32m+          accessModes: ["ReadWriteOnce"][0m
[0;32m[0m[0;32m+          resources:[0m
[0;32m[0m[0;32m+            requests:[0m
[0;32m[0m[0;32m+              storage: "2Gi"[0m
[0;32m[0m[0;32m+    resources:[0m
[0;32m[0m[0;32m+      requests:[0m
[0;32m[0m[0;32m+        cpu: "100m"[0m
[0;32m[0m[0;32m+        memory: "128Mi"[0m
[0;32m[0m[0;32m+      limits:[0m
[0;32m[0m[0;32m+        cpu: "200m"[0m
[0;32m[0m[0;32m+        memory: "256Mi"[0m
[0;32m[0m[0;32m+[0m
[0;32m[0m[0;32m+# Disable some components that might cause issues[0m
[0;32m[0m[0;32m+defaultRules:[0m
[0;32m[0m[0;32m+  create: true[0m
[0;32m[0m[0;32m+  rules:[0m
[0;32m[0m[0;32m+    alertmanager: true[0m
[0;32m[0m[0;32m+    etcd: true[0m
[0;32m[0m[0;32m+    configReloaders: true[0m
[0;32m[0m[0;32m+    general: true[0m
[0;32m[0m[0;32m+    k8s: true[0m
[0;32m[0m[0;32m+    kubeApiserverAvailability: true[0m
[0;32m[0m[0;32m+    kubeApiserverBurnrate: true[0m
[0;32m[0m[0;32m+    kubeApiserverHistogram: true[0m
[0;32m[0m[0;32m+    kubeApiserverSlos: true[0m
[0;32m[0m[0;32m+    kubelet: true[0m
[0;32m[0m[0;32m+    kubeProxy: true[0m
[0;32m[0m[0;32m+    kubePrometheusGeneral: true[0m
[0;32m[0m[0;32m+    kubePrometheusNodeRecording: true[0m
[0;32m[0m[0;32m+    kubernetesApps: true[0m
[0;32m[0m[0;32m+    kubernetesResources: true[0m
[0;32m[0m[0;32m+    kubernetesStorage: true[0m
[0;32m[0m[0;32m+    kubernetesSystem: true[0m
[0;32m[0m[0;32m+    kubeScheduler: true[0m
[0;32m[0m[0;32m+    kubeStateMetrics: true[0m
[0;32m[0m[0;32m+    network: true[0m
[0;32m[0m[0;32m+    node: true[0m
[0;32m[0m[0;32m+    nodeExporterAlerting: true[0m
[0;32m[0m[0;32m+    nodeExporterRecording: true[0m
[0;32m[0m[0;32m+    prometheus: true[0m
[0;32m[0m[0;32m+    prometheusOperator: true[0m
[0;32m[0m[0;32m+[0m
[0;32m[0m[0;32m+# Prometheus Operator configuration[0m
[0;32m[0m[0;32m+prometheusOperator:[0m
[0;32m[0m[0;32m+  resources:[0m
[0;32m[0m[0;32m+    requests:[0m
[0;32m[0m[0;32m+      cpu: "100m"[0m
[0;32m[0m[0;32m+      memory: "128Mi"[0m
[0;32m[0m[0;32m+    limits:[0m
[0;32m[0m[0;32m+      cpu: "200m"[0m
[0;32m[0m[0;32m+      memory: "256Mi"[0m
[0;32m[0m

2025-08-27 15:12:36,744 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:12:36,750 p=20016 u=root n=ansible INFO| TASK [install_prometheus : Check if Prometheus is already installed] ****************************************************************************************************************************************************************************************************
2025-08-27 15:12:37,029 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:12:37,036 p=20016 u=root n=ansible INFO| TASK [install_prometheus : Install Prometheus using Helm] ***************************************************************************************************************************************************************************************************************
2025-08-27 15:13:33,566 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:13:33,573 p=20016 u=root n=ansible INFO| TASK [install_prometheus : Upgrade Prometheus if already installed and values changed] **********************************************************************************************************************************************************************************
2025-08-27 15:13:33,583 p=20016 u=root n=ansible INFO| skipping: [rke2-manager2]
2025-08-27 15:13:33,590 p=20016 u=root n=ansible INFO| TASK [install_prometheus : Wait for Prometheus StatefulSet to be ready] *************************************************************************************************************************************************************************************************
2025-08-27 15:13:34,097 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:13:34,104 p=20016 u=root n=ansible INFO| TASK [install_prometheus : Wait for Grafana Deployment to be ready] *****************************************************************************************************************************************************************************************************
2025-08-27 15:13:34,505 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:13:34,512 p=20016 u=root n=ansible INFO| TASK [install_prometheus : Wait for AlertManager StatefulSet to be ready] ***********************************************************************************************************************************************************************************************
2025-08-27 15:13:34,995 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:13:35,002 p=20016 u=root n=ansible INFO| TASK [install_prometheus : Create Grafana Ingress] **********************************************************************************************************************************************************************************************************************
2025-08-27 15:13:35,379 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:13:35,386 p=20016 u=root n=ansible INFO| TASK [install_prometheus : Create Prometheus Ingress] *******************************************************************************************************************************************************************************************************************
2025-08-27 15:13:35,730 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:13:35,737 p=20016 u=root n=ansible INFO| TASK [install_prometheus : Get all pods status in monitoring namespace] *************************************************************************************************************************************************************************************************
2025-08-27 15:13:36,044 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:13:36,051 p=20016 u=root n=ansible INFO| TASK [install_prometheus : Get services in monitoring namespace] ********************************************************************************************************************************************************************************************************
2025-08-27 15:13:36,352 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:13:36,362 p=20016 u=root n=ansible INFO| TASK [install_prometheus : Get PersistentVolumeClaims status] ***********************************************************************************************************************************************************************************************************
2025-08-27 15:13:36,673 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:13:36,713 p=20016 u=root n=ansible INFO| TASK [install_prometheus : Get Ingress status] **************************************************************************************************************************************************************************************************************************
2025-08-27 15:13:37,045 p=20016 u=root n=ansible INFO| ok: [rke2-manager2]
2025-08-27 15:13:37,052 p=20016 u=root n=ansible INFO| TASK [install_prometheus : Display installation summary] ****************************************************************************************************************************************************************************************************************
2025-08-27 15:13:37,095 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": [
        "==========================================",
        "Prometheus Stack Installation Completed!",
        "==========================================",
        "Namespace: monitoring",
        "Release Name: prometheus",
        "Storage Class: nfs-k8s",
        "",
        "Pod Status:",
        "NAME                                                     READY   STATUS    RESTARTS   AGE   IP               NODE           NOMINATED NODE   READINESS GATES\nalertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running   0          37s   10.42.1.11       rke2-worker2   <none>           <none>\nprometheus-grafana-9df6b8cb9-7xk9d                       3/3     Running   0          42s   10.42.3.8        rke2-worker3   <none>           <none>\nprometheus-kube-prometheus-operator-8775db77c-9z5hb      1/1     Running   0          42s   10.42.4.6        rke2-worker1   <none>           <none>\nprometheus-kube-state-metrics-85d8489cdc-dfdfg           1/1     Running   0          42s   10.42.4.7        rke2-worker1   <none>           <none>\nprometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running   0          37s   10.42.4.8        rke2-worker1   <none>           <none>\nprometheus-prometheus-node-exporter-4cmtg                1/1     Running   0          42s   10.205.166.218   rke2-worker1   <none>           <none>\nprometheus-prometheus-node-exporter-9btck                1/1     Running   0          42s   10.205.166.215   rke2-worker3   <none>           <none>\nprometheus-prometheus-node-exporter-vc5zj                1/1     Running   0          42s   10.205.166.214   rke2-worker2   <none>           <none>",
        "",
        "Services:",
        "NAME                                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE\nalertmanager-operated                     ClusterIP   None            <none>        9093/TCP,9094/TCP,9094/UDP   38s\nprometheus-grafana                        ClusterIP   10.43.237.214   <none>        80/TCP                       42s\nprometheus-kube-prometheus-alertmanager   ClusterIP   10.43.199.242   <none>        9093/TCP,8080/TCP            42s\nprometheus-kube-prometheus-operator       ClusterIP   10.43.155.75    <none>        443/TCP                      42s\nprometheus-kube-prometheus-prometheus     ClusterIP   10.43.102.193   <none>        9090/TCP,8080/TCP            42s\nprometheus-kube-state-metrics             ClusterIP   10.43.232.233   <none>        8080/TCP                     42s\nprometheus-operated                       ClusterIP   None            <none>        9090/TCP                     37s\nprometheus-prometheus-node-exporter       ClusterIP   10.43.138.119   <none>        9100/TCP                     42s",
        "",
        "Persistent Volume Claims:",
        "NAME                                                                                                             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE\nalertmanager-prometheus-kube-prometheus-alertmanager-db-alertmanager-prometheus-kube-prometheus-alertmanager-0   Bound    pvc-afa88bd4-4a75-4167-896b-55ed2faa4467   2Gi        RWO            nfs-k8s        <unset>                 38s\nprometheus-grafana                                                                                               Bound    pvc-fc50776e-81ed-46ae-b959-194c7c0b39a7   3Gi        RWO            nfs-k8s        <unset>                 42s\nprometheus-prometheus-kube-prometheus-prometheus-db-prometheus-prometheus-kube-prometheus-prometheus-0           Bound    pvc-523c6647-58a1-4ec2-854f-ec0078c3e94f   5Gi        RWO            nfs-k8s        <unset>                 37s",
        "",
        "Ingress Resources:",
        "NAME                 CLASS   HOSTS                                 ADDRESS   PORTS   AGE\ngrafana-ingress      nginx   grafana.app.iac-svil.almaviva.it                80      2s\nprometheus-ingress   nginx   prometheus.app.iac-svil.almaviva.it             80      2s",
        "",
        "Access Information:",
        "- Grafana Web UI: https://grafana.app.iac-svil.almaviva.it",
        "- Prometheus Web UI: https://prometheus.app.iac-svil.almaviva.it",
        "- Grafana Admin User: admin",
        "- Grafana Admin Password: SecureGrafanaPassword123!",
        "",
        "",
        "",
        "",
        ""
    ]
}
2025-08-27 15:13:37,102 p=20016 u=root n=ansible INFO| TASK [install_prometheus : Clean up temporary values file] **************************************************************************************************************************************************************************************************************
2025-08-27 15:13:37,342 p=20016 u=root n=ansible INFO| [0;31m--- before[0m
[0;31m[0m[0;32m+++ after[0m
[0;32m[0m[0;36m@@ -1,4 +1,4 @@[0m
[0;36m[0m {
     "path": "/tmp/prometheus-values.yaml",
[0;31m-    "state": "file"[0m
[0;31m[0m[0;32m+    "state": "absent"[0m
[0;32m[0m }


2025-08-27 15:13:37,342 p=20016 u=root n=ansible INFO| changed: [rke2-manager2]
2025-08-27 15:13:37,353 p=20016 u=root n=ansible INFO| TASK [Display Prometheus access information] ****************************************************************************************************************************************************************************************************************************
2025-08-27 15:13:37,366 p=20016 u=root n=ansible INFO| ok: [rke2-manager2] => {
    "msg": [
        "Prometheus Stack installato con successo!",
        "Per configurare l'accesso esterno, aggiungi questi upstream al tuo Nginx:",
        "",
        "upstream prometheus_backend {",
        "    server 10.205.166.217:30090 max_fails=3 fail_timeout=5s;",
        "}",
        "",
        "upstream grafana_backend {",
        "    server 10.205.166.217:30300 max_fails=3 fail_timeout=5s;",
        "}",
        "",
        "Oppure usa port-forward per accesso diretto:",
        "kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80",
        "kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090"
    ]
}
2025-08-27 15:13:37,367 p=20016 u=root n=ansible INFO| PLAY RECAP **************************************************************************************************************************************************************************************************************************************************************
2025-08-27 15:13:37,367 p=20016 u=root n=ansible INFO| cornelio.app.iac-svil.almaviva.it : ok=77   changed=28   unreachable=0    failed=0    skipped=19   rescued=0    ignored=1   
2025-08-27 15:13:37,367 p=20016 u=root n=ansible INFO| localhost                  : ok=18   changed=3    unreachable=0    failed=0    skipped=12   rescued=0    ignored=1   
2025-08-27 15:13:37,367 p=20016 u=root n=ansible INFO| nfs-server-01              : ok=27   changed=7    unreachable=0    failed=0    skipped=15   rescued=0    ignored=0   
2025-08-27 15:13:37,368 p=20016 u=root n=ansible INFO| nginx                      : ok=33   changed=5    unreachable=0    failed=0    skipped=12   rescued=0    ignored=0   
2025-08-27 15:13:37,368 p=20016 u=root n=ansible INFO| rke2-manager2              : ok=143  changed=63   unreachable=0    failed=0    skipped=26   rescued=0    ignored=5   
2025-08-27 15:13:37,368 p=20016 u=root n=ansible INFO| rke2-worker1               : ok=22   changed=3    unreachable=0    failed=0    skipped=19   rescued=0    ignored=0   
2025-08-27 15:13:37,368 p=20016 u=root n=ansible INFO| rke2-worker2               : ok=22   changed=3    unreachable=0    failed=0    skipped=19   rescued=0    ignored=0   
2025-08-27 15:13:37,368 p=20016 u=root n=ansible INFO| rke2-worker3               : ok=22   changed=3    unreachable=0    failed=0    skipped=19   rescued=0    ignored=1   
